\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{anysize} %<-for setting margins
\marginsize {1.6cm}{1.6cm}{2.0cm}{2cm}
\usepackage{color}
\newcommand{\dean}[1]{\textsf{\emph{\textbf{\textcolor{red}{#1}}}}}

\title{Response To Reviews - A Data-Driven Framework for Neural Field Modeling}
\author{D. Freestone, P. Aram, M. Dewar, K. Scerri, D. Grayden, V. Kadirkamanathan}

\begin{document}
    \maketitle
    
    \section{Reviewer 1}
    
        We first address the reviewer's main concerns, followed by the reviewer's additional comments.
        
    \subsection{Main Concerns}
    It would be useful to mention the above (now below) explicitly in the introduction and try to address them in later sections of the paper.

    \begin{enumerate}
        \item Do not use real data for the validation of their approach

	\emph{The authors appreciate that the only way to truly validate the proposed framework is with real data. However, before validating the method on real data we believe that it is important to first validate the method artificial data, where the true and estimated parameters can be compared. Given the space limitations on the article, it is not feasible to show examples using real data in this paper. Another paper demonstrating the application of the method with data recorded from a Utah array implanted in an epilepsy surgical patient is currently in preparation. In an effort to make the method employed in the paper more explicit, we have added ``To illustrate the estimation framework, data is generated using the neural field equations incorporating modeled sensors enabling a comparison between the estimated and true parameters'' to the introduction.}
	
        \item Assume that a few kernel basis functions accurately represent the actual connectivity kernel, which might not be the case when using real data;

	\emph{The examples presented in the paper use the Mexican hat connectivity kernel. The framework is also valid for other connectivity kernels provided they are homogeneous and that they can be approximated by a set of isotropic basis functions parametrized by a coefficient vector $\mathbf{\theta}$. The mathematical formulation does not restrict the shape of the kernel, where anisotropic (we assume isotropy of the kernel basis functions to make the simplification in equation~18) arbitrary shapes can be represented. Furthermore, the kernel basis functions need not be a Gaussian shape, but can be, for example, Laplacian to describe the ``wizard hat'' connectivity or B-spline functions to describe a kernel with a more compact support. For more complex basis functions some of the analytic derivations described in the Appendix may have to be performed numerically. We have added the above statements to the methods section (after equation~13) to be more explicit in describing subtleties of the method.}

        \item Do not consider the EEG/MEG lead field when constructing their mapping between the membrane voltage and the electrophysiological data (observation function)

	\emph{In response to this important point, the authors have made an effort to be clear that we are not considering scalp EEG to be a valid data source for the estimation framework. Amendments have been made to the Introduction (last paragraph), Method (after equation 9), and Discussion. We have primarily developed the framework for higher density intracranial measurements. We believe that the spatial frequency response and sampling density of extracranial EEG/MEG measurements will be insufficient to estimate the connectivity structure.}
	
	\emph{..., and maybe we can say the framework can be altered in the case of scalp EEG by considering the lead field in the mapping between the membrane voltage and the electrophysiological data, resulting in different observation equation (if nonlinear we need unscented transform on observation and if not the state estimation remains unchanged.) This is from PLoS review which might be useful: In much of the previous papers cited the EEG/MEG lead field is
considered. This is different of course for scalp electrodes than for intracranial electrodes but should be explicitly spelled out because this factor cannot be ignored. In fact the observation model is not quite realistic, without lead field and without amplifier characteristics.}
    \end{enumerate}
    
    \subsection{Additional Comments}
    
    \begin{enumerate}
        \item The discrepancy of the results in Experiment II where the weights of the basis functions are not in agreement with the actual parameters while the reconstructed kernels seem satisfying.

        \item The use of Euler discretisation and how this affects the system's behavior.

		\emph{This point was made by a reviewer from our PLoS attempt. The reviewer stated: ``1. The authors might want to reference prior work in using Kalman
		Filtering for the estimation of neural mass models from data [1-3],
		[13]. In fact from those papers stressed the importance of carrying out
		a high order discretization of the differential equations, which should
		be in fact treated as stochastic. In equation (S1.1) of the extended
		derivation, it is shown that an Euler approximation is taken which may
		even change the dynamical properties of the system. The authors should
		discuss this issue.''}
		
        \item The derivation of Equations (30) and (31) [spacing of basis functions] and make them more transparent for the general audience.

\emph{Consider a regular lattice in $\mathbb R^n$, with lattice points $\left\lbrace \Delta J; J\in \mathbb Z^n\right\rbrace$ and where $\Delta$ is the distance between adjacent points. The continuous field $v(\mathbf r)$ can be sampled using a field of Dirac-delta functions, $\delta(\mathbf r)$ located at the lattice points as
\begin{equation}
 v_s(\Delta J)=\sum_{J \in \mathbb Z^n}\delta(\mathbf r-\Delta J)v(\mathbf r)
\end{equation}
taking Fourier transform of the sampled field $v_s(\Delta J)$ we have
\begin{align}
 V_s(\boldsymbol{\nu})&=\frac{1}{\Delta^n}\sum_{J\in\mathbb Z^n}\delta(\boldsymbol{\nu}-\frac{J}{\Delta})\ast V(\boldsymbol\nu)\\
&=\frac{1}{\Delta^n}V(\boldsymbol\nu)+\frac{1}{\Delta^n}\sum_{J\in\mathbb Z^n\setminus \mathbf \{0\}}V(\boldsymbol{\nu}-\frac{J}{\Delta})
\end{align}
hence,$V_s(\boldsymbol{\nu})$ contains $V(\boldsymbol\nu)$ plus infinitely many copies (aliases) of  $V(\boldsymbol\nu)$ repeated on the regular lattice $\left\lbrace \frac{J}{\Delta}; J\in \mathbb Z^n\right\rbrace$. Given the dynamic field is band-limited with a cut-off frequency $\boldsymbol\nu_c$ in order for $V(\boldsymbol\nu)$ to be disjointed with its aliases the amount of shift $\frac{1}{\Delta}$ must be at least $2\nu_c$, therefore
\begin{equation}\label{ap:Shannon}
	\Delta\le\frac{1}{2\rho\nu_c}
\end{equation}
where $\rho \in \mathbb{R} \ge 1$ is an oversampling parameter. To recover the continuous field, we first introduce a low-pass filter \begin{equation}
F(\boldsymbol \nu)=\begin{cases}
  \Delta^n & |\boldsymbol\nu|\le \boldsymbol\nu_c \\
0 & \text{otherwise}
\end{cases} 
\end{equation}
if the sampling rule \eqref{ap:Shannon} is satisfied, the original spectrum, $V(\boldsymbol\nu)$ can be recovered using
\begin{equation}
	V(\boldsymbol\nu)=F(\boldsymbol \nu)V_s(\boldsymbol\nu)
\end{equation}
taking inverse Fourier transform we have
\begin{align}
	v(\mathbf s)&=f(\mathbf s)\ast v_s(\Delta J) \\
	&= \sum_{J \in \mathbb Z^n}f(\mathbf s-\Delta J) v_s(\Delta J)
\end{align}
However many other basis functions can be potentially used for reconstruction, and a continuous field can be reconstructed from the sampled field by the sum of weighted, spatially translated, continuous  basis functions.}

Parham: I made a copy for my edits so that we don't get conflicts.

\emph{Consider the cortical sheet defined on the surface $\Omega$ with the sensors placed on a regular grid at positions $\mathbf{r}_n$ (where $n=0,...,n_y-1$ indexes the sensors) that are separated by the distance $\Delta_y$. The observations can be considered samples from a continuous field that is filtered (convolved) by the observation kernel as
\begin{equation}
	y(\mathbf{r}_n)=\sum_{n=0}^{n_y-1}\delta(\mathbf r - \mathbf{r}_n ) v_m(\mathbf{r}),
\end{equation} 
where $v_m(\mathbf{r}) = v_t(\mathbf{r})\ast m(\mathbf{r})$. The Fourier transform of $y(\mathbf{r}_n)$ is
\begin{align}
 Y(\boldsymbol{\nu}) &= \frac{1}{\Delta_y^2}\sum_{n=0}^{n_y-1} \delta\left(\boldsymbol{\nu}-\frac{n}{\Delta_y}\right) \ast V_m(\boldsymbol \nu) \\
	&= \frac{1}{\Delta_y^2}V_m(\boldsymbol\nu)+\frac{1}{\Delta_y^2}\sum_{n=1}^{n_y-1} V_m\left(\boldsymbol{\nu}-\frac{n}{\Delta_y}\right).
\end{align} 
It should be clear that $Y(\boldsymbol{\nu})$ contains $V_m(\boldsymbol\nu)$ plus $n_y-2$ copies (aliases) of $V_m(\boldsymbol\nu)$ repeated on the regular lattice with spacing $\frac{1}{\Delta_y}$. Due to the lowpass action of the observation kernel, the filtered dynamic field is band-limited with a cutoff frequency $\boldsymbol\nu_{cy}$. In order for $V_m(\boldsymbol\nu)$ to be disjointed from its aliases, the amount of shift $\frac{1}{\Delta_y}$ must be at least $2\boldsymbol\nu_{cy}$ (since each alias has a spectral range of
\begin{equation}
	\boldsymbol\nu = \left[\frac{n}{\Delta_y}-\boldsymbol{\nu}_{cy},\frac{n}{\Delta_y}+\boldsymbol{\nu}_{cy}\right].
\end{equation}
Therefore, to avoid aliasing the distance between observations must satisfy
\begin{equation}\label{eq:SamplingCriteria}
	\frac{1}{\Delta_y} \ge 2\nu_{cy}.
\end{equation}
Since the observation kernel is not an ideal filter an oversampling parameter should be included to further separate the aliases. Rearranging equation~\ref{eq:SamplingCriteria} and adding the oversampling parameter, $\rho$, gives
\begin{equation}\label{ap:Shannon}
	\Delta_y\le\frac{1}{2\rho\nu_{cy}},
\end{equation}
where $\rho \in \mathbb{R} \ge 1$. }

        \item The existing literature on inversion schemes for similar models of brain activity.
        \item In section 3, Table 6 is mentioned however there is no such table in the ms.
    \end{enumerate}
    
    \section{Reviewer 2}
    
    We thank the author for their encouraging review. Each point raised is dealt with in turn.

\begin{enumerate}
    \item The handling of the (to borrow a term from numerical weather forecasting) `sub-grid scale physics' is inadequate for nervous systems (see, e.g., equation 8 [The nonlinear IntegroDifference Equation]). I do very much appreciate the spectral estimation of spatial and temporal scales. But the model errors are lumped into an iid model error term that is not realistic. For instance, the flow of potassium in the extracellular spaces has a known dynamic that will affect network excitability, generating important sub-grid dynamics that is anything but iid. See, for instance, the example in Ullah et al PLoS Comp Biol 2010 where without such dynamics, state tracking and parameter estimation for a neural system fails. A beautiful overview of sub-grid effects in data assimilation is found in Kalnay's 2003 book. I would very much value the authors comments on these issues. I guess were I to respond to this question, that I might consider extending the model error term to an autoregressive model. But I will let the authors answer this question.\\

    \emph{talk about model order}

\item Although we all know the 1970s version of Wilson and Cowan's field equations, there was an intriguing stochasticization of these equations by Benayoun et al PloS Comp Biol 2010. I and others will value the authors' thoughts of how a more palatable stochasticization of these field equations might influence the future development of the data-assimilation framework presented in this paper.
\item This is NeuroImage, not the Physical Review. Although most of this framework involves mathematics that is reasonable to follow for typical readers of this journal, I would suggest that the authors explain and expand a bit on the introduction of Green's functions where introduced in equation 6.
\item What is the relation of the substitution of the temporal and spatial basis functions as employed in this paper to Galerkin's projection? How do the two techniques differ? If this is close, the non-local case for a Galerkin projection (typically used in diffusive processes) has not, to my knowledge, been done. Certainly not in a way that is obvious for neural field theory. Perhaps this is my ignorance. This seems important to clarify for the rest of us.
\item In the isotropic and translationally invariant case examined here, would a Fourier basis for temporal and spatial structure have been a formally correct alternative?\\

\emph{Gaussians has semi-compact support but Fourier basis functions are global functions. Gaussian can be easily extended to higher dimension (don't know about the Fourier basis functions), any orthonormal basis functions like Fourier basis functions simplify our equations, $\Gamma$ becomes identity matrix, but using Gaussian basis functions equations are in their generic form.}

\item In equation 29, what would a reasonable $v_c$ be for a real neuronal system (mammalian) given the number of neurons and scale of electrodes? I am concerned that it might be much higher than any of us wish to consider. One would perhaps be saved by the filter characteristics of brain (e.g. Phys. Rev. E 73, 051911 (2006)).
\item The two covariance inflation parameters in equation 41 make me cringe. How are we best to adjust these fudge factors?
\emph{The positive constant $\alpha$ determines the spread of the sigma points around $\mathbf{\bar x}$ which can be made arbitrary small (e.g. $0<\alpha\le 10^{-4} $) to minimise higher order effect (cite Julier (The Scaled Unscented Transformation) and  Haykin (Kalman filtering and neural network) , and  $\kappa$ is an additional scaling parameter which is usually set to $3-n_x$. cite Julier A New Approach for Filtering Nonlinear Systems}



\item I understand that the Mexican Hat bases are in this paper often a composite of $\theta_0$, $\theta_1$, and $\theta_2$, representing short range excitation, short range inhibition, and long range excitation. But many will be confused by this subtlety, and at times in the text the Mexican Hat basis is described as if it were a single function. I would show how the theta's sum to create the general spatial basis shape of the hat in figure 1, and go through the paper in order to clarify this issue.\\
\emph{I think we need to show each kernel basis function in Figure 1}
 \begin{figure*}[!ht]
 \begin{center}
 \includegraphics{./Graph/pdf/fig1_revised.pdf} 
 \end{center}
 \caption{}
 \label{fig:Figure1}
 \end{figure*}

\item A few comments on how to adapt the smoother for real time implementation would be helpful. I presume that one would perform the smoothing during a training phase. But one would also wish to adapt in real time as observation or control scenarios progressed. A few words of perspective would be invaluable to help others translate the author's wisdom on this method to other applications.
\item Figure 4. Should the axes be indicated as spatial frequency rather than just Hz?
\item Page 45, Appendix D, Par 1, line 6: should `transform of a n-' should be `transform of an n-' ? I am grammatically challenged perhaps.
\end{enumerate}

\end{document}