%% bare_jrnl.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/


\documentclass[onecolumn,draftcls]{IEEEtran}

\usepackage{graphicx}
\usepackage{color}                    % For creating coloured text and background
\usepackage{amsmath,amssymb,amsfonts} % Typical maths resource packages
% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}
% TODO environment
\newcommand{\todo}[1]{\textsf{\emph{\textbf{\textcolor{blue}{#1}}}}}

\begin{document}

% can use linebreaks \\ within to get better formatting as desired
\title{Estimation of Intracortical Connectivity in a Dynamical Neural Field Model}

\author{Dean~R.~Freestone,~\IEEEmembership{Graduate Student Member,~IEEE,}
        Parham~Aram,~\IEEEmembership{Graduate Student Member,~IEEE,}
        Michael~Dewar}
        Kenneth~Scerri,~\IEEEmembership{Member,~IEEE,}
        Visakan~Kadirkamanathan,~\IEEEmembership{Member,~IEEE,}
        David~B.~Grayden,~\IEEEmembership{Member,~IEEE,}
        Anthony~N.~Burkitt,~\IEEEmembership{Member,~IEEE,}
        and~other~Bosses/Supervisors,~\IEEEmembership{Member,~IEEE.}% <-this % stops a space% <-this % stops a space

\thanks{D. Freestone is with the Department
of Electrical and Electronic Engineering, University of Melbourne, Melbourne,
Vic, 3010 Australia {\tt\small dfreestone@bionicear.org}.}% <-this % stops a space
\thanks{P. Aram is with Sheffield...}
\thanks{M. Dewar is with Edinburgh...}
\thanks{K. Scerri is with Malta...}
\thanks{V. Kadirkamanathan is with Sheffield...}

\thanks{Manuscript received Month Day, Year; revised Month Day, Year.}


% The paper headers
\markboth{Journal of \"{U}ber C\~{o}\~{o}l Engineering,~Vol.~1, No.~1, Christmas~2009}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Journals}

\maketitle

\begin{abstract}
Will save this for last.
\end{abstract}


% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Integro-Difference Equation (IDE), Neural Field Model, Cortical Connectivity.
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle

\section{Introduction}
\IEEEPARstart{T}{he} human brain is arguably the world's most complex system. Approximately 100 billion neurons and 60 trillion synapses operate in concert to process information, resulting in the cognition that determines our behavior. The brain's efficiency, robustness, and adaptability are unparalleled by any man-made device and, despite decades of concerted research, our understanding of its complex dynamics remains modest. This has led researchers to study different scales of dynamics: starting from individual proteins, synapses, and neurons; moving through to neuronal networks and ensembles of neuronal networks. While our understanding of the function of neurons is well developed, the overall behavior of the brain's meso and macro-scale dynamics remains largely a mystery.  Understanding the brain at this level is extremely important since this is the scale where pathologies such as epilepsy, Parkinson's disease and schizophrenia are manifested.

To date, there has been a considerable volume of work in generating physiologically plausible neural field models to fill the void of understanding brain dynamics at the meso/macroscopic scale. Mathematical neural field  models provide insights into the underlying physics and dynamics of electroencephalography (EEG) and magnetoencephalography (MEG) (see~\cite{Deco2008}~\cite{David2003} for recent reviews). These models have demonstrated possible mechanisms for the genesis of neural rhythms (such as the alpha and gamma rhythms) \cite{Liley1999} \cite{RENNIE2000}, epileptic seizure generation \cite{DaSilva2003},~\cite{Suffczynski2004} and~\cite{Wendling2005} and insights into other pathologies~\cite{Moran2008} \cite{Schiff2009} that would be impossible to gain from experimental data alone. Unfortunately, to date the use of these models in the clinic has been limited, since the neural field model are constructed for a general brain dynamics and pathologies almost always have unique underlying patient specific causes. Data from EEG and functional magnetic resonance imaging (fMRI) offers the patient specific link to macro-scale cortical dynamics, making these tools readily applicable to the clinic. However the underlying system properties, or system states, are hidden from EEG and fMRI data, making predictions of the underlying physiology inherently difficult.

For models to be clinically viable they must be patient specific. A possible approach to achieve this would be to use a general neural field model, like the Wilson and Cohen~\cite{Wilson1973} model or a neural mass model like the Jansen and Ritt model~\cite{Jansen1995}, and fit it to a patients EEG data. Fitting the neural models to individuals is a highly non-trivial task, and until very recently this has not been reported in the literature. An estimation frame work for neural field models known as dynamical causal modeling (DCM)~\cite{David2003} \cite{David2006} has recently been proposed for studying evoked potential dynamics. Via a Bayesian inference scheme, DCM estimates the long range connectivity structure between the specific isolated brain regions that best explains a given data set. The interconnected brain regions are modeled by the Jansen and Ritt equations. The approach has proven very useful in understanding specific hierarchical networks of neural information processing. Another recent publication describing a parameter estimation method with a neural field model used an unscented Kalman filter with the Wilson-Cowan neural field equations~\cite{schiff2008kalman}.  This work takes a more system theoretic approach to the neural estimation problem and marks a first step in what has the potential to revolutionize the treatment of many neurological diseases where therapeutic electrical stimulation is viable.

Generating patient specific models allows the application of a range of techniques from control systems theory, where tailored electrical stimulation could be used therapeutically in a closed loop fashion.  Currently available epileptic seizure prediction and control devices (i.e., the vagal nerve stimulator) are implemented in an ``open loop".  That is, the therapeutic electrical stimulation waveforms are adjusted for each patient by trial and error, disregarding the patient's neurodynamics and information about their particular pathologies. Given access to an accurate model, the application of control theory in these circumstances would allow for robust therapeutic stimulations.
The work from Schiff and Sauer~\cite{schiff2008kalman} successfully demonstrated  it is possible to estimate parameters from the WC equations. This motivates the question, what parameters are the most important? How patient specific do the assumed parameters have to be for the model to be useful? In this paper, we address both of these questions and provide a theoretical platform to perform patient-specific grey-box modeling of the human neocortex.

\section{The Importance of Cortical Structure}
Neural field models use statistics of the cortex to relate mean firing rates of pre-synaptic neural populations to post-synaptic mean membrane potentials. Each neural population represents a functional cortical processing unit referred to as a column. The columnar organization of the cortex is not discrete, but is continuous, where pyramidal cells are members of many columns. In general, cortical structure can be modeled in a physiologically plausible manner as being locally homogeneous (in short range intracortical connectivity) and heterogeneous (in long range corticocortical and corticothalamic connectivity)~\cite{Jirsa2009}~\cite{Qubbaj2007}. Locally, each column is thought be connected via symmetric short range local excitation, with surround inhibition~\cite{Braitenberg1998}. For example, this structural organization is most studied in the visual system, where the surrounding inhibition effectively tunes a cortical column to a particular receptive visual field~\cite{Sullivan2006}.

Recent studies using neural field models have demonstrated the theoretical implications of specific connectivity structure of the cortical columns in neural field models, where the connectivity kernel governs the bifurcation points of the system~\cite{Hutt2005} and types of oscillations that can be generated~\cite{Schmidt2009}. This implies that if we could estimate the connectivity structure for an individual, then we could capture the essential patient specific neurodynamics that lead to various oscillatory states. Estimating functional cortical connectivity via EEG measures is currently a highly active area of research. There is a lot of interest in understanding the hierarchy of brain regions that are involved in specific tasks, motivating the use of techniques like DCM. Other comparable methods are based on auto-regressive (AR) modeling of the EEG, MEG, or fMRI times series. Studies have concentrated on finding functional connectivity patterns using information contained in the AR coefficients using techniques like Granger causality~\cite{Hesse2003}, the direct transfer function~\cite{Kaminski1991}, and partial directed coherence~\cite{Sameshima1999}. Again, like the DCM approach, these methods estimate long range connectivity patterns that does not provide a clear relationship between the continuum field models and data.
%Analysis of the connectivity estimates provided from these approaches has involved graph theoretical techniques (small worldedness).

Until now, estimation of local intracortical connectivity structure has not been attempted. Recently, it has been shown that it is possible to estimate local coupling of systems governed by integro-difference equations by formulating a state-space model~\cite{Dewar2009}. The key development in this work was to define a state-space model with an order independent of the number of observations (or ECoG recording electrodes in this case). In addition, the appropriate model selection tools have been developed~\cite{Scerri2009} allowing for the application of this technique to neural fields. Modeling the neural dynamics within this framework has a distinct advantage over AR models, such that the number of parameters to define the connectivity basis functions (three in the paper) is considerably smaller than the number of AR coefficients typically required to achieve the relevant information criteria (AIC or BIC). In this paper, we demonstrate for the first time how intracortical connectivity can be inferred from ECoG data, based on a variant of the  Wilson and Cowan neural field model~\cite{Wilson1973}. This work provides a fundamental link between the theoretical advances in neural field modeling and patient specific data.
%Long range connectivity can be estimated using DTI~\cite{Knock2009}, however this short range connectivity can not..

\section{Neural Field Model}
In this section, we describe a variant of the  Wilson and Cowan neural field model~\cite{Wilson1973} that will be used to create our intracortical connectivity estimator. This model is descriptive of a range of neurodynamics of the cortex such as evoked potentials, visual hallucinations, and epileptic behaviour. 
\begin{todo}
	{Really need to add some citations here.}
\end{todo}
It is also capable of generating complex patterns of activity such as Turing patterns, spirals, and traveling oscillations. 
\begin{todo}
	{Same here.}
\end{todo}
The neural field model is popular due to being parsimonious yet having a strong link with the underlying physiology. The model relates the average number of action potentials $g(\mathbf{r},t)$ arriving at position $r$ to the local post synaptic membrane voltage $v(\mathbf{r},t)$. The post-synaptic potentials generated at a neuronal population at location $\mathbf{r}$ by action potentials arriving from all other connected populations at locations $\mathbf{r}'$ can be described by
\begin{equation}\label{SpikesToPotential}
	v\left( {\mathbf{r},t} \right) = \int_{ - \infty }^t {h\left( {t - t'} \right)g\left( {\mathbf{r},t'} \right)dt'}.
\end{equation}
The post-synaptic response kernel $h(t)$ is described by
\begin{equation}\label{SynapticRespKernel}
	h(t) = \eta(t)\exp{\left(-\alpha t\right)}.
\end{equation}
where $\alpha=\tau^{-1}$ and $\tau$ is the synaptic time constant and $\eta(t)$ is the Heaviside step function. Nonlocal interactions between cortical populations are described by	
\begin{equation}\label{RateBasedInteractions}
	g\left( \mathbf{r},t \right) = \int_\Omega  {w\left( \mathbf{r},\mathbf{r}' \right)f\left( v\left( \mathbf{r}',t \right) \right)d\mathbf{r}'} + u\left(\mathbf{r},t\right),
\end{equation}
where $f(\cdot)$ is the firing rate function, $w(\cdot)$ is the spatial connectivity kernel, and $\Omega$ is the spatial domain, representing a cortical sheet or surface. The term $u\left(\mathbf{r},t\right)$ represents a known external input. The connectivity kernel is typically a ``Mexican hat'' function \todo{which describes strong local activation, weak mid-range repression and weak long-range activation \cite{}}. The exact shape of this kernel is assumed to vary across patients, and hence needs to be inferred from data. 
% \begin{eqnarray}\label{Connectivity Kernel}
% 	w\left(r,r'\right) =&& \\
% &&\boldsymbol{\theta}_1\exp{\left(-\frac{(r-r')^\top(r-r')}{\sigma_1^2}\right)}+\boldsymbol{\theta}_2\exp{\left(-\frac{(r-r')^\top(r-r')}{\sigma_2^2}\right)}+\boldsymbol{\theta}_3\exp{\left(-\frac{(r-r')^\top(r-r')}{\sigma_3^2}\right)}\nonumber
% \end{eqnarray}
% where $\sigma_i$ is the scale parameter.
% % where
% % \begin{equation}
% % 	\boldsymbol{\theta}=\begin{bmatrix} \boldsymbol{\theta}_{1}\\\boldsymbol{\theta}_{2}\\\boldsymbol{\theta}_{3}\end{bmatrix},\quad 		\Sigma_w=\begin{bmatrix}\Sigma_{w1}\\\Sigma_{w2}\\ \Sigma_{w3}\end{bmatrix}.
% % \end{equation}
% \begin{todo}
% 	{Parham ... how are you writing this? DF: I have tried to make it better, but Parham can you check.Parham: I have changed it but I think it's better to include it in the simulation section}
% \end{todo}
The firing rate of the presynaptic neurons is related to the postsynaptic membrane potential by the sigmoidal activation function
\begin{equation}\label{ActivationFunction}
	f\left( v\left( \mathbf{r}', t \right) \right) = \frac{\nu _0\left(\mathbf{r}\right)}{1 + \exp \left( \beta \left( v_0\left( \mathbf{r} \right) - v\left(\mathbf{r}',t\right) \right) \right)}.
\end{equation}
%Even though we will model $v_0$ and $\nu_0$ as being homogeneous across the cortical sheet we will express it as being spatially dependant allowing for generality at this point. 
By substituting \ref{RateBasedInteractions} into \ref{SpikesToPotential}  we get the spatiotemporal model
\begin{equation}\label{FullDoubleIntModel}
	v\left(\mathbf{r},t\right) = \int_{-\infty}^t h\left(t - t'\right) \left(\int_\Omega   w\left(\mathbf{r},\mathbf{r}'\right) f\left( v\left( \mathbf{r}',t \right)\right)d\mathbf{r}' + u(\mathbf{r},t) \right)dt'.
\end{equation}
% The rearrange the model into an integro-differential we take the Laplace transform of~\ref{FullDoubleIntModel} giving
% \begin{equation}
% 	V(s) = H(s)G(s).
% \end{equation}
% Since $H(s) = (s+\alpha)^{-1}$ we can write
% \begin{equation}
% 	(s+\alpha)V(s) = G(s).
% \end{equation}
To arrive at the final form of the model we shall state the synaptic response kernel as a Green's function
\begin{equation}\label{GreensFuncDef}
	Lh\left( t \right) = \delta \left( t \right),
\end{equation}
\todo{Is the above right? I can't think of any way to make $\frac{d}{dt} h(t) + \alpha h(t)$ equal to $\delta(t)$}
where $L=\frac{d}{dt} + \alpha$ is a temporal differential operator and $\delta(t)$ is the Dirac-delta function giving 
\begin{equation}\label{FinalFormContinuous}
	\frac{dv\left( \mathbf{r},t \right)}{dt} + \alpha v\left( \mathbf{r},t \right) = \int_\Omega  {w\left( \mathbf{r},\mathbf{r}' \right)f\left( {v\left( \mathbf{r}',t \right)} \right)dr'} + u\left(\mathbf{r},t\right).
\end{equation}
To arrive at the integro-difference equation (IDE) form of the model we discretize time using a first-order Euler method (see Appendix~\ref{Time Discretization}) giving
\begin{equation}\label{DiscreteTimeModel}
	v_{t+1}\left(\mathbf{r}\right) = \lambda v_t\left(\mathbf{r}\right) + T \int_\Omega { w\left(\mathbf{r}-\mathbf{r}'\right) f\left(v_t\left(\mathbf{r}'\right)\right) d\mathbf{r}'} + T u_t\left(\mathbf{r}\right) + e_t\left(\mathbf{r}\right),
\end{equation} 
where $T$ is the time step, $\lambda = 1-\alpha T$ and $e_t(\mathbf{r}) \sim \mathcal{GP}\left(0,\Sigma\left(\mathbf{r}-\mathbf{r}'\right)\right)$ 
is an iid spatially colored, temporally white disturbance term. This term was added to drive the system and account for uncertainty in the model. To generate iEEG/LFP data we used the output function 
\begin{equation}
	\mathbf{y}_t = \int_{\Omega}{m\left(\mathbf{r}_n-\mathbf{r}'\right)v_t\left(\mathbf{r}'\right)d\mathbf{r}'} + \boldsymbol{\varepsilon}_t,
\end{equation}
where $\mathbf{r}_n$ defines the location of the sensors in the field where $n=1,...,N$ indexes the sensors and $\varepsilon_t \sim \mathcal{N}\left(0,\Sigma_y\right)$. The output kernel $m(\mathbf{r}-\mathbf{r}')$ governs the geometry of pick-up range of sensors where
\begin{equation}
	m\left(\mathbf{r}-\mathbf{r}'\right) = \exp{\left(-\frac{(\mathbf{r}-\mathbf{r}')^\top(\mathbf{r}-\mathbf{r}')}{\sigma_m^2}\right)}.
\end{equation}

% \section{Generating Data}
% To generate iEEG/LFP data we proceeded with the assumption of spatial homogeneity for the connectivity and activation function parameters, implying $w(\mathbf{r},\mathbf{r}') = w\left(\mathbf{r}-\mathbf{r}'\right)$, $\nu_0(\mathbf{r}) = \nu_0$ and $v_0(\mathbf{r}) = v_0$. In addition we assume the synaptic transmission delay is negligible. The model was discretized in time   and a stochastic 
% 
% The IDE formulation in Eq.~\ref{DiscreteTimeModel} was solved using a numerical method described in Appendix~\ref{Space Discretization}. 
% All the model parameters are described in Table 1.
% 
% \begin{tabular}{|c|c|c|}
% \hline
% Parameter & Value & Reference\\
% \hline
% $\alpha$ & $100$ s$^{-1}$ &\\
% \hline
% $\nu_0$ & $1$ s$^{-1}$ &\\
% \hline
% $v_0$ & $6$ mV &\\
% \hline
% $\beta$ & $0.56$ spikes/mV$^{-1}$ & \\
% \hline
% $T$ & $1\times10^{-4}$ s &\\
% \hline
% $\Delta$ & $1$ &\\
% \hline
% $\mathbf{\theta}_1$ &  &\\
% \hline
% $\mathbf{\theta}_2$ &  &\\
% \hline
% $\mathbf{\theta}_3$ &  &\\
% \hline
% $\sigma_{w1}$ &  &\\
% \hline
% $\sigma_{w2}$ &  &\\
% \hline
% $\sigma_{w3}$ &  &\\
% \hline
% $\sigma_m$ &  &\\
% \hline
% \end{tabular}

\section{Derivation of Finite Dimensional State-Space Model}
In order to implement standard estimation techniques we use a decomposition of the field using a set of Gaussian basis functions. Decomposition allows a continuous field to be represented by a finite dimensional state vector. This allows for the application of standard nonlinear, non-Gaussian state estimation methods such as the unscented Kalman filter. The field decomposition is described by
\begin{equation}\label{DefFieldDecomp}
	v_t\left(\mathbf{r}\right) \approx \boldsymbol{\phi}^{\top}\left(\mathbf{r}\right) \mathbf{x}_t,
\end{equation}
where $\mathbf{\boldsymbol{\phi}}(\mathbf{r})$ is a vector of Gaussian basis functions that are scaled by the state vector, $\mathbf{x}_t$. The width and positioning of the basis function can be determined by spectral analysis explain in detail in Section~\ref{SpectralAnalysisSection}. The connectivity kernel can also be considered a decomposition into a set of basis functions in 
\begin{equation}\label{DefKernelDecomp}
	w\left(\mathbf{r}-\mathbf{r}'\right) = \boldsymbol{\theta}^{\top} \boldsymbol{\psi}\left(\mathbf{r}-\mathbf{r}'\right).
\end{equation}
We will assume we know the parametric form of the connectivity basis functions, where the parameter $\boldsymbol{\theta}$ is unknown. Each connectivity basis function can be considered a layer in the Wilson and Cowan model, representing short range excitation, surround inhibition and mid-range excitation. Making substitutions of~\ref{DefFieldDecomp} and~\ref{DefKernelDecomp} into~\ref{DiscreteTimeModel} we get
\begin{equation}\label{reduced continuous model}
	\boldsymbol{\phi}^{\top}\left( \mathbf{r} \right) \mathbf{x}_{t+1} = T\boldsymbol{\theta}^{\top}\int_\Omega  {\boldsymbol{\psi} \left(\mathbf{r}-\mathbf{r}'\right) f\left( \boldsymbol{\phi} ^{\top}\left(\mathbf{r}'\right)\mathbf{x}_t \right)d\mathbf{r}'}  - \lambda\boldsymbol{\phi}^{\top}\left(\mathbf{r}\right)x_t + T u_t\left(\mathbf{r}\right) + e_t\left(\mathbf{r}\right).
\end{equation}
By defining the term
\begin{equation}\label{DefGamma}
	\boldsymbol{\Gamma}  = \int_\Omega  {\boldsymbol{\phi} \left(\mathbf{r}\right)\boldsymbol{\phi} ^{\top}\left(\mathbf{r}\right)d\mathbf{r}}
\end{equation}
and manipulating the model (see Appendix~\ref{Simplifying Decomposition}) we isolate the state vector to form a state-space model
\begin{eqnarray}\label{Homogeneous SS Model}
	\mathbf{x}_t &=& q(\mathbf{x}_t)\boldsymbol{\theta} - \lambda\mathbf{x}_t + k\left(u_t\left(\mathbf{r}\right)\right) + \mathbf{e}_t \\
	q\left(\mathbf{x}_t\right) &=& T\boldsymbol{\Gamma}^{ - 1}\int_\Omega {\boldsymbol{\phi}\left(\mathbf{r}\right) \int_\Omega {f\left(\boldsymbol{\phi}^{\top}\left(\mathbf{r}'\right)\mathbf{x}_t\right)\boldsymbol{\psi}^{\top} \left(\mathbf{r}-\mathbf{r}'\right)d\mathbf{r}'} d\mathbf{r}} \\
	k(u_t(\mathbf{r})) &=& \boldsymbol{\Gamma}^{-1}T \int_\Omega{\boldsymbol{\phi} \left(\mathbf{r}\right) u_t\left(\mathbf{r}\right)d\mathbf{r}} \\
	\mathbf{e}_t &=& \boldsymbol{\Gamma}^{-1}\int_\Omega{\boldsymbol{\phi}\left(\mathbf{r}\right)e_t\left(\mathbf{r}\right)d\mathbf{r}},
\end{eqnarray}
where $\mathbf{e}_t$ is a zero-mean normal white noise term with the covariance (see Appendix D)
\begin{equation} 
\boldsymbol{\Sigma}_w=\boldsymbol{\Gamma}^{-1}\iint\limits_{\Omega}\boldsymbol{\phi}\left(\mathbf r\right) \rho\left(\mathbf{r}- \mathbf{r}' \right)\boldsymbol{\phi}\left(\mathbf{r}'\right)^{\top}d\mathbf{r}' d\mathbf{r}\boldsymbol{\Gamma}^{- \top} 
\end{equation}
\begin{equation}
 cov(\mathbf{e}_t,\mathbf{e}_{t+\tau})=
\begin{cases}
\rho\left(\mathbf{r}-\mathbf{r'}\right), & \tau=0 \\
0 & \mathrm{otherwise}.
\end{cases}
\label{eq:FieldCovariance}
\end{equation}
The observation equation of the reduced model is
\begin{equation}\label{ObservationEquation}
    \mathbf{y}_t = \mathbf{C}\mathbf{x}_t + \boldsymbol{\varepsilon}_t \\
\end{equation}
where the observation matrix is
\begin{equation}
\mathbf C=\begin{bmatrix}\int_{\Omega}m(\mathbf r_1 - \mathbf
r')\boldsymbol \phi_1(\mathbf r')d\mathbf r'&\int_{\Omega}m(\mathbf r_1
- \mathbf r')\boldsymbol \phi_2(\mathbf r')d\mathbf r'& \hdots
&\int_{\Omega}m(\mathbf r_1 - \mathbf r')\boldsymbol \phi_n(\mathbf
r')d\mathbf r' \\
\int_{\Omega}m(\mathbf r_2 - \mathbf r')\boldsymbol \phi_1(\mathbf
r')d\mathbf r'&\int_{\Omega}m(\mathbf r_2 - \mathbf r')\boldsymbol
\phi_2(\mathbf r')d\mathbf r'& \hdots &\int_{\Omega}m(\mathbf r_2 -
\mathbf r')\boldsymbol \phi_n(\mathbf r')d\mathbf r'\\
\vdots&\vdots&\ddots&\vdots\\
\int_{\Omega}m(\mathbf r_{n_y} - \mathbf r')\boldsymbol \phi_1(\mathbf
r')d\mathbf r'&\int_{\Omega}m(\mathbf r_{n_y} - \mathbf r')\boldsymbol
\phi_2(\mathbf r')d\mathbf r'& \hdots &\int_{\Omega}m(\mathbf r_{n_y} -
\mathbf r')\boldsymbol \phi_n(\mathbf r')d\mathbf r'
\end{bmatrix},
\end{equation}
and $m(\mathbf r_n - \mathbf{r}')$ is the observation kernel.

\section{Spectral Analysis and Decomposition}\label{SpectralAnalysisSection}

Spectral analysis has been utilized to identify the number of sensors and basis functions required to accurately represent dynamic spatial fields from sampled observations~\cite{Sanner1992,Scerri2009}. Based on the higher dimension extensions of Shannon's sampling theorem~\cite{Peterson1962}, the spatial bandwidth of the observed field provides a lower bound on both the number of sensors and the number of basis functions required.  

The spectral characteristics of the field are governed by the the shape of the connectivity kernel or the coloring of the covariance of the field disturbance. If the bandwidth of the disturbance covariance was greater than the bandwidth of the connectivity kernel than the disturbance governs the bandwidth of the system. Conversely, if the bandwidth of the kernel was higher then it will govern the systems bandwidth. Note, the theoretically observable characteristics of the kernel are governed by how it is excited by an input. 
 
Considering the spectral representation of the dynamic field at time $t$ denoted by $W_t(\boldsymbol{\nu})$, multi-dimensional sampling and reconstruction theory dictates that for and accurate representation, $W_t(\boldsymbol{\nu})$ need to be spatially bandlimited. Such a condition is usually satisfied due to the bandlimiting effect of most practical kernels and the spatially colored covariance functions \todo{~cite\{some eeg literature here\}}. In addition, the Gaussian pick-up of the sensors provides a low-pass filtering effect, further limiting the bandwidth of the field. Thus the spectral response $W_t(\boldsymbol{\nu})$ is usually band-limited with $W_t(\boldsymbol{\nu}) \approx 0 ~ \forall \boldsymbol{\nu} > \boldsymbol{\nu}_c$, where $\boldsymbol{\nu}_c$ is a cutoff frequency. Note that for the 3-dimensional, homogeneous and isotopic fields being considered $\boldsymbol{\nu}_c = [\nu_c ~ \nu_c ~ \nu_c]^\top$. Given such a band-limited field and based on Shannon's sampling theorem, the distance between adjacent sensors needs to within $\frac{1}{2\nu_c}$. Satisfying this condition ensures that the hidden dynamic field $w_t(\mathbf{r})$ can be inferred from the sampled observation $\mathbf{y}(t)$.

There are two options available to alleviate problems with aliasing. The first is to increase the spatial sampling frequency (using a higher resolution of sensors), and the second is to use wider sensors, which act like an anti-aliasing filter. The first option is more desirable, since more information is retained. In addition, if sensors are modelled as having a Gaussian pickup range than the slope of the attenuation will be gentle allowing for some low amplitude high frequency components to still be present and mild aliasing will still occur.  Mild aliasing results in distortions in the high frequency components of the field dynamics. The basis function decomposition the high frequency components are usually lost.

Similar considerations need to be made regarding the representation of the dynamic field $w_t(\mathbf{r})$ by its basis functions decompositions. The minimum distance between adjacent basis needs to again satisfy Shannon's sampling theorem. Thus the minimum distance between basis must again satisfy $\frac{1}{2\nu_c}$. Note that, since the dynamic field is frequently spatially oversampled by using a larger number of sensor then imposed by frequency considerations, the distance between basis functions is usually larger then the distance between sensors. Note also that since a limited spatial region and since basis functions placed on a regular square grid are being considered, knowledge of the distance between basis functions directly implies the total number of basis functions required. 

For the Gaussian basis functions $\phi(\mathbf{r})$ being considered, the basis width parameter $\sigma^2$ can also be inferred from spectral considerations \cite{Sanner1992,Scerri2009}. The Fourier transform of a Gaussian is another Gaussian with width $\sigma^2_{\nu}$ given by $\sigma^2_{\nu} = \frac{1}{\pi^2\sigma^2}$. Thus to ensure that the basis functions can represent fields with frequency content up to $\boldsymbol{\nu}_c$, $\sigma_{\nu}$ should be set with the 3dB frequency for the basis functions equal to $\boldsymbol{\nu}_c$, thus obtained $\sigma^2_{\nu}$ and therefore $\sigma^2$. 

\section{State and Parameter Estimation}
The unscented Kalman filter was used for the state estimation (reference the Julier and Uhlmann paper). This method is appropriate for state estimation of stochastic nonlinear dynamical systems. Alternatives to this approach are the extended Kalman filter (ref?) (EKF) and sequential Monte Carlo (SMC) filtering (ref?). The extended Kalman filter approximates the state transition equation by linearizing about the fixed points (by calculating the Jacobian) at each time step. The linearization maintains the Gaussianality in the model. The unscented Kalman filter approximates the \emph{a posteriori} state density by a Gaussian distribution, whilst maintaining the nonlinearity in the system. This has been shown to give superior performance over the EKF in state estimation, as the EKF maintains a first order approximation where the UKF provides a second order approximation. In addition, the computational requirements of the UKF are smaller than the EKF, since the EKF computes the Jacobian at each time step, where the UKF does not. SMC filtering (particle filtering) can theoretically provide an exact \emph{a posteriori} state density for nonlinear systems. However, this method is not appropriate for our problem, since it is not feasible for high dimensional systems, due to computation demands.

The approximation of the state density is made by propagating the so called sigma points through the state-space model. The sigma points describe the mean and covariance of the state estimate. The accuracy of the approximation is linked to the time step used in the temporal discretization of the system, where the state transition appears more linear with a finer resolution. To explain the operation of the UKF we begin by deriving the traditional Kalman filter with the state-space model
\begin{eqnarray}
	\mathbf{x}_t = q(\mathbf{x}_t)\boldsymbol{\theta} - \lambda\mathbf{x}_t + k\left(u_t\left(\mathbf{r}\right)\right) + \mathbf{e}_t \\
	\mathbf{y}_t = \mathbf{C}\mathbf{x}_t + \boldsymbol{\varepsilon}_t,
\end{eqnarray}
where $\boldsymbol{\theta}$ and $\lambda$ are the systems parameters. 

\section{Comparison of Estimated Parameters and True Parameters}

\section{Discussion}
\begin{itemize}

	\item Discuss the novelty of using the reduced model (BrainIDE) for creating patient specific models. 
	\item The size of the dentritic arbour is consistent in the cortex.
	\item Spatial frequency of field and the relationship to the required density of electrodes/sensors required for estimation or to capture the required information without aliasing.
	
	\item The flexibility of the approach
	
	\item Combining this method with other to incorporate longer range hetrogeneous connectivity.
	
	\item The need for activity to perform estimation.
\end{itemize}

\section{Conclusion}

\appendices

\section{Discrete Time Model}\label{Time Discretization}
To form the IDE neural field model a time discretization must be perform. We used a one-step Euler method where Eq.~\ref{FinalForm1} can be approximated by
\begin{equation}\label{Euler Approximation}	
\frac{v\left( \mathbf{r},t+T \right) - v\left( \mathbf{r},t\right)}{T} =   -\alpha v\left( \mathbf{r},t \right) + \int_\Omega  {w\left( \mathbf{r}-\mathbf{r}' \right)f\left( {v\left( \mathbf{r}',t \right)} \right)d\mathbf{r}'} + u\left(\mathbf{r},t\right).
\end{equation}
For clarity, we shall index time points in the discrete time form of the model using the subscript $t$ and the next time point as $t+1$. Rearranging Eq.~\ref{Euler Approximation} we get
\begin{eqnarray}\label{Euler Approximation}	
v_{t+1}\left( \mathbf{r}\right) &=& v_t\left( \mathbf{r}\right) -T \alpha v_t\left( \mathbf{r}\right) + T \int_\Omega  {w\left( \mathbf{r}-\mathbf{r}' \right)f\left( {v_t\left( \mathbf{r}'\right)} \right)d\mathbf{r}'} + T u_t\left(\mathbf{r}\right).
\end{eqnarray}
The discrete time form of the model is
\begin{equation}\label{Discrete Time Model1}
	v_{t+1}\left(\mathbf{r}\right) = \lambda v_t\left(\mathbf{r}\right) + T \int_\Omega { w\left(\mathbf{r}-\mathbf{r}'\right) f\left(v_t\left(\mathbf{r}'\right)\right) d\mathbf{r}'} + T u_t\left(\mathbf{r}\right),
\end{equation}
where $\lambda = 1 - T \alpha$.

\section{Numerical Simulation of IDE Model}\label{Space Discretization}
To solve the intregro-difference equation in Eq.~\ref{Discrete Time Model1} we define the spatial aspect of the model on a regular square $i,j$ grid of neural masses, where the spatial step size $\Delta \mathbf{r}_i = \Delta \mathbf{r}_j = \Delta $ giving
\begin{equation}\label{discrete space}
	v_{t+1}\left(\mathbf{r}_{ij}\right) = \lambda v_t\left(\mathbf{r}_{ij}\right) + T \Delta^2 \sum_{i=1}^{i=I}{ \sum_{j=1}^{j=J}{ w\left( \mathbf{r}-\mathbf{r}_{ij}' \right)f\left( v_t\left( \mathbf{r}_{ij}'\right) \right)} } + T u_t\left(\mathbf{r}\right) + e_t(\mathbf{r}_{i,j}),
\end{equation}
where $e_t(\mathbf{r}_{i,j}) \sim \mathcal{N}\left(0,\Sigma\right)$.
%  and $\tilde{w}(r_{ij}-r_{ij}'|)$ discretized connectivity kernel described by
% \begin{equation}\label{Discrete Connectivity Kernel}
% 	\tilde{w}(r_{ij},r_{ij}') = \boldsymbol{\theta}^{\top} e^{\frac{\left(\left(r_i'-r_i\right)\Delta\right)^2}{(2\sigma)^2} + \frac{\left(\left(r_j'-r_j\right)\Delta\right)^2}{(2\sigma)^2}}.
% \end{equation}
% The observation equation and kernel were discretized in the same way by
% \begin{equation}\label{Discrete Observation Equation}
%     y_k\left(r_{i_n j_n}\right) = \Delta^2\sum_{i=1}^{i=I}\sum_{j=1}^{j=J} \tilde{m}( |r_{i_n j_n}-r_{ij}'| ) v_k\left(r_{ij}'\right)  + \varepsilon_k( r_{i_n j_n})
% \end{equation}
% where
% \begin{equation}\label{Discrete Observation Kernel}
% 	\tilde{m}(|r_{ij}-r_{ij}'|) = e^{\frac{\left(\left(r_i'-r_i\right)\Delta\right)^2}{(2\sigma)^2} + \frac{\left(\left(r_j'-r_j\right)\Delta\right)^2}{(2\sigma)^2}}.
% \end{equation}
We have used periodic boundary conditions to alleviate problems associated with edge effects.

\section{Reduction to Finite State-Space Model}\label{Simplifying Decomposition}
Starting from Eq.~\ref{reduced continuous model} we multiply throughout by $\boldsymbol{\phi}(r)$ and integrate over the spatial domain $\Omega$ to get
\begin{eqnarray}\label{Poo}
	\int_\Omega  {\boldsymbol{\phi} \left(\mathbf{r}\right)\boldsymbol{\phi}^{\top}\left(\mathbf{r}\right) d\mathbf{r}} \mathbf{x}_{t+1} = T \int_\Omega  {\boldsymbol{\phi} (\mathbf{r}) \boldsymbol{\theta}^{\top} \int_\Omega  {\boldsymbol{\psi}  \left(\mathbf{r}-\mathbf{r}'\right) f\left(\boldsymbol{\phi}^{\top}\left(\mathbf{r}'\right) \mathbf{x}_t \right)d\mathbf{r}'}d\mathbf{r}} - \lambda\int_\Omega {\boldsymbol{\phi}(\mathbf{r})\boldsymbol{\phi}^{\top}(\mathbf{r})d\mathbf{r}} \mathbf{x}_t \\
	+ T \int_\Omega{\boldsymbol{\phi} \left(\mathbf{r}\right) u_t\left(\mathbf{r}\right)d\mathbf{r}} + \int_\Omega{\boldsymbol{\phi} \left(\mathbf{r}\right) e_t\left(\mathbf{r}\right)d\mathbf{r}}.
\end{eqnarray}
Substituting Eq.~\ref{DefGamma} into Eq.~\ref{Poo} and cross-multiplying by $\boldsymbol{\Gamma}^{-1}$ gives 
\begin{eqnarray}\label{Homogeneous SS Model}
	\mathbf{x}_{t+1} = T\boldsymbol{\Gamma}^{ - 1}\int_\Omega {\boldsymbol{\phi}\left(\mathbf{r}\right) \int_\Omega {f\left(\boldsymbol{\phi}^{\top}\left(\mathbf{r}'\right)\mathbf{x}_t\right) \boldsymbol{\psi}^{\top} \left(\mathbf{r}-\mathbf{r}'\right)d\mathbf{r}'} d\mathbf{r}} \boldsymbol{\theta} - \lambda \mathbf{x}_t \\
	+ \boldsymbol{\Gamma}^{-1}T \int_\Omega{\boldsymbol{\phi} \left(\mathbf{r}\right) u_t\left(\mathbf{r}\right)d\mathbf{r}} + \boldsymbol{\Gamma}^{-1} \int_\Omega{\boldsymbol{\phi}\left(\mathbf{r}\right)e_t\left(\mathbf{r}\right)d\mathbf{r}}.
\end{eqnarray}

\section{Colored noise}
We have
\begin{equation}
v_{t + 1}\left( \mathbf{r} \right)  = T\int_\Omega  {k\left( {\mathbf{r}} , {\mathbf{r}'} \right)f\left( {{v_t}\left( \mathbf{r'} \right)} \right)d{\mathbf{r'}}} + \lambda v_t\left( {\mathbf{r}} \right) +e_t\left( {\mathbf{r}} \right).
\end{equation}
where the disturbance $e_t(\mathbf{r})$ is a zero-mean normally distributed noise process, spatially coloured but temporally white, with covariance 

\begin{equation}
cov(e_t(\mathbf{r}),e_{t+\tau}(\mathbf{r'}))=
\begin{cases}
\beta\left(\mathbf{r}-\mathbf{r'}\right), & \tau=0 \\
0 & \mathrm{otherwise}
\end{cases}
\label{eq:FieldCovariance}
\end{equation}
The final state-space model is given by
\begin{equation}
\mathbf{x}_{t + 1} = T q(\mathbf{x}_t)\boldsymbol{\theta} -\lambda \mathbf{x}_t + \boldsymbol{\Gamma}^{-1}\int_\Omega  {\boldsymbol{\phi} ( \mathbf{r} )e_t( \mathbf{r} )d\mathbf{r}}
\end{equation}
then the covariance of the noise term is given by
\begin{eqnarray}
\mathbf{\Sigma}_w&=&\mathbf{\Gamma}^{-1}\mathbf E[\int_{\Omega}\boldsymbol{\phi}\left(\mathbf{r}\right)e_t\left(\mathbf{r}\right)d\mathbf{r} \int_{\Omega}\boldsymbol{\phi}\left(\mathbf{r}'\right)^{\top} e_t\left(\mathbf{r}'\right)d\mathbf{r}']\mathbf{\Gamma}^{- \top} \nonumber \\
&=&\mathbf{\Gamma}^{-1}\iint\limits_{\Omega} \boldsymbol{\phi}\left(\mathbf{r}\right) \mathbf E[e_t\left(\mathbf{r}\right)e_t\left(\mathbf{r}'\right)]\boldsymbol{\phi}\left(\mathbf{r}'\right)^{\top}d\mathbf{r}' d\mathbf r\mathbf{\Gamma}^{- \top} \nonumber\\
&=&\mathbf{\Gamma}^{-1}\iint\limits_{\Omega}\boldsymbol{\phi}\left(\mathbf r\right) \beta\left(\mathbf r- \mathbf r' \right)\boldsymbol{\phi}\left(\mathbf r'\right)^{\top}d\mathbf r' d\mathbf r\mathbf{\Gamma}^{- \top} 
\end{eqnarray}

\section{Parameter estimation using LS}
\begin{equation}
 \mathbf x_{t+1}=\mathbf{x}_{t}+T_s \mathbf{Q}(\mathbf{x}_t)\boldsymbol{\theta}-\alpha T_s\mathbf{x}_t+T_s\Gamma^{-1}\int_\Omega\boldsymbol{\phi}(\mathbf r)\varepsilon_t(\mathbf r)d\mathbf r
\label{eq:statespacemodel}
\end{equation}
where the matrix $\mathbf Q_{n_x \times n_{\boldsymbol{\theta}}}(\mathbf x_t)$ is defined as
\begin{equation}
 Q_{n_x \times n_{\boldsymbol{\theta}}}=\Gamma^{-1}\int_\Omega\boldsymbol{\phi}(\mathbf r)\int_\Omega f(\boldsymbol{\phi}^\top(\mathbf r')\mathbf x_t)\mathbf{\psi}^\top(\mathbf r-\mathbf r')d\mathbf r'd\mathbf r
\end{equation}
and $T_s$ is the sampling period. Least Square method can be used as (\ref{eq:statespacemodel}) is linear in parameters. At each time instant we have

\begin{eqnarray}
 \mathbf x_{1}&=&\mathbf x_{0}+T_s \mathbf Q(\mathbf x_0) \boldsymbol{\theta}-\alpha T_s\mathbf x_0+T_s\mathbf e_0 \nonumber \\
 \mathbf x_{2}&=&\mathbf x_{1}+T_s \mathbf Q(\mathbf x_1) \boldsymbol{\theta}-\alpha T_s\mathbf x_1+T_s\mathbf e_1\nonumber\\
&\vdots& \nonumber\\
 \mathbf x_{n}&=&\mathbf x_{n-1}+T_s \mathbf Q(\mathbf x_{n-1}) \boldsymbol{\theta}-\alpha T_s\mathbf x_{n-1}+T_s\mathbf e_{n-1}
\end{eqnarray}
in matrix form
\begin{equation}
 \begin{bmatrix} \mathbf x_{1}\\\mathbf x_{2}\\\vdots\\\mathbf x_{n}\end{bmatrix}-\begin{bmatrix} \mathbf x_{0}\\\mathbf x_{1}\\\vdots\\\mathbf x_{n-1}\end{bmatrix}=T_s\begin{bmatrix}\mathbf Q(\mathbf x_0)&-\mathbf x_{0}\\\mathbf Q(\mathbf x_1)&-\mathbf x_{1}\\\vdots\\ \mathbf Q(\mathbf x_{n-1})&-\mathbf x_{n-1}\end{bmatrix}\begin{bmatrix} \boldsymbol{\theta} \\ \alpha\end{bmatrix}+T_s\begin{bmatrix}\mathbf e_0\\\mathbf e_1\\\vdots\\\mathbf e_{n-1}\end{bmatrix}
\end{equation}
or in compact form we have
\begin{equation}
 \mathbf Z=\mathbf X \mathbf W+\boldsymbol \xi
\end{equation}
where
\begin{equation}
 \mathbf Z=\begin{bmatrix} \mathbf x_{1}-\mathbf x_{0}\\\mathbf x_{2}-\mathbf x_{1}\\\vdots\\\mathbf x_{n}-\mathbf x_{n-1}\end{bmatrix},\quad \mathbf X=T_s\begin{bmatrix}\mathbf Q(\mathbf x_0)&-\mathbf x_{0}\\\mathbf Q(\mathbf x_1)&-\mathbf x_{1}\\\vdots\\ \mathbf Q(\mathbf x_{n-1})&-\mathbf x_{n-1}\end{bmatrix},\quad \mathbf W=\begin{bmatrix} \boldsymbol{\theta} \\ \alpha\end{bmatrix} \text{and}\quad \boldsymbol \xi=T_s\begin{bmatrix}\mathbf e_0\\\mathbf e_1\\\vdots\\\mathbf e_{n-1}\end{bmatrix}
\end{equation}
using the method of Least Square $ \mathbf W$ can be estimated
\begin{equation}
 \mathbf W\approx(\mathbf X^\top\mathbf X)^{-1}\mathbf X^\top\mathbf Z
\end{equation}
% use section* for acknowledgement

\section{clarification for the derivation} 
I ignore noise here. From equation (\ref{SpikesToPotential}) we can say $h$ is the foundamental solution to the equation
\begin{equation}
 Lv(r,t)=g(r,t)
\label{eq:LV}
\end{equation}
where $L$ is a temporal differential operator, since $h(t)$ is the foundamental solution to (\ref{eq:LV}) it can be written as a Green's function
\begin{equation}
 Lh(t)=\delta(t)
\end{equation}
where $L=1+\tau\frac{d}{dt}$ therefore equation (\ref{eq:LV}) becomes:
\begin{equation}
\tau\frac{dv(r,t)}{dt}+ v(r,t)=g(r,t)
\end{equation}
substituting (\ref{RateBasedInteractions}) for $g(r,t)$ gives
\begin{equation}
\frac{dv\left( {r,t} \right)}{dt} + \frac{1}{\tau} v\left( r,t \right) = \frac{1}{\tau}\int_\Omega  {w\left( r,r' \right)f\left( {v\left( r',\bar t \right)} \right)dr'} 
\end{equation}

% \section{Ken's Discretization}
% Spatio-temporal interactions can be described by the continuous space continuous time stochastic differential equation
% \begin{equation}\label{differential eq}	
% \tau dv\left( r,t \right) = \left( -v\left( r,t \right) + \int_\Omega  {w\left( r,r' \right)f\left( {v\left( r',t \right)} \right)dr'} \right) dt  + d e\left( r,t \right).
% \end{equation}
% where $e\left( r,t \right)$ is a zero mean temporal Wiener process with incremental covariance $\Sigma_e(r)dt$. (not too sure about this noise since it needs to be white temporally but coloured spatially - which in continuous space-time I have never seen written ... has anyone?)
% To simplify the notation, but without loss of generality, \ref{differential eq} can be rewritten as
% \begin{equation}\label{differential simplified}
% 	dv\left( r,t \right) = q\left(v(r,t) \right) dt  + d e\left( r,t \right)
% \end{equation}
% Had $q\left(v(r,t) \right)$ been a linear function in $v$, then \ref{differential simplified} could be rewritten as
% \begin{equation}\label{differential linear}
% 	dv\left( r,t \right) = Av(r,t)dt  + d e\left( r,t \right)
% \end{equation}
% Considering temporal sampling instances $\delta$ apart given by $\{t_k = 0,~1,~2, \ldots$ \}, then a solution to \ref{differential linear} at each sampling time is given by
% \begin{equation}\label{linear solution}
% 	v(r,t_k+\delta) = \exp\{A(\delta)\}x(t) + \int_{t_k}^{t_k+\delta}\exp\{A(t_k+\delta-\tau)\}de(r,\tau)
% \end{equation}
% Let the discrete time random variable $e'(r,t_k)$ be
% \begin{equation}\label{discrete noise}
% 	e'(r,t_k) = \int_{t_k}^{t_k+\delta}\exp\{A(t_k+\delta-\tau)\}de(r,\tau)
% \end{equation}
% then, $e'(r,t_k)$ is a zero mean, temporally white noise term with covariance
% \begin{equation}\label{covariance of noise}
% 	E\left[e'(r,t_k),e'^\top(r,t_k) \right] = \int_t^{t+\delta}\exp\{A(t_k+\delta-\tau)\} \Sigma_e(r) \exp\{A(t+\delta-\tau)\} d\tau
% \end{equation}
% 
% But we are not so luck as to be able to go from \ref{differential simplified} to \ref{differential linear} and thus I doubt if we can obtain an exact solution to \ref{differential simplified} without an approximation. Agreed?
% 
% A possible approximate solution is the one you have already used. Thus, interpreting the difference equation \ref{differential simplified} as an integral equation gives
% \begin{equation}\label{euler}
% 	v\left(r,t\right) =  v(r,t_0) + \int_{t_0}^{t} q\left(v(r,\tau) \right) d\tau  + \int_{t_0}^{t} d e\left(r,\tau\right)
% \end{equation}
% Using an Euler approximation, \ref{euler} can be approximated at each sampling time by
% \begin{equation}\label{euler approx 1}
% 	v\left(r,t_{k+1} \right) =  v(r,t_k) + q\left(v(r,t_k)\right) (t_{k+1} - t_k) + (e(r,t_{k+1}) - e(r,t_k))
% \end{equation}
% which can be rewritten as
% \begin{equation}\label{euler approx 2}
% 	v\left(r,t_k + \delta \right) =  v(r,t_k) + \delta q\left(v(r,t_k)\right) + e'(r,t_k)
% \end{equation}
% where $e'(r,t_k)$ is a zero mean Gaussian random variable with covariance $\delta\Sigma_e$. (Note that, because of the way the noise is defined in \ref{differential eq} it is not $\delta^2$ but $\delta$)

\section*{Acknowledgment}
The authors would like to thank...

\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% references section

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,BrainIDE}


% biography section
%
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{biography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

\begin{IEEEbiography}{Dean R. Freestone}
Biography text here.
\end{IEEEbiography}

% if you will not have a photo at all:
\begin{IEEEbiography}{Parham Aram}
Biography text here.
\end{IEEEbiography}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

\begin{IEEEbiography}{Michael Dewar}
Biography text here.
\end{IEEEbiography}

\begin{IEEEbiography}{Kenneth Scerri}
Biography text here.
\end{IEEEbiography}

\begin{IEEEbiography}{Visakan Kadirkamanathani}
Biography text here.
\end{IEEEbiography}

\begin{IEEEbiography}{Other Bosses}
Biography text here.
\end{IEEEbiography}

% that's all folks
\end{document} 