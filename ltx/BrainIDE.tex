%% bare_jrnl.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/


\documentclass[onecolumn,draftcls]{IEEEtran}

\usepackage{graphicx}
\usepackage{color}                    % For creating coloured text and background
\usepackage{amsmath,amssymb,amsfonts} % Typical maths resource packages
% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}
% TODO environment
\newcommand{\todo}[1]{\textsf{\emph{\textbf{\textcolor{blue}{#1}}}}}

\begin{document}

% can use linebreaks \\ within to get better formatting as desired
\title{Estimation of Intracortical Connectivity in a Dynamical Neural Field Model}

\author{Dean~R.~Freestone,~\IEEEmembership{Graduate Student Member,~IEEE,}
        Parham~Aram,~\IEEEmembership{Graduate Student Member,~IEEE,}
        Michael~Dewar,~\IEEEmembership{Member,~IEEE,}
        Kenneth~Scerri,~\IEEEmembership{Member,~IEEE,}
        Visakan~Kadirkamanathan,~\IEEEmembership{Member,~IEEE,}
        David~B.~Grayden,~\IEEEmembership{Member,~IEEE,}
        Anthony~N.~Burkitt,~\IEEEmembership{Member,~IEEE,}
        and~other~Bosses/Supervisors,~\IEEEmembership{Member,~IEEE.}% <-this % stops a space% <-this % stops a space

\thanks{D. Freestone is with the Department
of Electrical and Electronic Engineering, University of Melbourne, Melbourne,
Vic, 3010 Australia {\tt\small dfreestone@bionicear.org}.}% <-this % stops a space
\thanks{P. Aram is with Sheffield...}
\thanks{M. Dewar is with Edinburgh...}
\thanks{K. Scerri is with Malta...}
\thanks{V. Kadirkamanathan is with Sheffield...}

\thanks{Manuscript received Month Day, Year; revised Month Day, Year.}}


% The paper headers
\markboth{Journal of \"{U}ber C\~{o}\~{o}l Engineering,~Vol.~1, No.~1, Christmas~2009}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Journals}

\maketitle

\begin{abstract}
Will save this for last.
\end{abstract}


% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Integro-Difference Equation (IDE), Neural Field Model, Cortical Connectivity.
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle

\section{Introduction}
\IEEEPARstart{T}{he} human brain is arguably the world's most complex system. Approximately 100 billion neurons and 60 trillion synapses operate in concert to process information, resulting in the cognition that determines our behavior. The brain's efficiency, robustness, and adaptability are unparalleled by any man-made device and, despite decades of concerted research, our understanding of its complex dynamics remains modest. This has led researchers to study different scales of dynamics: starting from individual proteins, synapses, and neurons; moving through to neuronal networks and ensembles of neuronal networks. While our understanding of the function of neurons is well developed, the overall behavior of the brain's meso and macro-scale dynamics remains largely a mystery.  Understanding the brain at this level is extremely important since this is the scale where pathologies such as epilepsy, Parkinson's disease and schizophrenia are manifested.

To date, there has been a considerable volume of work in generating physiologically plausible neural field models to fill the void of understanding brain dynamics at the meso/macroscopic scale. Mathematical neural field  models provide insights into the underlying physics and dynamics of electroencephalography (EEG) and magnetoencephalography (MEG) (see~\cite{Deco2008}~\cite{David2003} for recent reviews). These models have demonstrated possible mechanisms for the genesis of neural rhythms (such as the alpha and gamma rhythms) \cite{Liley1999} \cite{RENNIE2000}, epileptic seizure generation \cite{DaSilva2003},~\cite{Suffczynski2004} and~\cite{Wendling2005} and insights into other pathologies~\cite{Moran2008} \cite{Schiff2009} that would be impossible to gain from experimental data alone. Unfortunately, to date the use of these models in the clinic has been limited, since the neural field model are constructed for a general brain dynamics and pathologies almost always have unique underlying patient specific causes. Data from EEG and functional magnetic resonance imaging (fMRI) offers the patient specific link to macro-scale cortical dynamics, making these tools readily applicable to the clinic. However the underlying system properties, or system states, are hidden from EEG and fMRI data, making predictions of the underlying physiology inherently difficult.

For models to be clinically viable they must be patient specific. A possible approach to achieve this would be to use a general neural field model, like the Wilson and Cohen~\cite{Wilson1973} model or a neural mass model like the Jansen and Ritt model~\cite{Jansen1995}, and fit it to a patients EEG data. Fitting the neural models to individuals is a highly non-trivial task, and until very recently this has not been reported in the literature. An estimation frame work for neural field model known as dynamical causal modeling (DCM)~\cite{David2003} \cite{David2006} has recently been proposed for studying evoked potential dynamics. Via a Bayesian inference scheme, DCM estimates the long range connectivity structure between the specific isolated brain regions that best explains a given data set. The interconnected brain regions are modeled by the Jansen and Ritt equations. The approach has proven very useful in understanding specific hierarchical networks of neural information processing. Another recent publication describing a parameter estimation method with a neural field model used an unscented Kalman filter with the Wilson-Cowan neural field equations~\cite{schiff2008kalman}.  This work takes a more system theoretic approach to the neural estimation problem and marks a first step in what has the potential to revolutionize the treatment of many neurological diseases where therapeutic electrical stimulation is viable.

Generating patient specific models allows the application of a range of techniques from control systems theory, where tailored electrical stimulation could be used therapeutically in a closed loop fashion.  Currently available epileptic seizure prediction and control devices (i.e., the vagal nerve stimulator) are implemented in an ``open loop".  That is, the therapeutic electrical stimulation waveforms are adjusted for each patient by trial and error, disregarding the patient's neurodynamics and information about their particular pathologies. Given access to an accurate model, the application of control theory in these circumstances would allow for robust therapeutic stimulations.
The work from Schiff and Sauer~\cite{schiff2008kalman} successfully demonstrated  it is possible to estimate parameters from the WC equations. This motivates the question, what parameters are the most important? How patient specific do the assumed parameters have to be for the model to be useful? In this paper, we address both of these questions and provide a theoretical platform to perform patient-specific grey-box modeling of the human neocortex.

\section{The Importance of Cortical Structure}
Neural field models use statistics of the cortex to relate mean firing rates of pre-synaptic neural populations to post-synaptic mean membrane potentials. Each neural population represents a functional cortical processing unit referred to as a column. The columnar organization of the cortex is not discrete, but is continuous, where pyramidal cells are members of many columns. In general, cortical structure can be modeled in a physiologically plausible manner as being locally homogeneous (in short range intracortical connectivity) and heterogeneous (in long range corticocortical and corticothalamic connectivity)~\cite{Jirsa2009}~\cite{Qubbaj2007}. Locally, each column is thought be connected via symmetric short range local excitation, with surround inhibition~\cite{Braitenberg1998}. For example, this structural organization is most studied in the visual system, where the surrounding inhibition effectively tunes a cortical column to a particular receptive visual field~\cite{Sullivan2006}.

Recent studies using neural field models have demonstrated the theoretical implications of specific connectivity structure of the cortical columns in neural field models, where the connectivity kernel governs the bifurcation points of the system~\cite{Hutt2005} and types of oscillations that can be generated~\cite{Schmidt2009}. This implies that if we could estimate the connectivity structure for an individual, then we could capture the essential patient specific neurodynamics that lead to various oscillatory states. Estimating functional cortical connectivity via EEG measures is currently a highly active area of research. There is a lot of interest in understanding the hierarchy of brain regions that are involved in specific tasks, motivating the use of techniques like DCM. Other comparable methods are based on auto-regressive (AR) modeling of the EEG, MEG, or fMRI times series. Studies have concentrated on finding functional connectivity patterns using information contained in the AR coefficients using techniques like Granger causality~\cite{Hesse2003}, the direct transfer function~\cite{Kaminski1991}, and partial directed coherence~\cite{Sameshima1999}. Again, like the DCM approach, these methods estimate long range connectivity patterns that does not provide a clear relationship between the continuum field models and data.
%Analysis of the connectivity estimates provided from these approaches has involved graph theoretical techniques (small worldedness).

Until now, estimation of local intracortical connectivity structure has not been attempted. Recently, it has been shown that it is possible to estimate local coupling of systems governed by integro-difference equations by formulating a state-space model~\cite{Dewar2009}. The key development in this work was to define a state-space model with an order independent of the number of observations (or ECoG recording electrodes in this case). In addition, the appropriate model selection tools have been developed~\cite{Scerri2009} allowing for the application of this technique to neural fields. Modeling the neural dynamics within this framework has a distinct advantage over AR models, such that the number of parameters to define the connectivity basis functions (three in the paper) is considerably smaller than the number of AR coefficients typically required to achieve the relevant information criteria (AIC or BIC). In this paper, we demonstrate for the first time how intracortical connectivity can be inferred from ECoG data, based on a variant of the  Wilson and Cowan neural field model~\cite{Wilson1973}. This work provides a fundamental link between the theoretical advances in neural field modeling and patient specific data.
%Long range connectivity can be estimated using DTI~\cite{Knock2009}, however this short range connectivity can not..

\section{Neural Field Model}
In this section, we describe a variant of the  Wilson and Cowan neural field model~\cite{Wilson1973} that will be used to create our intracortical connectivity estimator. This model is descriptive of a range of neurodynamics of the cortex such as evoked potentials, visual hallucinations, and epileptic behavior. It is also capable of generating complex patterns of activity such as Turing patterns, spirals, and traveling oscillations. The neural field model is popular due to being parsimonious yet having a strong link with the underlying physiology.
The model relates the average number of action potentials $g$ arriving at $r$ to the local post synaptic membrane voltage. The post-synaptic potentials generated at a neuronal population at location $r$ by action potentials arriving from all other connected populations at locations $r'$ can be described by
\begin{equation}\label{SpikesToPotential}
v\left( {r,t} \right) = \int_{ - \infty }^t {h\left( {t - t'} \right)g\left( {r,t'} \right)dt'} + e\left( {r,t} \right).
\end{equation}
The term $e(r,t)$ can be considered the potential generated by unmodeled inputs and spontaneous background activity and is a zero mean temporal Wiener process with incremental covariance $\Sigma_e(r)dt$. The post-synaptic response kernel $h(t)$ is described by
\begin{equation}\label{SynapticRespKernel}
h(t) = \left\{ {\begin{array}{*{20}{c}}
   {\exp ( - \alpha t)} & {t \ge 0}  \\
   0 & {t < 0}  \\
\end{array}} \right.
\end{equation}
where $\alpha=\tau^{-1}$ and $\tau$ is the synaptic time constant. The synaptic response kernel can be more elaborate, using rise and fall time constants, however, this simple form serves our purposes. Nonlocal interactions between cortical populations are described by	
\begin{equation}\label{RateBasedInteractions}
g\left( r,t \right) = \int_\Omega  {w\left( r,r' \right)f\left( v\left( r',\bar t \right) \right)dr'}
\end{equation}
where $\bar t = t - d\left| {r - r'} \right|$, $d$ is the propagation delay of action potentials, $f$ is spiking rate of populations $r'$, $w$ is the spatial connectivity kernel, and $\Omega$ is the spatial domain, representing a cortical sheet or surface.

The average firing rate of the presynaptic neurons is related to the postsynaptic membrane potential by the sigmoidal activation function
\begin{equation}\label{ActivationFunction}
f\left( {{v_t}\left( {r'} \right)} \right) = \frac{{{\nu _0}}}{{1 + \exp \left( {\beta \left( {{v_0}\left( r \right) - {v}\left(r',\bar{t} \right)} \right)} \right)}}.
\end{equation}
%\begin{equation}\label{RateOfPreSynPops}
%p\left( {v\left( {r,\bar t} \right)} \right) = {\nu_0}u\left( {v\left( {r,\bar t} \right) - {v_0}\left( r \right)} \right)
%\end{equation}
%\begin{equation}\label{HeavisideDef}
%u\left( {v\left( {r,\bar t} \right) - {v_0}\left( r \right)} \right) = \left\{ {\begin{array}{*{20}{c}}
%   1 & {v\left( {r,\bar t} \right) \ge {v_0}\left( r \right)}  \\
%   0 & {otherwise}  \\
%\end{array}} \right.
%\end{equation}
Even though we will model  $v_0$ as being homogeneous across the cortical sheet we will express it as being spatially dependant allowing for generality when constructing the state-space representation in the next section. The parameters in equation~\ref{ActivationFunction} are detailed in Table 1. By combining equations~\ref{SpikesToPotential} and \ref{RateBasedInteractions} we get the spatiotemporal model
\begin{equation}\label{FullDoubleIntModel}
v\left(r,t\right) = \int\limits_{-\infty}^t\int\limits_\Omega  h\left(t - t'\right)w\left(r,r'\right)f\left( v\left( r',\bar t \right)\right)dr' dt'+e\left(r,t\right)
\end{equation}
To arrive at the final form of the model we shall state the synaptic response kernel as a Green's function
\begin{equation}\label{GreensFuncDef}
Lh\left( t \right) = \delta \left( t \right),
\end{equation}
where $L$ is a temporal differential operator and $\delta(t)$ is the Dirac-delta function. This provides the most general form
\begin{equation}\label{GenForm}
L\left( \frac{\partial }{dt} \right)v\left( {r,t} \right) = \int_\Omega  {w\left( {r,r'} \right)f\left( {v\left( {r',\bar t} \right)} \right)dr'}  + de\left( r,t \right),
\end{equation}
where $L$ is a polynomial of order $n$ with constant coefficients that provides an $n^{th}$ order model. Most studies considering this model use first or second order time derivatives. We shall use the first order model giving
\begin{equation}\label{FinalForm1}
dv\left( {r,t} \right) + \alpha v\left( r,t \right)dt = \int_\Omega  {w\left( r,r' \right)f\left( {v\left( r',\bar t \right)} \right)dr'dt}  +de\left( r,t \right).
\end{equation}
The PDE form of the model is
\begin{equation}\label{PDE}
\frac{\partial^2}{\partial t \partial r'} v\left( r,t \right) + \alpha \frac{\partial}{\partial r'} v\left( r,t \right) = w\left( r,r' \right)f\left( v\left( r',\bar t \right) \right)  + \frac{\partial}{\partial r'}de\left( {r,t} \right).
\end{equation}
% \subsection{Multi-Layer Model}
% \begin{equation}\label{ExcitatoryLayer}
% \frac{dv_e\left( {r,t} \right)}{{dt}} + \alpha_ev_e\left( {r,t} \right) = \int_\Omega  {w_{ee}\left( {r,r'} \right)f\left( v_e\left( r',\bar t \right) \right) + w_{ei}\left( {r,r'} \right)f\left( v_i\left( r',\bar t \right) \right)dr'}  +\varepsilon_e\left( {r,t} \right)
% \end{equation}
% \begin{equation}\label{InhibitoryLayer}
% \frac{dv_i\left( {r,t} \right)}{{dt}} + \alpha_iv_i\left( {r,t} \right) = \int_\Omega  {w_{ie}\left( {r,r'} \right)f\left( v_e\left( r',\bar t \right) \right) + w_{ii}\left( {r,r'} \right)f\left( v_i\left( r',\bar t \right) \right)dr'}  +\varepsilon_i\left( {r,t} \right)
% \end{equation}
% The subscript  denotes connectivity from an inhibitory neural element to an excitatory neural element. We can write the model in matrix form
% \begin{equation}\label{matrixEquation}
% \left[ {\begin{array}{*{20}{c}}
%    {\frac{{d{v_e}\left( t \right)}}
% {{dt}}}  \
%    {\frac{{d{v_i}\left( t \right)}}
% {{dt}}}  \
% 
%  \end{array} } \right] = \int_\Omega  {\left[ {\begin{array}{*{20}{c}}
%    {{w_{ee}}\left( {r,r'} \right)} & {{w_{ei}}\left( {r,r'} \right)}  \
%    {{w_{ie}}\left( {r,r'} \right)} & {{w_{ii}}\left( {r,r'} \right)}  \
% 
%  \end{array} } \right]} \left[ {\begin{array}{*{20}{c}}
%    {f\left( {{v_e}\left( {r',\bar{t}} \right)} \right)}  \
%    {f\left( {{v_i}\left( {r',\bar{t}} \right)} \right)}  \
% 
%  \end{array} } \right]dr' - \left[ {\begin{array}{*{20}{c}}
%    {{\alpha _e}{v_e}\left( {r,t} \right)}  \
%    {{\alpha _i}{v_i}\left( {r,t} \right)}  \
% 
%  \end{array} } \right] + \left[ {\begin{array}{*{20}{c}}
%    {{\varepsilon _e}\left( t \right)}  \
%    {{\varepsilon _i}\left( t \right)}  \
% 
%  \end{array} } \right]
% \end{equation}
% 
% \begin{equation}\label{matrix_equation_in_vector_notation}
%    \frac{d\mathbf{V}(r,t)}{dt} = \int_\Omega \mathbf{W}(r,r')f\left(\mathbf{V}(r,t)\right)dr' - \mathbf{A}\mathbf{V}(r,t) + \mathbf{E}(r,t)
% \end{equation}
% 
% The iEEG observation equation is
% \begin{equation}\label{iEEGObservationEquation}
%     y(k,t) = \int_{\Omega} m(k,r)\left(v_e(r,t) + v_i(r,t)\right)dr' + w(k,t)
% \end{equation}{} 

% \subsection{Alternate Model Description I}
% An alternate and equivalent way to remove the temporal integration using the Laplace transform . We start by considering a discrete neural mass $i$ with inputs with zero delay from another discrete mass $j$ and
% \begin{equation}\label{TwoDiscreteMasses}
% {v_i}\left( t \right) = \int_{ - \infty }^t {{h_j}(t - t'){w_{ij}}p\left( {{v_j}\left( t \right)} \right)dt'},
% \end{equation}
% where $w_{ij}$ can be considered the synaptic efficiency from $j$ to $i$. Now we take the Laplace transform $\mathcal{L}(v_i(t))=V(s)$ yielding
% \begin{eqnarray}
% % \nonumber to remove numbering (before each equation)
%   {V_i}\left( s \right) &=& {H_j}\left( s \right){w_{ij}}P\left( {{V_j}\left( s \right)} \right) \nonumber \\
%    &=& {\left( {s + \alpha} \right)^{ - 1}}{w_{ij}}P\left( {{V_j}\left( s \right)} \right)  \\
%   \Rightarrow s{V_i}\left( s \right) + \alpha{V_i}\left( s \right) &=& {w_{ij}}P\left( {{V_j}\left( s \right)} \right).
% \end{eqnarray}
% Now taking the inverse Laplace transform we get
% \begin{equation}\label{InverseLaplace}
% \frac{{d{v_i}\left( t \right)}}{{dt}} + \alpha{v_i}\left( t \right) = {w_{ij}}p\left( {{v_j}\left( t \right)} \right).
% \end{equation}
% Next we convert the model from discrete neural masses to a continuum by
% \begin{equation}\label{Discrete2Continuum}
% \frac{{dv\left( {r,t} \right)}}{{dt}} + \alpha{v\left( {r,t} \right)} = \int_\Omega  {w\left( {r,r'} \right)p\left( {v\left( {r',\bar t} \right)} \right)dr'}  + e\left( {s,t} \right).
% \end{equation}

%\subsection{Alternate Model Description II}
%\begin{figure*}
%\centering
%\includegraphics[scale=0.4]{NeuralMassFigure.eps}
%\caption[NeuralMassModel]{This figure describes the basic interactions of neural masses that constitute the neural field model.}
%\label{NeuralMassModel}
%\end{figure*}
%An alternative description of the model is as follows. The model can be thought of as a continuum of neural masses. To illustrate this idea, we shall first describe the interaction of discrete neural masses as seen in Figure~\ref{NeuralMassModel}. Assuming that post-synaptic potentials sum linearly, the membrane potential at the soma of neuron $i$ is described as
%\begin{equation}\label{VoltageFromPSPs}
%{v_i}\left( t \right) = \sum\nolimits_{j,k} {{w_{ij}}{h_{j}}\left( t - {t_k}\right)} u\left( {t - {t_k}} \right),
%\end{equation}
%where $w_{ij}$ is the synaptic weight from neuron $j$ to $i$, $h(t)$ is the synaptic response kernel, $S$ is the Heaviside step function and action potential arrive at neuron $i$ at times $t_k$. Alternatively the membrane potential can described by the convolution of synaptic response kernel with the pre-synaptic firing rate $r_j(t)$ where
%\begin{equation}\label{VoltageFromRate}
%{v_i}\left( t \right) = \sum\nolimits_j {\int_{ - \infty }^t {{w_{ij}}{h_j}\left( {t - t'} \right){r_j}\left( t \right)} } dt'.
%\end{equation}
%To convert the equation system into an integral differential system, we shall take derivative of~\ref{EqVoltageFromPSPs} yielding
%\begin{eqnarray}
%% \nonumber to remove numbering (before each equation)
%  \frac{{d{v_i}\left( t \right)}}{{dt}} &=& \sum\nolimits_{j,k} {\left( {{w_{ij}}{h_j}\left( t- {t_k} \right)\frac{d}{{dt}}u\left( {t - {t_k}} \right) + {w_{ij}}u\left( {t - {t_k}} \right)\frac{d}{{dt}}{h_j}\left( t- {t_k} \right)} \right)} \nonumber \\
%   &=& \sum\nolimits_{j,k} {{w_{ij}}{h_j}\left( t- {t_k} \right)\delta \left( {t - {t_k}} \right)}  - \sum\nolimits_{j,k} {{w_{ij}}u\left( {t - {t_k}} \right)\alpha_j{h_j}\left( t- {t_k} \right)}   \nonumber \\
%   &=& \sum\nolimits_{j,k} {{w_{ij}}\delta \left( {t - {t_k}} \right)}  - {v_i}\left( t \right)\sum\nolimits_j {\alpha_j} \nonumber  \\
%   &=& \sum\nolimits_j {{w_{ij}}{\nu_j}\left( t \right)}  - {v_i}\left( t \right)\sum\nolimits_j {\alpha_j} \nonumber \\
%   &=& \sum\nolimits_j {{w_{ij}}p\left( {{v_j}\left( t \right)} \right)}  - {v_i}\left( t \right)\sum\nolimits_j {\alpha_j}.
%\end{eqnarray}
%This form can easily be extended to represent a continuous field.

%\subsection{Assumptions and Simplifications}
%We assume that the propagation delay of action potentials is zero. This is a common assumption that is made when modeling intracortical circuits (\textcolor[rgb]{1.00,0.00,0.00}{refs}). As stated in Wilson and Cohen's original works, the time constant $\tau$ is much greater than the axonal propagation delays, therefore it is reasonable to approximate the the delays as being negligible. Recent studies have demonstrated the importance of having a fast, but finite, propagation velocity for models to make realistic and complete predictions of cortical dynamics~\cite{Atay2005}. The purpose of this paper, however, is not to precisely simulate complex cortical dynamics, but to create a useful platform where estimation of hidden states is possible for the purpose of detecting abnormal neural states with intentions of generating informed patient specific control sequences. Therefore, the model simply has to be `good enough' for this purpose. Starting with this simplified version of the general model is a logical choice. The next assumption we will make is spatial homogeneity such that synaptic interaction between neural populations is only dependant on their spatial separation, not on their spatial location. Again, this is a common assumption when dealing with intracortical connectivity (\textcolor[rgb]{1.00,0.00,0.00}{ref}). This assumption is not essential for our IDE estimation techniques, where a heterogeneous structure is possible. The IDE estimation is derived for a general heterogeneous structure however, for clarity of results we shall only present the heterogeneous case. We will assume the synaptic response kernel has the same shape (scaled by the connectivity kernel) irrespective of the respective presynaptic population. In this case, inhibition can be modeled by having a negative response kernel and excitation with a positive response kernel, where the connectivity kernel provides the (scaled) polarity. This reduces the model to a single field. We will also assume the activation function $p(.)$ is a Heaviside step function. This is in agreement with the work of Schiff and Sauer~\cite{schiff2008kalman}. With these assumptions, the model has the form
%\begin{equation}\label{SimplifiedModel}
%\frac{{dv\left( {r,t} \right)}}{{dt}} = \int_\Omega  {w\left( {r-r'} \right)f\left( {v\left( {r',t} \right)} \right)dr'}  - \alpha{v\left( {r,t} \right)} + e\left( {r,t} \right).
%\end{equation}
%In order to make the problem trackable, in terms of model order, we assume that we can decompose the field (membranes voltages) into a set of $n_\phi$ Gaussian basis functions. We will also assume that we know the location and width of the basis function that is required to decompose the field. The state of the system is the height basis functions, rather than the membrane potential. Therefore, the number of basis functions required to decompose the field is equal to the number of states. Also, we assume that we can represent the connectivity by a regular grid of three connectivity Gaussian basis functions (corresponding to short, mid and long range connectivity) of known width. The height of the connectivity basis functions ($\theta$ parameter) is to be estimated.
%\subsection{IDE Formulation}

%For the state-space derivation, to create an estimator, the model can be reformulated like so
%\begin{equation}\label{new_spatiotemporal_kernels}
%\frac{{dv\left( {r,t} \right)}}{{dt}} = \int_\Omega  {k\left( {r - r'} \right)\tilde v\left( {r',t} \right) - \alpha \delta \left( {r - r'} \right)\bar v\left( {r,t} \right)dr'}  + e\left( {r,t} \right)
%\end{equation}
%\begin{equation}\label{define_v_tilde}
%\tilde v\left( {r,t} \right) = \left\{ {\begin{array}{*{20}{c}}
%   {v\left( {r,t} \right)} & {v\left( {r,t} \right) \ge {v_0}}  \\
%   0 & {otherwise}  \\
%\end{array}} \right.
%\end{equation}
%\begin{equation}\label{define_v_bar}
%\bar v\left( {r,t} \right) = \left\{ {\begin{array}{*{20}{c}}
%   {v\left( {r,t} \right)} & {v\left( {r,t} \right) < {v_0}}  \\
%   0 & {otherwise}  \\
%\end{array}} \right.
%\end%equation}
% \begin{figure*}
% \centering
% \includegraphics[scale=0.3]{ExampleFieldDecomposition.eps}
% \caption[ExampleFieldDecomposition]{Illustration of a one dimensional field decomposition using 5 Gaussian basis functions, each weighted an element of the state vector. Each basis function has a width of 0.01 and is uniformly positioned with a separation of 0.25 (with arbitrary units).}
% \label{ExampleFieldDecomposition}
% \end{figure*}

%\section{State-Space Neural Field Model}
%In this section we shall reconstruct the model in a state-space form that is a novel representation of the neural field equations. The benefit of restating the model in this format is that it becomes easier to simulate and allows the connectivity to be inferred from data. \begin{equation}
% v_{t+1}(\mathbf r)=\int_{\Omega} \alpha \delta(\mathbf r-\mathbf r')\bar v_t(\mathbf r)d \mathbf r'+\int_{\Omega} w(\mathbf r -\mathbf r')\tilde v_t(\mathbf r')d\mathbf r'+e_t(\mathbf r)
%\label{eq:cortexfield}
%\end{equation}
%decomposing the field and the kernel we have
%\begin{equation}
%\bar v_t(\mathbf r)=\boldsymbol \phi^{\top}(\mathbf r)\bar{\mathbf x}_t
%\label{eq:cortexfieldlv0}
%\end{equation}
%\begin{equation}
%\tilde v_t(\mathbf r)=\boldsymbol \phi^{\top}(\mathbf r)\tilde{\mathbf x}_t=\nu_0
%\label{eq:cortexfieldgv0}
%\end{equation}
%\begin{equation}
% w(\mathbf r -\mathbf r')=\boldsymbol \theta^{\top}\psi(\mathbf r -\mathbf r')
%\label{eq:kerneldecomposition}
%\end{equation}
%Substituting (\ref{eq:cortexfieldlv0}), (\ref{eq:cortexfieldgv0}) and (\ref{eq:kerneldecomposition}) in (\ref{eq:cortexfield}) we get
%\begin{equation}
% \boldsymbol \phi^{\top}(\mathbf r)\bar{\mathbf x}_{t+1}+\boldsymbol \phi^{\top}(\mathbf r)\tilde{\mathbf x}_{t+1}=
%\int_\Omega\alpha\delta(\mathbf r- \mathbf r')\boldsymbol\phi^{\top}(\mathbf r)\bar{\mathbf x}_td\mathbf r'+\int_\Omega\boldsymbol \theta^{\top}\boldsymbol\psi(\mathbf r -\mathbf r')\boldsymbol\phi^{\top}(\mathbf r')\tilde{\mathbf x}_td\mathbf r'+e_t(\mathbf r)
%\label{eq:decomposedcortexfield}
%\end{equation}
%\begin{equation}
% \boldsymbol \phi^{\top}(\mathbf r)(\bar{\mathbf x}_{t+1}+\tilde{\mathbf x}_{t+1})=\alpha \boldsymbol\phi^{\top}(\mathbf r)\bar{\mathbf x}_{t}+\boldsymbol \theta^{\top}\int_{\Omega}\boldsymbol\psi(\mathbf r -\mathbf r')\boldsymbol\phi^{\top}(\mathbf r')\tilde{\mathbf x}_td\mathbf r'+e_t(\mathbf r)
%\end{equation}
%by defining
%\begin{equation}
% \boldsymbol\Phi(\mathbf r)=\int_{\Omega}\boldsymbol\psi(\mathbf r -\mathbf r')\boldsymbol\phi^{\top}(\mathbf r')d\mathbf r'
%\label{eq:convolution}
%\end{equation}
%we have
%\begin{equation}
% \boldsymbol \phi^{\top}(\mathbf r)(\bar{\mathbf x}_{t+1}+\tilde{\mathbf x}_{t+1})=\alpha \boldsymbol\phi^{\top}(\mathbf r)\bar{\mathbf x}_{t}+\boldsymbol \theta^{\top}\boldsymbol\Phi(\mathbf r)\tilde{\mathbf x}_t+e_t(\mathbf r)
%\label{eq:cortexfieldwithPhi}
%\end{equation}
%Premultiplying (\ref{eq:cortexfieldwithPhi}) by $\boldsymbol\phi(\mathbf r) $, and integrating over $\mathbf r$
%\begin{equation}
%\int_{\Omega} \boldsymbol\phi(\mathbf r)\boldsymbol \phi^{\top}(\mathbf r)d\mathbf s(\bar{\mathbf x}_{t+1}+\tilde{\mathbf x}_{t+1})=\alpha\int_{\Omega} \boldsymbol\phi(\mathbf r)\boldsymbol \phi^{\top}(\mathbf r)d\mathbf r \bar{\mathbf x}_{t}+\int_{\Omega} \boldsymbol\phi(\mathbf r)\boldsymbol \theta^{\top}\boldsymbol\Phi(\mathbf r)d\mathbf r\tilde{\mathbf x}_t+\int_{\Omega} \boldsymbol\phi(\mathbf r)e_t(\mathbf r)d\mathbf r
%\label{eq:cortexfieldversion2}
%\end{equation}
%Now we define
%\begin{equation}
% \boldsymbol\Psi_x(\mathbf r)=\int_{\Omega}\boldsymbol\phi(\mathbf r)\boldsymbol\phi^{\top}(\mathbf r)d\mathbf r
%\label{eq:psix}
%\end{equation}
%\begin{equation}
%  \boldsymbol\Psi_{\theta}(\mathbf r)=\int_{\Omega} \boldsymbol\phi(\mathbf r)\boldsymbol \theta^{\top}\boldsymbol\Phi(\mathbf r)d\mathbf r
%\label{eq:psitheta}
%\end{equation}
%substituting (\ref{eq:psix}) and (\ref{eq:psitheta}) in (\ref{eq:cortexfieldversion2}) we get
%\begin{equation}
%\boldsymbol\Psi_x(\mathbf r)(\bar{\mathbf x}_{t+1}+\tilde{\mathbf x}_{t+1})=\alpha\boldsymbol\Psi_x(\mathbf r) \bar{\mathbf x}_{t}+\boldsymbol\Psi_{\theta}(\mathbf r)\tilde{\mathbf x}_t+\int_{\Omega} \boldsymbol\phi(\mathbf r)e_t(\mathbf r)d\mathbf r
%\end{equation}
%pre-multiplying both sides by $\boldsymbol\Psi_x^{-1}$
%\begin{equation}
%\bar{\mathbf x}_{t+1}+\tilde{\mathbf x}_{t+1}=\alpha\bar{\mathbf x}_{t}+\boldsymbol\Psi_x^{-1}\boldsymbol\Psi_{\theta}\tilde{\mathbf x}_t+\boldsymbol\Psi_x^{-1}\int_{\Omega} \boldsymbol\phi(\mathbf r)e_t(\mathbf r)d\mathbf r
%\end{equation}
%????????????????????????????????
%\subsection{Observation equation}
%\begin{eqnarray}
% \mathbf y_t(\mathbf r)&=&\\
%&=&\int_{\Omega}H(\mathbf r-\mathbf r')v_t(\mathbf r')d\mathbf r'+\boldsymbol\epsilon_t \\
%&=&\int_{\Omega}H(\mathbf r-\mathbf r')\bar v_t(\mathbf r')d\mathbf r'+\int_{\Omega}H(\mathbf r-\mathbf r')\tilde v_t(\mathbf r')d\mathbf r'+\epsilon_t
%\label{eq:observationequation}
%\end{eqnarray}
%
% we decompose $ H(\mathbf r-\mathbf r')$ as
% \begin{equation}
%  H(\mathbf r-\mathbf r')= \mathbf c^{\top}\mu(\mathbf r-\mathbf r')
%\label{eq:decomposedobservationkernel}
%\end{equation}
%and substituting  (\ref{eq:cortexfieldlv0}), (\ref{eq:cortexfieldgv0}) and (\ref{eq:decomposedobservationkernel}) in (\ref{eq:observationequation}) we get
%\begin{equation}
% \mathbf y_t(\mathbf r)=\int_{\Omega}\mathbf c^{\top}\mu(\mathbf r-\mathbf r')\phi^{\top}(\mathbf r')\bar{\mathbf x}_{t}d\mathbf r'+\int_{\Omega}\mathbf c^{\top}\mu(\mathbf r-\mathbf r')\phi^{\top}(\mathbf r')\tilde{\mathbf x}_{t}d\mathbf r'+\epsilon_t
%\end{equation}
%and finally we have
%\begin{equation}
%\mathbf y_t(\mathbf r)= \begin{bmatrix}\mathbf c^{\top}\mathbf M(\mathbf r)&\mathbf c^{\top}\mathbf M(\mathbf r)\end{bmatrix}\underline{\mathbf x}_t+\epsilon_t
%\end{equation}
%where
%\begin{equation}
% \mathbf M(\mathbf r)=\int_{\Omega}\mu(\mathbf r-\mathbf r')\phi^{\top}(\mathbf r')d\mathbf r'
%\end{equation}
%Next we shall demonstrate the stability of the model and in the following section the estimator for the state vector and connectivity structure is derived.

%\subsection{State-Space Neural Field Model}
%In this section we shall reconstruct the model in a state-space form that is a novel representation of the neural field equations. The benefit of restating the model in this format is that it becomes easier to simulate and allows the connectivity to be inferred from data. We begin with the model in the form
%\begin{eqnarray}\label{InitialModel}
%% \nonumber to remove numbering (before each equation)
%  {v_{t + 1}}\left( r \right) &=& \int_\Omega  {k\left( {r - r'} \right)p\left( {{v_t}\left( {r'} \right)} \right) - \alpha v\left( {r,t} \right)dr'}  + \varepsilon_t \left( {r} \right) \nonumber \\
%   &=& \int_\Omega  {k\left( {r - r'} \right)p\left( {{v_t}\left( {r'} \right)} \right)dr'}  - \alpha v\left( {r,t} \right) + \varepsilon_t \left( {r} \right).
%\end{eqnarray}
%Next we defining the basis functions the decompositions of the field and connectivity kernel, where $\phi(s)$ is a vector is Gaussian basis functions, $x_t$ is the state, $\psi(r-r')$ is a vector of three connectivity basis functions and $\theta$ is the corresponding weight of the connectivity
%\begin{equation}\label{FieldDecomp}
%   {v_t}\left( r \right) = {\phi ^T}\left( r \right){x_t}
%\end{equation}
%\begin{equation}\label{KernelDecomp}
%k\left( {r - r'} \right) = {\theta ^T}\psi \left( {r - r'} \right).
%\end{equation}
%Assume we know $\phi(r')$ and it is constant, and the form of the activation function $p(.)$ is known, and $\phi(r')$ is independent of $x_t$, then we can uniquely define the function $\rho(.)$ such that
%\begin{eqnarray}\label{NonlinearState}
%% \nonumber to remove numbering (before each equation)
%  p\left( {{v_t}\left( {r'} \right)} \right)  &=& p\left( {{\phi ^T}\left( {r'} \right){x_t}} \right) \nonumber \\
%   &=& \phi ^T\left(r'\right)\rho \left( {{x_t}} \right).
%\end{eqnarray}
%Making substitutions of~\ref{FieldDecomp}, \ref{KernelDecomp} and \ref{NonlinearState} into \ref{InitialModel} we get
%\begin{equation}\label{ModelDecomposed}
%{\phi ^T}\left( r \right){x_{t + 1}} = {\theta ^T}\int_\Omega  {\psi \left( {r - r'} \right){\phi ^T}\left( {r'}\right)dr'} \rho \left( {{x_t}} \right) - \alpha {\phi ^T}\left( r \right){x_t} + \varepsilon_t \left( {r} \right)
%\end{equation}
%Now we define
%\begin{equation}\label{DefPhi}
%\Phi \left( r \right) = \int_\Omega  {\psi \left( {r - r'} \right){\phi ^T}\left( {r'} \right)dr'}.
%\end{equation}
%Substituting~\ref{DefPhi} into \ref{ModelDecomposed} we get
%\begin{equation}\label{Sub2}
%{\phi ^T}\left( r \right){x_{t + 1}} = {\theta ^T}\Phi \left( r \right)\rho \left( {{x_t}} \right) - \alpha {\phi ^T}\left( r \right){x_t} + \varepsilon_t \left( {r} \right).
%\end{equation}
%Cross multiplying by $\phi(r)$ in integrating over $r$ gives
%\begin{equation}\label{CorssMultAndInt}
%\int_\Omega  {\phi \left( r \right){\phi ^T}\left( r \right)dr} {x_{t + 1}} = {\theta ^T}\int_\Omega  {\phi \left( r \right)\Phi \left( r \right)dr} \rho \left( {{x_t}} \right) - \alpha \int_\Omega  {\phi \left( r \right){\phi ^T}\left( r \right)dr} {x_t} + \int_\Omega  {\phi \left( r \right)\varepsilon_t \left( {r} \right)dr}.
%\end{equation}
%Now we define
%\begin{equation}\label{DefPsix}
%{\Psi _x} = \int_\Omega  {\phi \left( r \right){\phi ^T}\left( r \right)dr}
%\end{equation}
%\begin{equation}\label{DefPsitheta}
%{\Psi _\theta } = \int_\Omega  {\phi \left( r \right)\Phi \left( r \right)dr}.
%\end{equation}
%Substituting~\ref{DefPsix} and \ref{DefPsitheta} into \ref{CorssMultAndInt} we get
%\begin{equation}\label{Subs3}
%{\Psi _x}{x_{t + 1}} = {\theta ^T}{\Psi _\theta }\rho \left( {{x_t}} \right) - \alpha {\Psi _x}{x_t} + \int_\Omega  {\phi \left( r \right)\varepsilon_t \left( {r} \right)dr}.
%\end{equation}
%Now we isolate the state by inverting $\Psi_x$
%\begin{equation}\label{IsolateState}
%{x_{t + 1}} = {\Psi _x}^{ - 1}{\theta ^T}{\Psi _\theta }\rho \left( {{x_t}} \right) - \alpha {x_t} + {\Psi _x}^{ - 1}\int_\Omega  {\phi \left( r \right)\varepsilon_t \left( {r} \right)dr}.
%\end{equation}
%This gives the state-space model
%\begin{equation}\label{StateSpaceModel}
%{x_{t + 1}} = f\left( {x,\theta } \right) + {\Psi _x}^{ - 1}\int_\Omega  {\phi \left( r \right)\varepsilon_t \left( {r} \right)dr}.
%\end{equation}

\section{State-Space Model}
We begin the derivation of the state-space model with the discrete time, continuous space IDE neural field model. A one-step Euler method was used to transform the nonlinear stochastic IDE to a discrete time form.
\begin{equation}\label{SystemEq}
\frac{v_{t + 1}\left( \mathbf{r} \right) - v_t(\mathbf{r})}{T_s} = \int_\Omega  {k\left( \mathbf{r} , \mathbf{r'} \right)f\left( {v_t\left( \mathbf{r'} \right)} \right)d{\bf{r'}}}  - \alpha v_t\left( \mathbf{r} \right) + e_t\left( \mathbf{r} \right),
\end{equation}
\begin{equation}\label{SystemEq}
v_{t + 1}\left( \mathbf{r} \right)  = T_s\int_\Omega  {k\left( {{\bf{r}} , {\bf{r'}}} \right)f\left( {{v_t}\left( \bf{r'} \right)} \right)d{\bf{r'}}} - T_s\alpha {v_t}\left( {\bf{r}} \right) + v_t(\mathbf{r}) + T_se_t\left( {\mathbf{r}} \right),
\end{equation}
\begin{equation}\label{SystemEq}
v_{t + 1}\left( \mathbf{r} \right)  = T_s\int_\Omega  {k\left( {{\bf{r}} , {\bf{r'}}} \right)f\left( {{v_t}\left( \mathbf{r'} \right)} \right)d{\bf{r'}}} + (1 - T_s\alpha) v_t\left( {\bf{r}} \right) + T_se_t\left( {\mathbf{r}} \right),
\end{equation}
\begin{equation}\label{SystemEq}
v_{t + 1}\left( \mathbf{r} \right)  = T_s\int_\Omega  {k\left( {{\bf{r}} , {\bf{r'}}} \right)f\left( {{v_t}\left( \bf{r'} \right)} \right)d{\bf{r'}}} + \lambda v_t\left( {\bf{r}} \right) + T_se_t\left( {\mathbf{r}} \right).
\end{equation}
where $e_t(r)\sim \mathcal{GP}(0,R)$, $\lambda = 1-T_s\alpha$ and $T_s$ is the time step or sampling step.

The observation equation for the model is
\begin{equation}\label{ObservationEquation}
    y_t( k ) = \int_\Omega  m( k,\mathbf{r} )v_t( \mathbf{r}\rm ) d\mathbf{r}  + \varepsilon ( k ),
\end{equation}
where $\varepsilon( k )\sim\mathcal{N}(0,\Sigma)$, $k$ indexes the iEEG electrodes, and $m(k,r)$ is the observation kernel relating the neural field to EEG measurements accounting for electrodes characteristics.


%To infer the state from observations we can write the likelihood function
%\begin{equation}\label{Likelihood}
%p\left( {{y_{1,...,T}}|{v_{1,...,T}}} \right) = \prod\limits_{t = 1}^T {p\left( {{y_t}|{v_t}} \right)} \prod\limits_{t = 2}^T {p\left( {{v_{t + 1}}|{v_t}} \right)} p\left( {{v_0}} \right)
%\end{equation}
%where
%\begin{eqnarray}
%% \nonumber to remove numbering (before each equation)
%  p\left( {{y_t}|{v_t}} \right) &=& \mathcal{N}\left( {\int_\Omega  {m\left( {k,r} \right)v_t( r )dr} ,R} \right) \\
%  p\left( {{v_{t + 1}}|{v_t}} \right) &=& ??? \\
%  p\left( {{v_0}} \right) &=& \mathcal{GP}\left( {{\mu _0},{\pi _0}} \right).
%\end{eqnarray}
%We can not calculate the posterior in this Bayesian model for some reason (Mike and Ken can elaborate), so we need to decompose the field into weighted basis functions.

In order to implement standard estimation techniques we propose a decomposition of the The field using a set of Gaussian basis functions. Decomposition allows a continuous field to be represented by a finite dimensional state vector. This allows for the application of standard nonlinear, non-Gaussian state estimation methods such as sequential Monte-Carlo techniques like particle filtering. The field decomposition is described by
\begin{equation}\label{DefFieldDecomp}
{v_t}\left( {\bf{r}} \right) = {\phi ^T}\left( {\bf{r}} \right){x_t}.
\end{equation}
where $\phi(r)$ is the Gaussian basis functions that are scaled by the the state vector. The connectivity kernel can all be decomposed into a set of basis functions
\begin{equation}\label{DefKernelDecomp}
k( \mathbf{r} , \mathbf{r'} ) = \theta^T\psi(\mathbf{r} , \mathbf{r'} )
\end{equation}
Will will assume we know the parametric form of the basis functions, where the parameter $\theta$ is unknown. Each connectivity basis function can be considered a layer in the WC model, representing short range excitation, surround inhibition and long range excitation. Making substitutions we get
\begin{equation}\label{DecompModel}
{\phi ^T}\left( \mathbf{r} \right){x_{t+1}} = T_s\theta^T\int_\Omega  {\psi \left( {{\bf{r}} , {\mathbf{r'}}} \right)f\left( {{\phi ^T}\left( {{\mathbf{r'}}} \right){x_t}} \right)d{\mathbf{r'}}}  - \lambda\phi^T(\mathbf{r})x_t + T_s{e_t}\left( {\mathbf{r}} \right).
\end{equation}
To isolate the state we cross multiply by $\phi(\mathbf{r})$ and integrate over $\mathbf{r}$
\begin{equation}\label{Poo}
\int_\Omega  {\phi \left( {\bf{r}} \right){\phi ^T}\left( {\bf{r}} \right)d{\bf{r}}{x_{t+1}}}  = T_s\int_\Omega  {\phi ( \mathbf{r} )\theta^T\int_\Omega  {\psi \left( {{\bf{r}} , {\bf{r'}}} \right)f\left( {{\phi ^T}\left( {{\bf{r'}}} \right){x_t}} \right)d{\bf{r'}}} d{\bf{r}}}  - \lambda\int_\Omega \phi(\mathbf{r})\phi^T(\mathbf{r})d\mathbf{r} x_t  + T_s\int_\Omega  {\phi \left( {\mathbf{r}} \right)e_t\left( {\mathbf{r}} \right)d{\mathbf{r}}}.
\end{equation}
Now we define
\begin{equation}\label{DefGamma}
\Gamma  = \int_\Omega  {\phi \left( {\bf{r}} \right){\phi ^T}\left( {\bf{r}} \right)d{\bf{r}}}.
\end{equation}
Substituting equation~\ref{DefGamma} into \ref{Poo} and rearranging gives
\begin{equation}\label{SS Model}
{x_{t+1}} = T_s{\Gamma ^{ - 1}}\int_\Omega  {\phi \left( {\bf{r}} \right)\theta^T\int_\Omega  {\psi \left( {{\bf{r}} , {\bf{r'}}} \right)f\left( {{\phi ^T}\left( {{\bf{r'}}} \right){x_t}} \right)d{\bf{r'}}} d{\bf{r}}} - \lambda x_t + T_s{\Gamma ^{ - 1}}\int_\Omega  {\phi \left( {\bf{r}} \right)e_t\left( {\bf{r}} \right)d{\bf{r}}}.
\end{equation}
We can rearrange to get
\begin{equation}\label{Homogeneous SS Model}
	x_{t+1} = T_s\Gamma ^{ - 1}\int_\Omega  {\phi \left( {\bf{r}} \right)\int_\Omega  {f\left( {{\phi ^T}\left( {{\bf{r'}}} \right){x_t}} \right)\psi^T \left( {{\bf{r}} - {\bf{r'}}} \right)d{\bf{r'}}} d{\bf{r}}}\theta - \lambda x_t + T_s{\Gamma ^{ - 1}}\int_\Omega  {\phi \left( {\bf{r}} \right)e_t\left( {\bf{r}} \right)d{\bf{r}}}.
\end{equation}
The disturbance term can be rewritten as 
\begin{equation}
	\Gamma ^{ - 1}\int_\Omega  {\phi \left( \bf{r} \right)e_t\left( \bf{r} \right)d\bf{r}} = \int_\Omega  {\phi \left( \bf{r} \right)\tilde{e}_t\left( \bf{r} \right)d\bf{r}},
\end{equation}
where $\tilde{e}_t(r)~N(0,R'R)$ and $R'$ is the Cholesky decomposition of $\Gamma^{-1}$. This provides the final state-space model
\begin{equation}\label{AbbrevSSModel}
x_{t + 1} = T_sq(x_t)\theta -\lambda x_t + T_s\int_\Omega  {\phi ( \mathbf{r} )\tilde{e}_t( \mathbf{r} )d\mathbf{r}}
\end{equation}
with observation equation
\begin{equation}\label{ObservationEquation}
    y_t( k ) = \int_\Omega  m( k,\mathbf{r} )\phi^T( \mathbf{r} ) d\mathbf{r}x_t  + \varepsilon ( k ),
\end{equation}

\subsection{Discrete Space For Simulation}

In this section we describe a discrete time, discrete space model that we are using for the simulations. Now we define the spatial aspect of the model on a regular square $i,j$ grid of neural masses, where the spatial step size $\Delta \mathbf{r}_i = \Delta \mathbf{r}_j = \Delta \mathbf{r}$ giving
\begin{equation}\label{DiscreteSpaceModelij}
	x_{t+1} = T_s\Gamma ^{ - 1}\sum_i \sum_j  {\phi \left( \mathbf{r} \right)\sum_i\sum_j  {f\left( \phi^T \left( \mathbf{r'} \right) x_t \right)\psi^T \left( \mathbf{r} - \bf{r'} \right)\Delta\mathbf{r'}^2} \Delta\mathbf{r}^2} \theta - \lambda x_t + T_s\sum_i\sum_j  {\phi \left( {\bf{r}} \right)\tilde{e}_t\left( \bf{r} \right)\Delta\mathbf{r}^2}.
\end{equation}
\section{Estimation of the Nonlinear Homogeneous IDE Neural Field Model}


\section{Comparison of Estimated and True Connectivity}

\section{Discussion}

% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex,
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation Results}
%\label{fig_sim}
%\end{figure}

% Note that IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.
%
% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command, the
% \label for the overall figure must come after \caption.
% \hfil must be used as a separator to get equal spacing.
% The subfigure.sty package works much the same way, except \subfigure is
% used instead of \subfloat.
%
%\begin{figure*}[!t]
%\centerline{\subfloat[Case I]\includegraphics[width=2.5in]{subfigcase1}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{subfigcase2}%
%\label{fig_second_case}}}
%\caption{Simulation results}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.

% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table. Table text will default to
% \footnotesize as IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}

% Note that IEEE does not put floats in the very first column - or typically
% anywhere on the first page for that matter. Also, in-text middle ("here")
% positioning is not used. Most IEEE journals use top floats exclusively.
% Note that, LaTeX2e, unlike IEEE journals, places footnotes above bottom
% floats. This can be corrected via the \fnbelowfloat command of the
% stfloats package.

\section{Conclusion}
The conclusion goes here.

\appendices
\section{Ken's Discretization}
Spatio-temporal interactions can be described by the continuous space continuous time stochastic differential equation
\begin{equation}\label{differential eq}	
\tau dv\left( r,t \right) = \left( -v\left( r,t \right) + \int_\Omega  {w\left( r,r' \right)f\left( {v\left( r',t \right)} \right)dr'} \right) dt  + d e\left( r,t \right).
\end{equation}
where $e\left( r,t \right)$ is a zero mean temporal Wiener process with incremental covariance $\Sigma_e(r)dt$. (not too sure about this noise since it needs to be white temporally but coloured spatially - which in continuous space-time I have never seen written ... has anyone?)
To simplify the notation, but without loss of generality, \ref{differential eq} can be rewritten as
\begin{equation}\label{differential simplified}
	dv\left( r,t \right) = q\left(v(r,t) \right) dt  + d e\left( r,t \right)
\end{equation}
Had $q\left(v(r,t) \right)$ been a linear function in $v$, then \ref{differential simplified} could be rewritten as
\begin{equation}\label{differential linear}
	dv\left( r,t \right) = Av(r,t)dt  + d e\left( r,t \right)
\end{equation}
Considering temporal sampling instances $\delta$ apart given by $\{t_k = 0,~1,~2, \ldots$ \}, then a solution to \ref{differential linear} at each sampling time is given by
\begin{equation}\label{linear solution}
	v(r,t_k+\delta) = \exp\{A(\delta)\}x(t) + \int_{t_k}^{t_k+\delta}\exp\{A(t_k+\delta-\tau)\}de(r,\tau)
\end{equation}
Let the discrete time random variable $e'(r,t_k)$ be
\begin{equation}\label{discrete noise}
	e'(r,t_k) = \int_{t_k}^{t_k+\delta}\exp\{A(t_k+\delta-\tau)\}de(r,\tau)
\end{equation}
then, $e'(r,t_k)$ is a zero mean, temporally white noise term with covariance
\begin{equation}\label{covariance of noise}
	E\left[e'(r,t_k),e'^\top(r,t_k) \right] = \int_t^{t+\delta}\exp\{A(t_k+\delta-\tau)\} \Sigma_e(r) \exp\{A(t+\delta-\tau)\} d\tau
\end{equation}

But we are not so luck as to be able to go from \ref{differential simplified} to \ref{differential linear} and thus I doubt if we can obtain an exact solution to \ref{differential simplified} without an approximation. Agreed?

A possible approximate solution is the one you have already used. Thus, interpreting the difference equation \ref{differential simplified} as an integral equation gives
\begin{equation}\label{euler}
	v\left(r,t\right) =  v(r,t_0) + \int_{t_0}^{t} q\left(v(r,\tau) \right) d\tau  + \int_{t_0}^{t} d e\left(r,\tau\right)
\end{equation}
Using an Euler approximation, \ref{euler} can be approximated at each sampling time by
\begin{equation}\label{euler approx 1}
	v\left(r,t_{k+1} \right) =  v(r,t_k) + q\left(v(r,t_k)\right) (t_{k+1} - t_k) + (e(r,t_{k+1}) - e(r,t_k))
\end{equation}
which can be rewritten as
\begin{equation}\label{euler approx 2}
	v\left(r,t_k + \delta \right) =  v(r,t_k) + \delta q\left(v(r,t_k)\right) + e'(r,t_k)
\end{equation}
where $e'(r,t_k)$ is a zero mean Gaussian random variable with covariance $\delta\Sigma_e$. (Note that, because of the way the noise is defined in \ref{differential eq} it is not $\delta^2$ but $\delta$)

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
\section{Colored noise}
We have
\begin{equation}
v_{t + 1}\left( \mathbf{r} \right)  = T_s\int_\Omega  {k\left( {{\bf{r}} , {\bf{r'}}} \right)f\left( {{v_t}\left( \bf{r'} \right)} \right)d{\bf{r'}}} + \lambda v_t\left( {\bf{r}} \right) + T_se_t\left( {\mathbf{r}} \right).
\end{equation}
where the disturbance $e_t(\mathbf r)$ is a zero-mean normally distributed noise process, spatially coloured but temporally independent, with covariance 

\begin{equation}
cov(e_t(\mathbf{s}),e_{t+\tau}(\mathbf{r}))=
\begin{cases}
\beta\left(\mathbf{s}-\mathbf{r}\right), & \tau=0 \\
0 & \mathrm{otherwise}
\end{cases}
\label{eq:FieldCovariance}
\end{equation}
The final state-space model is given by
\begin{equation}
x_{t + 1} = T_sq(x_t)\theta -\lambda x_t + T_s\Gamma^{-1}\int_\Omega  {\phi ( \mathbf{r} )e_t( \mathbf{r} )d\mathbf{r}}
\end{equation}
then the covariance of the noise term is given by
\begin{eqnarray}
\mathbf \Sigma_w&{}={}&T_s^2 \times \mathbf{\Gamma}^{-1}\mathbf E[\int_{\mathcal{S}}\boldsymbol\phi\left(\mathbf s\right)e_t\left(\mathbf s\right)d\mathbf s\int_{\mathcal{S}}\boldsymbol\phi\left(\mathbf r\right)^{\top}e_t\left(\mathbf r\right)d\mathbf r]\mathbf{\Gamma}_{x}^{- \top} \nonumber \\
&=&T_s^2 \times\mathbf{\Gamma}^{-1}\iint\limits_{\mathcal{S}}\boldsymbol\phi\left(\mathbf s\right) \mathbf E[e_t\left(\mathbf s\right)e_t\left(\mathbf r\right)]\boldsymbol\phi\left(\mathbf r\right)^{\top}d\mathbf r d\mathbf s\mathbf{\Gamma}^{- \top} \nonumber\\
&=&{}T_s^2 \times\mathbf{\Gamma}^{-1}\iint\limits_{\mathcal{S}}\boldsymbol\phi\left(\mathbf s\right) \beta\left(\mathbf s- \mathbf r \right)\boldsymbol\phi\left(\mathbf r\right)^{\top}d\mathbf r d\mathbf s\mathbf{\Gamma}^{- \top} 
\end{eqnarray}

\section{Parameter estimation using LS}
\begin{equation}
 \mathbf x_{t+1}=\mathbf x_{t}+T_s \mathbf Q(\mathbf x_t)\mathbf \theta-\alpha T_s\mathbf x_t+T_s\boldsymbol\Gamma^{-1}\int_\Omega\boldsymbol\phi(\mathbf r)\varepsilon_t(\mathbf r)d\mathbf r
\label{eq:statespacemodel}
\end{equation}
where the matrix $\mathbf Q_{n_x \times n_{\theta}}(\mathbf x_t)$ is defined as
\begin{equation}
 Q_{n_x \times n_{\theta}}=\boldsymbol\Gamma^{-1}\int_\Omega\boldsymbol\phi(\mathbf r)\int_\Omega f(\boldsymbol\phi^\top(\mathbf r')\mathbf x_t)\boldsymbol\psi^\top(\mathbf r-\mathbf r')d\mathbf r'd\mathbf r
\end{equation}
and $T_s$ is the sampling period. Least Square method can be used as (\ref{eq:statespacemodel}) is linear in parameters. At each time instant we have

\begin{eqnarray}
 \mathbf x_{1}&=&\mathbf x_{0}+T_s \mathbf Q(\mathbf x_0)\mathbf \theta-\alpha T_s\mathbf x_0+T_s\mathbf e_0 \nonumber \\
 \mathbf x_{2}&=&\mathbf x_{1}+T_s \mathbf Q(\mathbf x_1)\mathbf \theta-\alpha T_s\mathbf x_1+T_s\mathbf e_1\nonumber\\
&\vdots& \nonumber\\
 \mathbf x_{n}&=&\mathbf x_{n-1}+T_s \mathbf Q(\mathbf x_{n-1})\mathbf \theta-\alpha T_s\mathbf x_{n-1}+T_s\mathbf e_{n-1}
\end{eqnarray}
in matrix form
\begin{equation}
 \begin{bmatrix} \mathbf x_{1}\\\mathbf x_{2}\\\vdots\\\mathbf x_{n}\end{bmatrix}-\begin{bmatrix} \mathbf x_{0}\\\mathbf x_{1}\\\vdots\\\mathbf x_{n-1}\end{bmatrix}=T_s\begin{bmatrix}\mathbf Q(\mathbf x_0)&-\mathbf x_{0}\\\mathbf Q(\mathbf x_1)&-\mathbf x_{1}\\\vdots\\ \mathbf Q(\mathbf x_{n-1})&-\mathbf x_{n-1}\end{bmatrix}\begin{bmatrix}\mathbf \theta \\ \alpha\end{bmatrix}+T_s\begin{bmatrix}\mathbf e_0\\\mathbf e_1\\\vdots\\\mathbf e_{n-1}\end{bmatrix}
\end{equation}
or in compact form we have
\begin{equation}
 \mathbf Z=\mathbf X \mathbf W+\boldsymbol \xi
\end{equation}
where
\begin{equation}
 \mathbf Z=\begin{bmatrix} \mathbf x_{1}-\mathbf x_{0}\\\mathbf x_{2}-\mathbf x_{1}\\\vdots\\\mathbf x_{n}-\mathbf x_{n-1}\end{bmatrix},\quad \mathbf X=T_s\begin{bmatrix}\mathbf Q(\mathbf x_0)&-\mathbf x_{0}\\\mathbf Q(\mathbf x_1)&-\mathbf x_{1}\\\vdots\\ \mathbf Q(\mathbf x_{n-1})&-\mathbf x_{n-1}\end{bmatrix},\quad \mathbf W=\begin{bmatrix}\mathbf \theta \\ \alpha\end{bmatrix} \text{and}\quad \boldsymbol \xi=T_s\begin{bmatrix}\mathbf e_0\\\mathbf e_1\\\vdots\\\mathbf e_{n-1}\end{bmatrix}
\end{equation}
using the method of Least Square $ \mathbf W$ can be estimated
\begin{equation}
 \mathbf W\approx(\mathbf X^\top\mathbf X)^{-1}\mathbf X^\top\mathbf Z
\end{equation}
% use section* for acknowledgement
\section*{Acknowledgment}
The authors would like to thank...

\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% references section

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,BrainIDE}


% biography section
%
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{biography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

\begin{IEEEbiography}{Dean R. Freestone}
Biography text here.
\end{IEEEbiography}

% if you will not have a photo at all:
\begin{IEEEbiography}{Parham Aram}
Biography text here.
\end{IEEEbiography}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

\begin{IEEEbiography}{Michael Dewar}
Biography text here.
\end{IEEEbiography}

\begin{IEEEbiography}{Kenneth Scerri}
Biography text here.
\end{IEEEbiography}

\begin{IEEEbiography}{Visakan Kadirkamanathani}
Biography text here.
\end{IEEEbiography}

\begin{IEEEbiography}{Other Bosses}
Biography text here.
\end{IEEEbiography}

% that's all folks
\end{document} 