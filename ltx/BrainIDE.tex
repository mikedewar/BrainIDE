%% bare_jrnl.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/


\documentclass[onecolumn,draftcls]{IEEEtran}

\usepackage{graphicx}
\usepackage{color}                    % For creating coloured text and background
\usepackage{amsmath,amssymb,amsfonts} % Typical maths resource packages
% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}
% TODO environment
\newcommand{\todo}[1]{\textsf{\emph{\textbf{\textcolor{blue}{#1}}}}}

\begin{document}

% can use linebreaks \\ within to get better formatting as desired
\title{Estimation of Intracortical Connectivity in a Dynamical Neural Field Model}

\author{Dean~R.~Freestone,~\IEEEmembership{Graduate Student Member,~IEEE,}
        Parham~Aram,~\IEEEmembership{Graduate Student Member,~IEEE,}
        Michael~Dewar,~\IEEEmembership{Member,~IEEE,}
        Kenneth~Scerri,~\IEEEmembership{Member,~IEEE,}
        Visakan~Kadirkamanathan,~\IEEEmembership{Member,~IEEE,}
        David~B.~Grayden,~\IEEEmembership{Member,~IEEE,}
        Anthony~N.~Burkitt,~\IEEEmembership{Member,~IEEE,}
        and~other~Bosses/Supervisors,~\IEEEmembership{Member,~IEEE.}% <-this % stops a space% <-this % stops a space

\thanks{D. Freestone is with the Department
of Electrical and Electronic Engineering, University of Melbourne, Melbourne,
Vic, 3010 Australia {\tt\small dfreestone@bionicear.org}.}% <-this % stops a space
\thanks{P. Aram is with Sheffield...}
\thanks{M. Dewar is with Edinburgh...}
\thanks{K. Scerri is with Malta...}
\thanks{V. Kadirkamanathan is with Sheffield...}

\thanks{Manuscript received Month Day, Year; revised Month Day, Year.}}


% The paper headers
\markboth{Journal of \"{U}ber C\~{o}\~{o}l Engineering,~Vol.~1, No.~1, Christmas~2009}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Journals}

\maketitle

\begin{abstract}
Will save this for last.
\end{abstract}


% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Integro-Difference Equation (IDE), Neural Field Model, Cortical Connectivity.
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle

\section{Introduction}
\IEEEPARstart{T}{he} human brain is arguably the world's most complex system. Approximately 100 billion neurons and 60 trillion synapses operate in concert to process information, resulting in the cognition that determines our behavior. The brain's efficiency, robustness, and adaptability are unparalleled by any man-made device and, despite decades of concerted research, our understanding of its complex dynamics remains modest. This has led researchers to study different scales of dynamics: starting from individual proteins, synapses, and neurons; moving through to neuronal networks and ensembles of neuronal networks. While our understanding of the function of neurons is well developed, the overall behavior of the brain's meso and macro-scale dynamics remains largely a mystery.  Understanding the brain at this level is extremely important since this is the scale where pathologies such as epilepsy, Parkinson's disease and schizophrenia are manifested.

To date, there has been a considerable volume of work in generating physiologically plausible neural field models to fill the void of understanding brain dynamics at the meso/macroscopic scale. Mathematical neural field  models provide insights into the underlying physics and dynamics of electroencephalography (EEG) and magnetoencephalography (MEG) (see~\cite{Deco2008}~\cite{David2003} for recent reviews). These models have demonstrated possible mechanisms for the genesis of neural rhythms (such as the alpha and gamma rhythms) \cite{Liley1999} \cite{RENNIE2000}, epileptic seizure generation \cite{DaSilva2003},~\cite{Suffczynski2004} and~\cite{Wendling2005} and insights into other pathologies~\cite{Moran2008} \cite{Schiff2009} that would be impossible to gain from experimental data alone. Unfortunately, to date the use of these models in the clinic has been limited, since the neural field model are constructed for a general brain dynamics and pathologies almost always have unique underlying patient specific causes. Data from EEG and functional magnetic resonance imaging (fMRI) offers the patient specific link to macro-scale cortical dynamics, making these tools readily applicable to the clinic. However the underlying system properties, or system states, are hidden from EEG and fMRI data, making predictions of the underlying physiology inherently difficult.

For models to be clinically viable they must be patient specific. A possible approach to achieve this would be to use a general neural field model, like the Wilson and Cohen~\cite{Wilson1973} model or a neural mass model like the Jansen and Ritt model~\cite{Jansen1995}, and fit it to a patients EEG data. Fitting the neural models to individuals is a highly non-trivial task, and until very recently this has not been reported in the literature. An estimation frame work for neural field models known as dynamical causal modeling (DCM)~\cite{David2003} \cite{David2006} has recently been proposed for studying evoked potential dynamics. Via a Bayesian inference scheme, DCM estimates the long range connectivity structure between the specific isolated brain regions that best explains a given data set. The interconnected brain regions are modeled by the Jansen and Ritt equations. The approach has proven very useful in understanding specific hierarchical networks of neural information processing. Another recent publication describing a parameter estimation method with a neural field model used an unscented Kalman filter with the Wilson-Cowan neural field equations~\cite{schiff2008kalman}.  This work takes a more system theoretic approach to the neural estimation problem and marks a first step in what has the potential to revolutionize the treatment of many neurological diseases where therapeutic electrical stimulation is viable.

Generating patient specific models allows the application of a range of techniques from control systems theory, where tailored electrical stimulation could be used therapeutically in a closed loop fashion.  Currently available epileptic seizure prediction and control devices (i.e., the vagal nerve stimulator) are implemented in an ``open loop".  That is, the therapeutic electrical stimulation waveforms are adjusted for each patient by trial and error, disregarding the patient's neurodynamics and information about their particular pathologies. Given access to an accurate model, the application of control theory in these circumstances would allow for robust therapeutic stimulations.
The work from Schiff and Sauer~\cite{schiff2008kalman} successfully demonstrated  it is possible to estimate parameters from the WC equations. This motivates the question, what parameters are the most important? How patient specific do the assumed parameters have to be for the model to be useful? In this paper, we address both of these questions and provide a theoretical platform to perform patient-specific grey-box modeling of the human neocortex.

\section{The Importance of Cortical Structure}
Neural field models use statistics of the cortex to relate mean firing rates of pre-synaptic neural populations to post-synaptic mean membrane potentials. Each neural population represents a functional cortical processing unit referred to as a column. The columnar organization of the cortex is not discrete, but is continuous, where pyramidal cells are members of many columns. In general, cortical structure can be modeled in a physiologically plausible manner as being locally homogeneous (in short range intracortical connectivity) and heterogeneous (in long range corticocortical and corticothalamic connectivity)~\cite{Jirsa2009}~\cite{Qubbaj2007}. Locally, each column is thought be connected via symmetric short range local excitation, with surround inhibition~\cite{Braitenberg1998}. For example, this structural organization is most studied in the visual system, where the surrounding inhibition effectively tunes a cortical column to a particular receptive visual field~\cite{Sullivan2006}.

Recent studies using neural field models have demonstrated the theoretical implications of specific connectivity structure of the cortical columns in neural field models, where the connectivity kernel governs the bifurcation points of the system~\cite{Hutt2005} and types of oscillations that can be generated~\cite{Schmidt2009}. This implies that if we could estimate the connectivity structure for an individual, then we could capture the essential patient specific neurodynamics that lead to various oscillatory states. Estimating functional cortical connectivity via EEG measures is currently a highly active area of research. There is a lot of interest in understanding the hierarchy of brain regions that are involved in specific tasks, motivating the use of techniques like DCM. Other comparable methods are based on auto-regressive (AR) modeling of the EEG, MEG, or fMRI times series. Studies have concentrated on finding functional connectivity patterns using information contained in the AR coefficients using techniques like Granger causality~\cite{Hesse2003}, the direct transfer function~\cite{Kaminski1991}, and partial directed coherence~\cite{Sameshima1999}. Again, like the DCM approach, these methods estimate long range connectivity patterns that does not provide a clear relationship between the continuum field models and data.
%Analysis of the connectivity estimates provided from these approaches has involved graph theoretical techniques (small worldedness).

Until now, estimation of local intracortical connectivity structure has not been attempted. Recently, it has been shown that it is possible to estimate local coupling of systems governed by integro-difference equations by formulating a state-space model~\cite{Dewar2009}. The key development in this work was to define a state-space model with an order independent of the number of observations (or ECoG recording electrodes in this case). In addition, the appropriate model selection tools have been developed~\cite{Scerri2009} allowing for the application of this technique to neural fields. Modeling the neural dynamics within this framework has a distinct advantage over AR models, such that the number of parameters to define the connectivity basis functions (three in the paper) is considerably smaller than the number of AR coefficients typically required to achieve the relevant information criteria (AIC or BIC). In this paper, we demonstrate for the first time how intracortical connectivity can be inferred from ECoG data, based on a variant of the  Wilson and Cowan neural field model~\cite{Wilson1973}. This work provides a fundamental link between the theoretical advances in neural field modeling and patient specific data.
%Long range connectivity can be estimated using DTI~\cite{Knock2009}, however this short range connectivity can not..

\section{Neural Field Model}
In this section, we describe a variant of the  Wilson and Cowan neural field model~\cite{Wilson1973} that will be used to create our intracortical connectivity estimator. This model is descriptive of a range of neurodynamics of the cortex such as evoked potentials, visual hallucinations, and epileptic behaviour. It is also capable of generating complex patterns of activity such as Turing patterns, spirals, and traveling oscillations. The neural field model is popular due to being parsimonious yet having a strong link with the underlying physiology.
The model relates the average number of action potentials $g(r,t)$ arriving at position $r$ to the local post synaptic membrane voltage $v(r,t)$. The post-synaptic potentials generated at a neuronal population at location $r$ by action potentials arriving from all other connected populations at locations $r'$ can be described by
\begin{equation}\label{SpikesToPotential}
v\left( {r,t} \right) = \int_{ - \infty }^t {h\left( {t - t'} \right)g\left( {r,t'} \right)dt'}.
\end{equation}
The post-synaptic response kernel $h(t)$ is described by
\begin{equation}\label{SynapticRespKernel}
h(t) = \left\{ {\begin{array}{*{20}{c}}
   {\exp ( - \alpha t)} & {t \ge 0}  \\
   0 & {t < 0}  \\
\end{array}} \right.
\end{equation}
where $\alpha=\tau^{-1}$ and $\tau$ is the synaptic time constant. The synaptic response kernel can be more elaborate, using rise and fall time constants, however, this simple form serves our purposes. Nonlocal interactions between cortical populations are described by	
\begin{equation}\label{RateBasedInteractions}
g\left( r,t \right) = \int_\Omega  {w\left( r,r' \right)f\left( v\left( r',\bar t \right) \right)dr'} + e\left(r,t\right)
\end{equation}
where $\bar t = t - d\left| {r - r'} \right|$, $d$ is the propagation delay of action potentials, $f$ is spiking rate of populations $r'$, $w$ is the spatial connectivity kernel, and $\Omega$ is the spatial domain, representing a cortical sheet or surface. The term $de(r,t)$ can be considered the action potentials generated by unmodeled inputs and spontaneous background activity and is a Wiener process with mean $\mu$ incremental covariance $\Sigma_e(r)dt$. 

The average firing rate of the presynaptic neurons is related to the postsynaptic membrane potential by the sigmoidal activation function
\begin{equation}\label{ActivationFunction}
f\left( {{v_t}\left( {r'} \right)} \right) = \frac{{{\nu _0}}}{{1 + \exp \left( {\beta \left( {{v_0}\left( r \right) - {v}\left(r',\bar{t} \right)} \right)} \right)}}.
\end{equation}

Even though we will model  $v_0$ as being homogeneous across the cortical sheet we will express it as being spatially dependant allowing for generality when constructing the state-space representation in the next section. The parameters in equation~\ref{ActivationFunction} are detailed in Table 1. By combining Eqs.~\ref{SpikesToPotential} and \ref{RateBasedInteractions} we get the spatiotemporal model
\begin{equation}\label{FullDoubleIntModel}
v\left(r,t\right) = \int\limits_{-\infty}^t h\left(t - t'\right) \left(\int\limits_\Omega  w\left(r,r'\right)f\left( v\left( r',\bar t \right)\right)dr' + e\left(r,t\right)\right) dt'
\end{equation}
To arrive at the final form of the model we shall state the synaptic response kernel as a Green's function
\begin{equation}\label{GreensFuncDef}
Lh\left( t \right) = \delta \left( t \right),
\end{equation}
where $L$ is a temporal differential operator and $\delta(t)$ is the Dirac-delta function. This provides the most general form
\begin{equation}\label{GenForm}
L\left( \frac{\partial }{dt} \right)v\left( {r,t} \right) = \int_\Omega  {w\left( {r,r'} \right)f\left( {v\left( {r',\bar t} \right)} \right)dr'}  + de\left( r,t \right),
\end{equation}
where $L$ is a polynomial of order $n$ with constant coefficients that provides an $n^{th}$ order model. Most studies considering this model use first or second order time derivatives. We shall use the first order model giving
\begin{equation}\label{FinalForm1}
dv\left( {r,t} \right) + \alpha v\left( r,t \right)dt = \alpha\int_\Omega  {w\left( r,r' \right)f\left( {v\left( r',\bar t \right)} \right)dr'dt}  +de\left( r,t \right).
\end{equation}

\section{Discrete Time and Space Model}
We begin the derivation of the state-space model with the continuous time, continuous space IDE neural field model. 
Spatio-temporal interactions can be described by the continuous space continuous time stochastic differential equation
\begin{equation}\label{differential eq}	
dv\left( r,t \right) = \alpha \left( -v\left( r,t \right) + \int_\Omega  {w\left( r,r' \right)f\left( {v\left( r',t \right)} \right)dr'} \right) dt  + d e\left( r,t \right).
\end{equation}
To simplify the notation, but without loss of generality, \ref{FinalForm1} can be rewritten as
\begin{equation}\label{differential simplified}
	dv\left( r,t \right) = \alpha q\left(v(r,t) \right) dt  + d e\left( r,t \right)
\end{equation}
A one-step Euler method was used to approximate the nonlinear stochastic IDE to a discrete time form. 
\begin{equation}\label{euler}
	v\left(r,t\right) =  v(r,t_0) + \alpha \int_{t_0}^{t} q\left(v(r,\tau) \right) d\tau  + \int_{t_0}^{t} d e\left(r,\tau\right)
\end{equation}
Using an Euler approximation,~\ref{euler} can be approximated at each sampling time by
\begin{equation}\label{euler approx 1}
	v\left(r,t_{k+1} \right) =  v(r,t_k) + \alpha q\left(v(r,t_k)\right) (t_{k+1} - t_k) + (e(r,t_{k+1}) - e(r,t_k))
\end{equation}
which can be rewritten as
\begin{equation}\label{euler approx 2}
	v\left(r,t_k + \delta \right) =  v(r,t_k) + \delta \alpha q\left(v(r,t_k)\right) + e'(r,t_k)
\end{equation}
where $e'(r,t_k)$ is a zero mean Gaussian random variable with covariance $\delta\Sigma_e$. (Note that, because of the way the noise is defined in \ref{differential eq} it is not $\delta^2$ but $\delta$). Expanding $q\left(v(r,t_k)\right)$ we have
\begin{equation}\label{discrete time model}
	v\left(r,t_k + \delta \right) =  v(r,t_k) + \delta \alpha \left( -v\left( r,t_k \right) + \int_\Omega  {w\left( r,r' \right)f\left( {v\left( r',t_k \right)} \right)dr'} \right) + e'(r,t_k)
\end{equation}
which can be simplified to
\begin{equation}\label{discrete time model2}
	v\left(r,t_k + \delta \right) =  (1 - \delta \alpha) v(r,t_k) + \delta \alpha \int_\Omega  {w\left( r,r' \right)f\left( {v\left( r',t_k \right)} \right)dr'} + e'(r,t_k)
\end{equation}
Now we define the spatial aspect of the model on a regular square $i,j$ grid of neural masses, where the spatial step size $\Delta \mathbf{r}_i = \Delta \mathbf{r}_j = \Delta $ giving
\begin{equation}\label{discrete space}
	v\left(r_{ij},t_k + \delta \right) =  (1 - \delta \alpha)v(r_{ij},t_k) + \delta \alpha \Delta^2 \sum_{i=-I}^{i=I}{\sum_{j=-J}^{j=J}{\tilde{w}\left( r_{ij},r_{ij}' \right)f\left( v\left( r_{ij}',t_k \right) \right)} }+ e'(r_{i,j},t_k).
\end{equation}
where $\tilde{w}(r_{ij},r_{ij}')$ discretized connectivity kernel. In Eq~\ref{discrete space} we have used periodic boundary conditions to alleviate problems associated with edge effects. The connectivity kernel is in the form of a modified Mexican hat, and is described by the sum of three weighted Gaussian functions as
\begin{equation}
	\tilde{w}(r_{ij},r_{ij}') = \mathbf{\theta}^T e^{\frac{\left(\left(r_i'-r_i\right)\Delta\right)^2}{(2\mathbf{\sigma})^2} + \frac{\left(\left(r_j'-r_j\right)\Delta\right)^2}{(2\mathbf{\sigma})^2}}
\end{equation}
where
\begin{equation}
 \mathbf{\theta}=\begin{bmatrix} \mathbf{\theta}_{1}\\\mathbf{\theta}_{2}\\\mathbf \theta_{3}\end{bmatrix},\quad \mathbf{\sigma}=\begin{bmatrix}\mathbf{\sigma}_1\\\mathbf{\sigma}_2\\ \mathbf{\sigma}_3\end{bmatrix}.
\end{equation}.
The first Gaussian function (parametrized by $\mathbf{\theta}_1, \mathbf{\sigma}_1$) describes local excitation, the second describes lateral inhibition and the third describes medium range excitation. 
 
To generate data will approximate either local field potential recordings or intracranial EEG we have used the observation equation
\begin{equation}\label{ObservationEquation}
    y(r_n,t_k') = \int_\Omega  m( r_n,r' ) v( r',t_k' ) dr'  + \varepsilon ( r_n,t_k ),
\end{equation}
where $\varepsilon( n )\sim\mathcal{N}(0,\Sigma)$, $n$ indexes the iEEG electrodes, and $m(r_n,r)$ is the observation kernel relating the neural field to measurements accounting for electrodes characteristics (spatial averaging). In Eq.~\ref{ObservationEquation} we have indexed time by $t_k'$, which is a decimated version of $t_k$. 

All the model parameters are described in Table 1.


\begin{tabular}{ccc}
\hline
Parameter & Value & Reference\\
\hline
$\alpha$ & $100$ s$^{-1}$ &\\
\hline
$\nu_0$ & $1$ s$^{-1}$ &\\
\hline
$v_0$ & $6$ mV &\\
\hline
$\beta$ & $0.56$ spikes/mV$^{-1}$ & \\
\hline
$\delta$ & $1\times10^{-4}$ s &\\
\hline
$\Delta$ & $1$ &\\
\hline
$\theta_1$ &  &\\
\hline
$\theta_2$ &  &\\
\hline
$\theta_3$ &  &\\
\hline
$\sigma_1$ &  &\\
\hline
$\sigma_2$ &  &\\
\hline
$\sigma_3$ &  &\\
\hline
\end{tabular}


\section{Reduced Model For Estimation}

In order to implement standard estimation techniques we propose a decomposition of the field using a set of Gaussian basis functions. Decomposition allows a continuous field to be represented by a finite dimensional state vector. This allows for the application of standard nonlinear, non-Gaussian state estimation methods such as sequential Monte-Carlo techniques like particle filtering. The field decomposition is described by
\begin{equation}\label{DefFieldDecomp}
{v_t}\left( {\bf{r}} \right) = {\phi ^T}\left( {\bf{r}} \right){x_t}.
\end{equation}
where $\phi(r)$ is the Gaussian basis functions that are scaled by the the state vector. The connectivity kernel can all be decomposed into a set of basis functions
\begin{equation}\label{DefKernelDecomp}
k( \mathbf{r} , \mathbf{r'} ) = \theta^T\psi(\mathbf{r} , \mathbf{r'} )
\end{equation}
Will will assume we know the parametric form of the basis functions, where the parameter $\theta$ is unknown. Each connectivity basis function can be considered a layer in the WC model, representing short range excitation, surround inhibition and long range excitation. Making substitutions we get
\begin{equation}\label{DecompModel}
{\phi ^T}\left( \mathbf{r} \right){x_{t+1}} = T_s\theta^T\int_\Omega  {\psi \left( {{\bf{r}} , {\mathbf{r'}}} \right)f\left( {{\phi ^T}\left( {{\mathbf{r'}}} \right){x_t}} \right)d{\mathbf{r'}}}  - \lambda\phi^T(\mathbf{r})x_t + T_s{e_t}\left( {\mathbf{r}} \right).
\end{equation}
To isolate the state we cross multiply by $\phi(\mathbf{r})$ and integrate over $\mathbf{r}$
\begin{equation}\label{Poo}
\int_\Omega  {\phi \left( {\bf{r}} \right){\phi ^T}\left( {\bf{r}} \right)d{\bf{r}}{x_{t+1}}}  = T_s\int_\Omega  {\phi ( \mathbf{r} )\theta^T\int_\Omega  {\psi \left( {{\bf{r}} , {\bf{r'}}} \right)f\left( {{\phi ^T}\left( {{\bf{r'}}} \right){x_t}} \right)d{\bf{r'}}} d{\bf{r}}}  - \lambda\int_\Omega \phi(\mathbf{r})\phi^T(\mathbf{r})d\mathbf{r} x_t  + T_s\int_\Omega  {\phi \left( {\mathbf{r}} \right)e_t\left( {\mathbf{r}} \right)d{\mathbf{r}}}.
\end{equation}
Now we define
\begin{equation}\label{DefGamma}
\Gamma  = \int_\Omega  {\phi \left( {\bf{r}} \right){\phi ^T}\left( {\bf{r}} \right)d{\bf{r}}}.
\end{equation}
Substituting equation~\ref{DefGamma} into \ref{Poo} and rearranging gives
\begin{equation}\label{SS Model}
{x_{t+1}} = T_s{\Gamma ^{ - 1}}\int_\Omega  {\phi \left( {\bf{r}} \right)\theta^T\int_\Omega  {\psi \left( {{\bf{r}} , {\bf{r'}}} \right)f\left( {{\phi ^T}\left( {{\bf{r'}}} \right){x_t}} \right)d{\bf{r'}}} d{\bf{r}}} - \lambda x_t + T_s{\Gamma ^{ - 1}}\int_\Omega  {\phi \left( {\bf{r}} \right)e_t\left( {\bf{r}} \right)d{\bf{r}}}.
\end{equation}
We can rearrange to get
\begin{equation}\label{Homogeneous SS Model}
	x_{t+1} = T_s\Gamma ^{ - 1}\int_\Omega  {\phi \left( {\bf{r}} \right)\int_\Omega  {f\left( {{\phi ^T}\left( {{\bf{r'}}} \right){x_t}} \right)\psi^T \left( {{\bf{r}} - {\bf{r'}}} \right)d{\bf{r'}}} d{\bf{r}}}\theta - \lambda x_t + T_s{\Gamma ^{ - 1}}\int_\Omega  {\phi \left( {\bf{r}} \right)e_t\left( {\bf{r}} \right)d{\bf{r}}}.
\end{equation}
The disturbance term can be rewritten as 
\begin{equation}
	\Gamma ^{ - 1}\int_\Omega  {\phi \left( \bf{r} \right)e_t\left( \bf{r} \right)d\bf{r}} = \int_\Omega  {\phi \left( \bf{r} \right)\tilde{e}_t\left( \bf{r} \right)d\bf{r}},
\end{equation}
where $\tilde{e}_t(r)~N(0,R'R)$ and $R'$ is the Cholesky decomposition of $\Gamma^{-1}$. This provides the final state-space model
\begin{equation}\label{AbbrevSSModel}
x_{t + 1} = T_sq(x_t)\theta -\lambda x_t + T_s\int_\Omega  {\phi ( \mathbf{r} )\tilde{e}_t( \mathbf{r} )d\mathbf{r}}
\end{equation}
with observation equation
\begin{equation}\label{ObservationEquation}
    y_t( k ) = \int_\Omega  m( k,\mathbf{r} )\phi^T( \mathbf{r} ) d\mathbf{r}x_t  + \varepsilon ( k ),
\end{equation}

\subsection{Discrete Space For Simulation}

In this section we describe a discrete time, discrete space model that we are using for the simulations. Now we define the spatial aspect of the model on a regular square $i,j$ grid of neural masses, where the spatial step size $\Delta \mathbf{r}_i = \Delta \mathbf{r}_j = \Delta \mathbf{r}$ giving
\begin{equation}\label{DiscreteSpaceModelij}
	x_{t+1} = T_s\Gamma ^{ - 1}\sum_i \sum_j  {\phi \left( \mathbf{r} \right)\sum_i\sum_j  {f\left( \phi^T \left( \mathbf{r'} \right) x_t \right)\psi^T \left( \mathbf{r} - \bf{r'} \right)\Delta\mathbf{r'}^2} \Delta\mathbf{r}^2} \theta - \lambda x_t + T_s\sum_i\sum_j  {\phi \left( {\bf{r}} \right)\tilde{e}_t\left( \bf{r} \right)\Delta\mathbf{r}^2}.
\end{equation}
\section{Estimation of the Nonlinear Homogeneous IDE Neural Field Model}


\section{Comparison of Estimated Parameters and True Parameters}

\section{Discussion}

% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex,
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation Results}
%\label{fig_sim}
%\end{figure}

% Note that IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.
%
% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command, the
% \label for the overall figure must come after \caption.
% \hfil must be used as a separator to get equal spacing.
% The subfigure.sty package works much the same way, except \subfigure is
% used instead of \subfloat.
%
%\begin{figure*}[!t]
%\centerline{\subfloat[Case I]\includegraphics[width=2.5in]{subfigcase1}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{subfigcase2}%
%\label{fig_second_case}}}
%\caption{Simulation results}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.

% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table. Table text will default to
% \footnotesize as IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}

% Note that IEEE does not put floats in the very first column - or typically
% anywhere on the first page for that matter. Also, in-text middle ("here")
% positioning is not used. Most IEEE journals use top floats exclusively.
% Note that, LaTeX2e, unlike IEEE journals, places footnotes above bottom
% floats. This can be corrected via the \fnbelowfloat command of the
% stfloats package.

\section{Conclusion}
The conclusion goes here.

\appendices
\section{Ken's Discretization}
Spatio-temporal interactions can be described by the continuous space continuous time stochastic differential equation
\begin{equation}\label{differential eq}	
\tau dv\left( r,t \right) = \left( -v\left( r,t \right) + \int_\Omega  {w\left( r,r' \right)f\left( {v\left( r',t \right)} \right)dr'} \right) dt  + d e\left( r,t \right).
\end{equation}
where $e\left( r,t \right)$ is a zero mean temporal Wiener process with incremental covariance $\Sigma_e(r)dt$. (not too sure about this noise since it needs to be white temporally but coloured spatially - which in continuous space-time I have never seen written ... has anyone?)
To simplify the notation, but without loss of generality, \ref{differential eq} can be rewritten as
\begin{equation}\label{differential simplified}
	dv\left( r,t \right) = q\left(v(r,t) \right) dt  + d e\left( r,t \right)
\end{equation}
Had $q\left(v(r,t) \right)$ been a linear function in $v$, then \ref{differential simplified} could be rewritten as
\begin{equation}\label{differential linear}
	dv\left( r,t \right) = Av(r,t)dt  + d e\left( r,t \right)
\end{equation}
Considering temporal sampling instances $\delta$ apart given by $\{t_k = 0,~1,~2, \ldots$ \}, then a solution to \ref{differential linear} at each sampling time is given by
\begin{equation}\label{linear solution}
	v(r,t_k+\delta) = \exp\{A(\delta)\}x(t) + \int_{t_k}^{t_k+\delta}\exp\{A(t_k+\delta-\tau)\}de(r,\tau)
\end{equation}
Let the discrete time random variable $e'(r,t_k)$ be
\begin{equation}\label{discrete noise}
	e'(r,t_k) = \int_{t_k}^{t_k+\delta}\exp\{A(t_k+\delta-\tau)\}de(r,\tau)
\end{equation}
then, $e'(r,t_k)$ is a zero mean, temporally white noise term with covariance
\begin{equation}\label{covariance of noise}
	E\left[e'(r,t_k),e'^\top(r,t_k) \right] = \int_t^{t+\delta}\exp\{A(t_k+\delta-\tau)\} \Sigma_e(r) \exp\{A(t+\delta-\tau)\} d\tau
\end{equation}

But we are not so luck as to be able to go from \ref{differential simplified} to \ref{differential linear} and thus I doubt if we can obtain an exact solution to \ref{differential simplified} without an approximation. Agreed?

A possible approximate solution is the one you have already used. Thus, interpreting the difference equation \ref{differential simplified} as an integral equation gives
\begin{equation}\label{euler}
	v\left(r,t\right) =  v(r,t_0) + \int_{t_0}^{t} q\left(v(r,\tau) \right) d\tau  + \int_{t_0}^{t} d e\left(r,\tau\right)
\end{equation}
Using an Euler approximation, \ref{euler} can be approximated at each sampling time by
\begin{equation}\label{euler approx 1}
	v\left(r,t_{k+1} \right) =  v(r,t_k) + q\left(v(r,t_k)\right) (t_{k+1} - t_k) + (e(r,t_{k+1}) - e(r,t_k))
\end{equation}
which can be rewritten as
\begin{equation}\label{euler approx 2}
	v\left(r,t_k + \delta \right) =  v(r,t_k) + \delta q\left(v(r,t_k)\right) + e'(r,t_k)
\end{equation}
where $e'(r,t_k)$ is a zero mean Gaussian random variable with covariance $\delta\Sigma_e$. (Note that, because of the way the noise is defined in \ref{differential eq} it is not $\delta^2$ but $\delta$)

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
\section{Colored noise}
We have
\begin{equation}
v_{t + 1}\left( \mathbf{r} \right)  = T_s\int_\Omega  {k\left( {{\bf{r}} , {\bf{r'}}} \right)f\left( {{v_t}\left( \bf{r'} \right)} \right)d{\bf{r'}}} + \lambda v_t\left( {\bf{r}} \right) + T_se_t\left( {\mathbf{r}} \right).
\end{equation}
where the disturbance $e_t(\mathbf r)$ is a zero-mean normally distributed noise process, spatially coloured but temporally independent, with covariance 

\begin{equation}
cov(e_t(\mathbf{s}),e_{t+\tau}(\mathbf{r}))=
\begin{cases}
\beta\left(\mathbf{s}-\mathbf{r}\right), & \tau=0 \\
0 & \mathrm{otherwise}
\end{cases}
\label{eq:FieldCovariance}
\end{equation}
The final state-space model is given by
\begin{equation}
x_{t + 1} = T_sq(x_t)\theta -\lambda x_t + T_s\Gamma^{-1}\int_\Omega  {\phi ( \mathbf{r} )e_t( \mathbf{r} )d\mathbf{r}}
\end{equation}
then the covariance of the noise term is given by
\begin{eqnarray}
\mathbf \Sigma_w&{}={}&T_s^2 \times \mathbf{\Gamma}^{-1}\mathbf E[\int_{\mathcal{S}}\boldsymbol\phi\left(\mathbf s\right)e_t\left(\mathbf s\right)d\mathbf s\int_{\mathcal{S}}\boldsymbol\phi\left(\mathbf r\right)^{\top}e_t\left(\mathbf r\right)d\mathbf r]\mathbf{\Gamma}_{x}^{- \top} \nonumber \\
&=&T_s^2 \times\mathbf{\Gamma}^{-1}\iint\limits_{\mathcal{S}}\boldsymbol\phi\left(\mathbf s\right) \mathbf E[e_t\left(\mathbf s\right)e_t\left(\mathbf r\right)]\boldsymbol\phi\left(\mathbf r\right)^{\top}d\mathbf r d\mathbf s\mathbf{\Gamma}^{- \top} \nonumber\\
&=&{}T_s^2 \times\mathbf{\Gamma}^{-1}\iint\limits_{\mathcal{S}}\boldsymbol\phi\left(\mathbf s\right) \beta\left(\mathbf s- \mathbf r \right)\boldsymbol\phi\left(\mathbf r\right)^{\top}d\mathbf r d\mathbf s\mathbf{\Gamma}^{- \top} 
\end{eqnarray}

\section{Parameter estimation using LS}
\begin{equation}
 \mathbf x_{t+1}=\mathbf x_{t}+T_s \mathbf Q(\mathbf x_t)\mathbf \theta-\alpha T_s\mathbf x_t+T_s\boldsymbol\Gamma^{-1}\int_\Omega\boldsymbol\phi(\mathbf r)\varepsilon_t(\mathbf r)d\mathbf r
\label{eq:statespacemodel}
\end{equation}
where the matrix $\mathbf Q_{n_x \times n_{\theta}}(\mathbf x_t)$ is defined as
\begin{equation}
 Q_{n_x \times n_{\theta}}=\boldsymbol\Gamma^{-1}\int_\Omega\boldsymbol\phi(\mathbf r)\int_\Omega f(\boldsymbol\phi^\top(\mathbf r')\mathbf x_t)\boldsymbol\psi^\top(\mathbf r-\mathbf r')d\mathbf r'd\mathbf r
\end{equation}
and $T_s$ is the sampling period. Least Square method can be used as (\ref{eq:statespacemodel}) is linear in parameters. At each time instant we have

\begin{eqnarray}
 \mathbf x_{1}&=&\mathbf x_{0}+T_s \mathbf Q(\mathbf x_0)\mathbf \theta-\alpha T_s\mathbf x_0+T_s\mathbf e_0 \nonumber \\
 \mathbf x_{2}&=&\mathbf x_{1}+T_s \mathbf Q(\mathbf x_1)\mathbf \theta-\alpha T_s\mathbf x_1+T_s\mathbf e_1\nonumber\\
&\vdots& \nonumber\\
 \mathbf x_{n}&=&\mathbf x_{n-1}+T_s \mathbf Q(\mathbf x_{n-1})\mathbf \theta-\alpha T_s\mathbf x_{n-1}+T_s\mathbf e_{n-1}
\end{eqnarray}
in matrix form
\begin{equation}
 \begin{bmatrix} \mathbf x_{1}\\\mathbf x_{2}\\\vdots\\\mathbf x_{n}\end{bmatrix}-\begin{bmatrix} \mathbf x_{0}\\\mathbf x_{1}\\\vdots\\\mathbf x_{n-1}\end{bmatrix}=T_s\begin{bmatrix}\mathbf Q(\mathbf x_0)&-\mathbf x_{0}\\\mathbf Q(\mathbf x_1)&-\mathbf x_{1}\\\vdots\\ \mathbf Q(\mathbf x_{n-1})&-\mathbf x_{n-1}\end{bmatrix}\begin{bmatrix}\mathbf \theta \\ \alpha\end{bmatrix}+T_s\begin{bmatrix}\mathbf e_0\\\mathbf e_1\\\vdots\\\mathbf e_{n-1}\end{bmatrix}
\end{equation}
or in compact form we have
\begin{equation}
 \mathbf Z=\mathbf X \mathbf W+\boldsymbol \xi
\end{equation}
where
\begin{equation}
 \mathbf Z=\begin{bmatrix} \mathbf x_{1}-\mathbf x_{0}\\\mathbf x_{2}-\mathbf x_{1}\\\vdots\\\mathbf x_{n}-\mathbf x_{n-1}\end{bmatrix},\quad \mathbf X=T_s\begin{bmatrix}\mathbf Q(\mathbf x_0)&-\mathbf x_{0}\\\mathbf Q(\mathbf x_1)&-\mathbf x_{1}\\\vdots\\ \mathbf Q(\mathbf x_{n-1})&-\mathbf x_{n-1}\end{bmatrix},\quad \mathbf W=\begin{bmatrix}\mathbf \theta \\ \alpha\end{bmatrix} \text{and}\quad \boldsymbol \xi=T_s\begin{bmatrix}\mathbf e_0\\\mathbf e_1\\\vdots\\\mathbf e_{n-1}\end{bmatrix}
\end{equation}
using the method of Least Square $ \mathbf W$ can be estimated
\begin{equation}
 \mathbf W\approx(\mathbf X^\top\mathbf X)^{-1}\mathbf X^\top\mathbf Z
\end{equation}
% use section* for acknowledgement
\section{clarification for the derivation} 
I ignore noise here. From equation (\ref{SpikesToPotential}) we can say $h$ is the foundamental solution to the equation
\begin{equation}
 Lv(r,t)=g(r,t)
\label{eq:LV}
\end{equation}
where $L$ is a temporal differential operator, since $h(t)$ is the foundamental solution to (\ref{eq:LV}) it can be written as a Green's function
\begin{equation}
 Lh(t)=\delta(t)
\end{equation}
where $L=1+\tau\frac{d}{dt}$ therefore equation (\ref{eq:LV}) becomes:
\begin{equation}
\tau\frac{dv(r,t)}{dt}+ v(r,t)=g(r,t)
\end{equation}
substituting (\ref{RateBasedInteractions}) for $g(r,t)$ gives
\begin{equation}
\frac{dv\left( {r,t} \right)}{dt} + \frac{1}{\tau} v\left( r,t \right) = \frac{1}{\tau}\int_\Omega  {w\left( r,r' \right)f\left( {v\left( r',\bar t \right)} \right)dr'} 
\end{equation}



\section*{Acknowledgment}
The authors would like to thank...

\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% references section

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,BrainIDE}


% biography section
%
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{biography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

\begin{IEEEbiography}{Dean R. Freestone}
Biography text here.
\end{IEEEbiography}

% if you will not have a photo at all:
\begin{IEEEbiography}{Parham Aram}
Biography text here.
\end{IEEEbiography}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

\begin{IEEEbiography}{Michael Dewar}
Biography text here.
\end{IEEEbiography}

\begin{IEEEbiography}{Kenneth Scerri}
Biography text here.
\end{IEEEbiography}

\begin{IEEEbiography}{Visakan Kadirkamanathani}
Biography text here.
\end{IEEEbiography}

\begin{IEEEbiography}{Other Bosses}
Biography text here.
\end{IEEEbiography}

% that's all folks
\end{document} 