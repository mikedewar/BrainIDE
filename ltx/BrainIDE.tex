%% bare_jrnl.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/


\documentclass[onecolumn,draftcls]{IEEEtran}

\usepackage{graphicx}
\usepackage{color}                    % For creating coloured text and background
\usepackage{amsmath,amssymb,amsfonts} % Typical maths resource packages
% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}
% TODO environment
\newcommand{\todo}[1]{\textsf{\emph{\textbf{\textcolor{blue}{#1}}}}}

\begin{document}

% can use linebreaks \\ within to get better formatting as desired
\title{Estimation of Spatiotemporal Integro-Difference Equations for Dynamical Neural Field Models}

\author{Dean~Freestone,~\IEEEmembership{Graduate Student Member,~IEEE,}
        Parham~Aram,~\IEEEmembership{Graduate Student Member,~IEEE,}
        Michael~Dewar,~\IEEEmembership{Member,~IEEE,}
        Kenneth~Scerri,~\IEEEmembership{Member,~IEEE,}
        and Visakan~Kadirkamanathan,~\IEEEmembership{Member,~IEEE,}
        and~other~Bosses/Supervisors,~\IEEEmembership{Member,~IEEE.}% <-this % stops a space% <-this % stops a space

\thanks{D. Freestone is with the Department
of Electrical and Electronic Engineering, University of Melbourne, Melbourne,
Vic, 3010 Australia {\tt\small dfreestone@bionicear.org}.}% <-this % stops a space
\thanks{P. Aram is with Sheffield...}
\thanks{M. Dewar is with Edinburgh...}
\thanks{K. Scerri is with Malta...}
\thanks{V. Kadirkamanathan is with Sheffield...}

\thanks{Manuscript received Month Day, Year; revised Month Day, Year.}}


% The paper headers
\markboth{Journal of \"{U}ber C\~{o}\~{o}l Engineering,~Vol.~1, No.~1, Christmas~2009}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Journals}

\maketitle

\begin{abstract}
Will save this for last.
\end{abstract}


% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Integro-Difference Equation (IDE), Neural Field Model, Cortical Connectivity.
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle

\section{Introduction}

\IEEEPARstart{T}{he} human brain is arguably the world's most complex system. Approximately 100 billion neurons and 60 trillion synapses operate in concert to process information, resulting in the cognition that determines our behavior. The brain's efficiency, robustness, and adaptability are unparalleled by any man-made device and, despite decades of concerted research, our understanding of its complex dynamics remains modest. This has led researchers to study different scales of dynamics: starting from individual proteins, synapses, and neurons; moving through to neuronal networks and ensembles of neuronal networks. Whilst our understanding of the function of neurons is well developed, the overall behavior of the brain's meso and macro-scale dynamics remains largely a mystery.  Understanding the brain at this level is extremely important since this is the scale where pathologies such as epilepsy, Parkinson's disease and schizophrenia are manifested.

To date, there has been a considerable volume of work in generating physiologically plausible neural field models to fill the void of understanding brain dynamics at the meso/macroscopic scale. Mathematical neural field  models have provided insights into the underlying physics and dynamics of electroencephalography (EEG) and magnetoencephalography (MEG) (see~\cite{Deco2008}~\cite{David2003} for recent reviews). These models have demonstrated possible mechanisms for the genesis of neural rhythms (such as the alpha and gamma rhythms) \cite{Liley1999} \cite{RENNIE2000}, epileptic seizure generation \cite{DaSilva2003},~\cite{Suffczynski2004} and~\cite{Wendling2005} and insights into other pathologies~\cite{Moran2008} \cite{Schiff2009} that would be impossible to gain from experimental data alone. Unfortunately, to date the use of these models in the clinic has been limited, since the neural field model are constructed for a general brain dynamics and pathologies almost always have unique underlying patient specific causes. Data from EEG and functional magnetic resonance imaging (fMRI) offers the patient specific link to macro-scale cortical dynamics, making these tools readily applicable to the clinic. However the underlying system properties, or system states, are hidden from EEG and fMRI data, making predictions of the underlying physiology inherently difficult.

For models to be clinically viable they must be patient specific. A possible approach to achieve this would be to use a general neural field model, like the Wilson and Cohen~\cite{Wilson1973} or Jansen and Ritt model~\cite{Jansen1995}, and fit it to a patients EEG data. Fitting the general neural field models to individuals is a highly non-trivial task, and until very recently this has not been reported in the literature. An estimation frame work for neural field model known as dynamical causal modeling (DCM)~\cite{David2003} \cite{David2006} has recently been proposed for studying evoked potential dynamics. Via a Bayesian inference scheme, DCM estimates the long range connectivity structure between the specific isolated brain regions that best explains a given data set. The approach has proven very useful in understanding specific hierarchical networks of neural information processing. Another recent publication describing parameter estimation method with a neural field model used an unscented Kalman filter with the Wilson-Cowan neural field equations~\cite{schiff2008kalman}.  This work takes a more system theoretic approach to the neural estimation problem and marks a first step in what has the potential to revolutionize the treatment of many neurological diseases where therapeutic electrical stimulation is viable.

Generating patient specific models allows that application of a range of techniques from control systems theory, where tailored electrical stimulation could be used therapeutically in a closed loop fashion.  Currently available epileptic seizure prediction and control devices (i.e., the vagal nerve stimulator) are implemented in an ``open loop".  That is, the therapeutic electrical stimulation waveforms are adjusted for each patient by trial and error, disregarding the patient's neurodynamics and information about their particular pathologies. Given access to an accurate model, the application of control theory in these circumstances would allow for robust therapeutic stimulations.
The work from Schiff and Sauer~\cite{schiff2008kalman} successfully demonstrated  estimation of two parameters from the WC equations, whilst the other parameters were fixed. This motivates the question, what parameters are the most important? How patient specific do the assumed parameters have to be for the model to be useful? In this paper we address both of these questions and provide a theoretical platform to perform patient specific grey-box modeling of the human neocortex.

\section{The Importance of Cortical Structure}

The neural field models use statistics of the cortex to relate mean firing rates of pre-synaptic neural populations to post-synaptic mean membrane potentials. Each neural population represents a functional cortical processing unit referred to as a column. The columnar organization of the cortex is not discrete, but is continuous, where pyramidal cells are members of many columns. In general, cortical structure can be modeled in a physiologically plausible manner as being locally homogeneous (in short range intracortical connectivity) and heterogeneous (in long range corticocortical and corticothalamic connectivity)~\cite{Jirsa2009}~\cite{Qubbaj2007}. Locally, each column is thought be connected via symmetric short range local excitation, with surround inhibition~\cite{Braitenberg1998}. For example, this structural organization is most studied in the visual system, where the surrounding inhibition effectively tunes a cortical column to a particular receptive visual field~\cite{Sullivan2006}.

Recent studies using neural field models have demonstrated the theoretical implications of the connectivity structure of the cortical columns, where the connectivity kernel governs the bifurcation points of the models~\cite{Hutt2005} and types of oscillations that can be generated~\cite{Schmidt2009}. This implies that if we could estimate the connectivity structure for an individual, than we could model patient specific neurodynamics. Estimating functional cortical connectivity via EEG measures is currently a highly active area of research. There is a lot of interest in understanding the hierarchy of brain regions are involved in specific tasks, motivating the use of techniques like DCM. Other comparable methods are based on auto-regressive (AR) modeling of the EEG, MEG or fMRI times series. Studies have concentrated on finding functional connectivity patterns using information contained in the AR coefficients using techniques like Granger causality~\cite{Hesse2003}, the direct transfer function~\cite{Kaminski1991}, and partial directed coherence~\cite{Sameshima1999}). Again, like the DCM approach, these methods estimate long range connectivity patterns that does not provide a clear relationship between the continuum field models and data. Analysis of the connectivity estimates provided from these approaches has involved graph theoretical techniques (small worldedness).

Until now, estimation of local intracortical connectivity structure has not been attempted. Recently, it has been shown that it is possible to estimate local coupling of systems governed by integro-difference equations by formulating a state-space model~\cite{Dewar2009}. The key development in this work was to define the state-space model order independent of the number of observations (or ECoG recording electrodes in this case). In addition, the appropriate model selection tools have been developed~\cite{Scerri2009} allowing for the application of this technique to neural fields. Modeling the neural dynamics within this framework has a distinct advantage over AR models, such that the number of parameters to define the connectivity basis functions is considerably smaller than the number of AR coefficients typically required to achieve the relevant information criteria. In this paper we demonstrate for the first time how intracortical connectivity can be inferred from ECoG data, based on a variant of the  Wilson and Cowan neural field model~\cite{Wilson1973}. This work provides a fundamental link between the theoretical advances in neural field modeling and patient specific data.

Long range connectivity can be estimated using DTI~\cite{Knock2009}, however this short range connectivity can not..

\section{Neural Field Model}
In this section we describe a variant of the  Wilson and Cowan neural field model~\cite{Wilson1973} that will be used to create our connectivity estimator. This model has shown to be descriptive of a range of neurodynamics of the cortex such as evoked potentials, visual hallucinations and epileptic behavior. It is also capable of generating complex patterns of activity such as Turing patterns, spirals and traveling oscillations. The general form of the model is described here. The post-synaptic potentials generated at a neuronal population $r$ by action potentials arriving from all other connected populations at position $r'$ can be described by
\begin{equation}\label{SpikesToPotential}
v\left( {r,t} \right) = \int_{ - \infty }^t {h\left( {t - t'} \right)f\left( {r,t'} \right)dt'} + e\left( {r,t} \right).
\end{equation}
Equation~\ref{SpikesToPotential} relates, $f$, or the average number of action potentials arriving at $r$ to the post synaptic potential at $r$, to the local membrane voltage. The term $e(r,t)$ can be considered the potential generated by unmodeled inputs and spontaneous background activity. The post-synaptic response kernel $h(t)$ is described by
\begin{equation}\label{SynapticRespKernel}
h(t) = \left\{ {\begin{array}{*{20}{c}}
   {\exp ( - \alpha t)} & {t \ge 0}  \\
   0 & {t < 0}  \\
\end{array}} \right.
\end{equation}
where $\alpha=\tau^{-1}$ and $\tau$ is the synaptic time constant. The synaptic response kernel can be more elaborate, using a rise and fall time constants, however this simple form serves our purposes. Nonlocal interactions between cortical populations are described by	
\begin{equation}\label{RateBasedInteractions}
f\left( {r,t} \right) = \int_\Omega  {w\left( {r,r'} \right)p\left( {v\left( {r',\bar t} \right)} \right)dr'}
\end{equation}
where $\bar t = t - d\left| {r - r'} \right|$, $p$ is spiking rate of populations $r'$, $w$ is the spatial connectivity kernel, $\Omega$ is the spatial domain, representing a cortical sheet or surface and $d$ is the propagation delay from $r'$ to $r$.

The average firing rate of the presynaptic neurons is related to the postsynaptic membrane potential by the Heaviside step function
\begin{equation}\label{RateOfPreSynPops}
p\left( {v\left( {r,\bar t} \right)} \right) = {\nu_0}u\left( {v\left( {r,\bar t} \right) - {v_0}\left( r \right)} \right)
\end{equation}
\begin{equation}\label{HeavisideDef}
u\left( {v\left( {r,\bar t} \right) - {v_0}\left( r \right)} \right) = \left\{ {\begin{array}{*{20}{c}}
   1 & {v\left( {r,\bar t} \right) \ge {v_0}\left( r \right)}  \\
   0 & {otherwise}  \\
\end{array}} \right.
\end{equation}
Even though we will model  $v_0$ as being homogeneous across the cortical sheet we will express it as being spatially dependant allowing for generality when constructing the state-space representation in the next section. The parameters in equation~\ref{RateOfPreSynPops} are... $v_0$ is typically 6~mV, $\nu_0$ is 5~$s^{-1}$ (from Wendling and David papers). By combining equations~\ref{SpikesToPotential} and \ref{RateBasedInteractions} we get the spatiotemporal model
\begin{equation}\label{FullDoubleIntModel}
v\left(r,t\right) = \int\limits_{-\infty}^t\int\limits_\Omega  h\left(t - t'\right)w\left(r,r'\right)p\left( v\left( r',\bar t \right)\right)dr' dt'+e\left(r,t\right)
\end{equation}
To arrive at the final form of the model we shall state the synaptic response kernel as a Green's function
\begin{equation}\label{GreensFuncDef}
Lh\left( t \right) = \delta \left( t \right),
\end{equation}
where $L$ is a temporal differential operator and $\delta(t)$ is the Dirac-delta function. This provides the most general form
\begin{equation}\label{GenForm}
L\left( {\frac{\partial }{{dt}}} \right)v\left( {r,t} \right) = \int_\Omega  {w\left( {r,r'} \right)p\left( {v\left( {r',\bar t} \right)} \right)dr'}  + e\left( {r,t} \right),
\end{equation}
where $L$ is a polynomial of order $n$ with constant coefficients that provides an $n^{th}$ order model. Most studies considering this model use first or second order time derivatives. We shall use the first order model giving
\begin{equation}\label{FinalForm1}
\frac{{dv\left( {r,t} \right)}}{{dt}} + \alpha{v\left( {r,t} \right)} = \int_\Omega  {w\left( {r,r'} \right)p\left( {v\left( {r',\bar t} \right)} \right)dr'}  +e\left( {r,t} \right).
\end{equation}
\subsection{Alternate Model Description I}
An alternate and equivalent way to remove the temporal integration using the Laplace transform . We start by considering a discrete neural mass $i$ with inputs with zero delay from another discrete mass $j$ and
\begin{equation}\label{TwoDiscreteMasses}
{v_i}\left( t \right) = \int_{ - \infty }^t {{h_j}(t - t'){w_{ij}}p\left( {{v_j}\left( t \right)} \right)dt'},
\end{equation}
where $w_{ij}$ can be considered the synaptic efficiency from $j$ to $i$. Now we take the Laplace transform $\mathcal{L}(v_i(t))=V(s)$ yielding
\begin{eqnarray}
% \nonumber to remove numbering (before each equation)
  {V_i}\left( s \right) &=& {H_j}\left( s \right){w_{ij}}P\left( {{V_j}\left( s \right)} \right) \nonumber \\
   &=& {\left( {s + \alpha} \right)^{ - 1}}{w_{ij}}P\left( {{V_j}\left( s \right)} \right)  \\
  \Rightarrow s{V_i}\left( s \right) + \alpha{V_i}\left( s \right) &=& {w_{ij}}P\left( {{V_j}\left( s \right)} \right).
\end{eqnarray}
Now taking the inverse Laplace transform we get
\begin{equation}\label{InverseLaplace}
\frac{{d{v_i}\left( t \right)}}{{dt}} + \alpha{v_i}\left( t \right) = {w_{ij}}p\left( {{v_j}\left( t \right)} \right).
\end{equation}
Next we convert the model from discrete neural masses to a continuum by
\begin{equation}\label{Discrete2Continuum}
\frac{{dv\left( {r,t} \right)}}{{dt}} + \alpha{v\left( {r,t} \right)} = \int_\Omega  {w\left( {r,r'} \right)p\left( {v\left( {r',\bar t} \right)} \right)dr'}  + e\left( {s,t} \right).
\end{equation}
The neural field model is popular due to being parsimonious yet having a strong link with the underlying physiology.

\subsection{Alternate Model Description II}

\begin{figure*}
\centering
\includegraphics[scale=0.4]{NeuralMassFigure.eps}
\caption[NeuralMassModel]{This figure describes the basic interactions of neural masses that constitute the neural field model.}
\label{NeuralMassModel}
\end{figure*}

An alternative description of the model is as follows. The model can be thought of as a continuum of neural masses. To illustrate this idea we shall first describe the interaction of discrete neural masses as seen in Figure~\ref{NeuralMassModel}. Assuming post-synaptic potentials sum linearly, the membrane potential at the soma of neuron $i$ is described as
\begin{equation}\label{VoltageFromPSPs}
{v_i}\left( t \right) = \sum\nolimits_{j,k} {{w_{ij}}{h_{j}}\left( t - {t_k}\right)} u\left( {t - {t_k}} \right),
\end{equation}
where $w_{ij}$ is the synaptic weight from neuron $j$ to $i$, $h(t)$ is the synaptic response kernel, $S$ is the Heaviside step function and action potential arrive at neuron $i$ at times $t_k$. Alternatively the membrane potential can described by the convolution of synaptic response kernel with the pre-synaptic firing rate $r_j(t)$ where
\begin{equation}\label{VoltageFromRate}
{v_i}\left( t \right) = \sum\nolimits_j {\int_{ - \infty }^t {{w_{ij}}{h_j}\left( {t - t'} \right){r_j}\left( t \right)} } dt'.
\end{equation}
To convert the equation system into and integral differential system we shall take derivative of~\ref{Eq1.1} yielding
\begin{eqnarray}
% \nonumber to remove numbering (before each equation)
  \frac{{d{v_i}\left( t \right)}}{{dt}} &=& \sum\nolimits_{j,k} {\left( {{w_{ij}}{h_j}\left( t- {t_k} \right)\frac{d}{{dt}}u\left( {t - {t_k}} \right) + {w_{ij}}u\left( {t - {t_k}} \right)\frac{d}{{dt}}{h_j}\left( t- {t_k} \right)} \right)} \nonumber \\
   &=& \sum\nolimits_{j,k} {{w_{ij}}{h_j}\left( t- {t_k} \right)\delta \left( {t - {t_k}} \right)}  - \sum\nolimits_{j,k} {{w_{ij}}u\left( {t - {t_k}} \right)\alpha_j{h_j}\left( t- {t_k} \right)}   \nonumber \\
   &=& \sum\nolimits_{j,k} {{w_{ij}}\delta \left( {t - {t_k}} \right)}  - {v_i}\left( t \right)\sum\nolimits_j {\alpha_j} \nonumber  \\
   &=& \sum\nolimits_j {{w_{ij}}{\nu_j}\left( t \right)}  - {v_i}\left( t \right)\sum\nolimits_j {\alpha_j} \nonumber \\
   &=& \sum\nolimits_j {{w_{ij}}p\left( {{v_j}\left( t \right)} \right)}  - {v_i}\left( t \right)\sum\nolimits_j {\alpha_j}.
\end{eqnarray}
This form can easily be extended to represent a continuous field.

\subsection{Assumptions and Simplifications}
We assume the velocity of the action potentials is infinite. This is also a common assumption that is made when modeling intracortical circuits. As stated in Wilson and Cohen's original works, the time constant $\tau$ is much greater than the axonal propagation delays, therefore it is reasonable to approximate the the delays as being infinite. Recent studies have demonstrated the importance of having a fast, but finite, propagation velocity for models to make realistic and complete predictions of cortical dynamics (Coombes paper, and Atay paper). The purpose of this paper, however, is not to precisely simulate complex cortical dynamics, but to create a useful platform where estimation of hidden states is possible for the purpose of detecting abnormal neural states with intentions of generating informed patient specific control sequences. Therefore, the model simply has to be 'good enough' for this purpose. Starting with this simplified version of the general model is a logical choice. The next assumption we will make is spatial homogeneity such that synaptic interaction between neural populations is only dependant on their spatial separation, and not on their spatial location. Again, this is a common assumption when dealing with intracortical connectivity. This assumption is not essential for our IDE estimation techniques, where a heterogeneous structure is possible. The IDE estimation is derived for a general heterogeneous structure, however for clarity of results we shall only present the heterogeneous case. We will assume the synaptic response kernel has the same shape (scaled by the connectivity kernel) irrespective of the respective presynaptic population. In this case inhibition can be modeled by having a negative response kernel and excitation with a positive kernel, where the kernel provides the (scaled) polarity. This reduced the model to a single field. I am not sure how this will limit the dynamics??
With these assumptions we have the model in the form
\begin{equation}\label{SimplifiedModel}
\frac{{dv\left( {s,t} \right)}}{{dt}} = \int_\Omega  {w\left( {s-r} \right)P\left( {v\left( {r,t} \right)} \right)dr}  - \alpha{v\left( {s,t} \right)} + e\left( {s,t} \right).
\end{equation}

\subsection{IDE Formulation}

\begin{equation}\label{new_spatiotemporal_kernels}
\frac{{dv\left( {s,t} \right)}}{{dt}} = \int_\Omega  {k\left( {s - r} \right)\tilde v\left( {r,t} \right) - \alpha \delta \left( {s - r} \right)\bar v\left( {s,t} \right)dr}  + e\left( {s,t} \right)
\end{equation}

\begin{equation}\label{define_v_tilde}
\tilde v\left( {r,t} \right) = \left\{ {\begin{array}{*{20}{c}}
   {v\left( {r,t} \right)} & {v\left( {r,t} \right) \ge {v_0}}  \\
   0 & {otherwise}  \\
\end{array}} \right.
\end{equation}

\begin{equation}\label{define_v_bar}
\bar v\left( {r,t} \right) = \left\{ {\begin{array}{*{20}{c}}
   {v\left( {r,t} \right)} & {v\left( {r,t} \right) < {v_0}}  \\
   0 & {otherwise}  \\
\end{array}} \right.
\end{equation}


\section{State-Space Neural Field Model}
In this section we shall reconstruct the model in a state-space form that is a novel representation of the neural field equations. The benefit of restating the model in this format is that it becomes easier to simulate and allows the connectivity to be inferred from data. To simplify the notation in the derivation we will let the external input $\epsilon(t)=0$. Also, we shall define
\begin{equation}\label{Eq14}
W\left( {s,r} \right) = {\theta ^T}\psi \left( {s,r} \right)
\end{equation}
and
\begin{equation}\label{Eq15}
{v_t}\left( s \right) = \phi {\left( s \right)^T}{x_t},
\end{equation}
where $\theta \in \mathbb{R}^n$ is a vector of unknown parameters (amplitudes of kernels), $\psi(s,r)$ is a matrix that can be considered a field that is scaled by $\theta$, and $\phi(s)$ is a vector of basis functions that decomposes the field and $x_t \in\mathbb{R}^n$ is the state vector at sampling time $t$. The voltage field at $t$ is comprised of a scaled version (by the state $x_t$) of the basis functions $\phi(s)$.
 The spatial kernel $W$ and the field $v$ are decomposed using the same basis functions, and we will define them as being complete and orthogonal. Also, we shall define
\begin{equation}\label{Eq16}
\Phi \left( s \right) = \int_\Omega  {\psi \left( {s,r} \right)\phi {{\left( r \right)}^T}dr}
\end{equation}
and
\begin{equation}\label{Eq17}
\Gamma \left( s \right) = \int_\Omega  {\psi \left( {s,r} \right){{\bar v}_0}\left( r \right)dr}.
\end{equation}
By substituting equations~\ref{Eq14} and \ref{Eq15} into equation~\ref{Eq13} we get
\begin{align}\label{Eq18}
 {v_{t + 1}}\left( s \right) = \frac{{{R_0}}}{2}{\theta ^T}\int_\Omega  {\psi \left( {s,r} \right){{\bar v}_0}\left( r \right)dr} \nonumber \\
  + \frac{{{R_0}}}{2}\beta {\theta ^T}\int_\Omega  {\psi \left( {s,r} \right)\phi {{\left( r \right)}^T}dr} {x_t} \\
  - \frac{1}{\tau }\phi {\left( s \right)^T}{x_t} \nonumber
\end{align}
and substituting in equations~\ref{Eq16} and \ref{Eq17} into equation~\ref{Eq18} we get
\begin{equation}\label{Eq19}
{v_{t + 1}}\left( s \right) = \frac{{{R_0}}}{2}{\theta ^T}\Gamma \left( s \right) + \frac{{{R_0}}}{2}\beta {\theta ^T}\Phi \left( s \right){x_t} - \frac{1}{\tau }\phi {\left( s \right)^T}{x_t}.
\end{equation}
By substituting the field for the basis functions weighted by the state vector we get
\begin{equation}\label{Eq20}
\phi {\left( s \right)^T}{x_{t + 1}} = \frac{{{R_0}}}{2}{\theta ^T}\Gamma \left( s \right) + \frac{{{R_0}}}{2}\beta {\theta ^T}\Phi \left( s \right){x_t} - \frac{1}{\tau }\phi {\left( s \right)^T}{x_t}.
\end{equation}
Multiplying both sides by $\phi(s)$ and integrating over $s$ we get
\begin{align}\label{21}
 \int_\Omega  {\phi \left( s \right)\phi {{\left( s \right)}^T}ds{x_{t + 1}}}  = \frac{{{R_0}}}{2}\int_\Omega  {\phi \left( s \right){\theta ^T}\Gamma \left( s \right)ds}  \nonumber \\
  + \frac{{{R_0}}}{2}\beta \int_\Omega  {\phi \left( s \right){\theta ^T}\Phi \left( s \right)ds} {x_t} \\
  - \frac{1}{\tau }\int_\Omega  {\phi \left( s \right)\phi {{\left( s \right)}^T}ds} {x_t}. \nonumber
\end{align}
Now we define
\begin{equation}\label{22}
\Psi  = \int_\Omega  {\phi \left( s \right)\phi {{\left( s \right)}^T}ds}
\end{equation}
and
\begin{equation}\label{23}
\Upsilon  = \int_\Omega  {\phi \left( s \right){\theta ^T}\Phi \left( s \right)ds}
\end{equation}
and
\begin{equation}\label{24}
\Pi  = \int_\Omega  {\phi \left( s \right){\theta ^T}\Gamma \left( s \right)ds},
\end{equation}
where $\Psi$ is a square positive definite matrix. Substituting equations~\ref{22}, ~\ref{23} and \ref{24} into equation~\ref{21} we get
\begin{equation}\label{25}
\Psi {x_{t + 1}} = \frac{{{R_0}}}{2}\Pi  + \frac{{{R_0}}}{2}\beta \Upsilon {x_t} - \frac{1}{\tau }\Psi {x_t}.
\end{equation}
Since $\Psi$ is a square positive definite matrix we can invert it to isolate $x_t$ yielding
\begin{equation}\label{26}
{x_{t + 1}} = \frac{{{R_0}}}{2}{\Psi _x}^{ - 1}\Pi  + \left( {\frac{{{R_0}}}{2}\beta {\Psi _x}^{ - 1}\Upsilon  - \frac{1}{\tau }{\bf{I}}} \right){x_t},
\end{equation}
where \textbf{I} is the identity matrix. The model can now be written in the canonical state-space form
\begin{equation}\label{27}
\begin{array}{c}
 {x_{t + 1}} = A{x_t} + Bu \\
 {v_t}\left( s \right) = C{x_t}, \\
 \end{array}
\end{equation}
where
\begin{equation}\label{28}
\begin{array}{l}
 A = \frac{{{R_0}}}{4}\beta {\Psi _x}^{ - 1}\Upsilon  - \frac{1}{\tau }{\bf{I}} \\
 B = \frac{{{R_0}}}{2}{\Psi _x}^{ - 1}\Pi  \\
 C = {\phi ^T}\left( s \right) \\
 u = 1. \\
 \end{array}
\end{equation}
Next we shall demonstrate the stability of the model and in the following section the estimator for the state vector and connectivity structure is derived.

\section{Stability Analysis}
Using the analytically calculated basis functions. Need to show that the linearized model will not blow up.

\section{Estimation of the IDE Neural Field Model}
Based on the other papers.

\section{Comparison of Estimated and True Connectivity}
Try and estimate the linear and nonlinear model based on the linear estimator. Might work ok?

\subsection{Linear Model}

\subsection{Nonlinear Model}

\section{Discussion}

% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex,
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation Results}
%\label{fig_sim}
%\end{figure}

% Note that IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.
%
% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command, the
% \label for the overall figure must come after \caption.
% \hfil must be used as a separator to get equal spacing.
% The subfigure.sty package works much the same way, except \subfigure is
% used instead of \subfloat.
%
%\begin{figure*}[!t]
%\centerline{\subfloat[Case I]\includegraphics[width=2.5in]{subfigcase1}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{subfigcase2}%
%\label{fig_second_case}}}
%\caption{Simulation results}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.

% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table. Table text will default to
% \footnotesize as IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}

% Note that IEEE does not put floats in the very first column - or typically
% anywhere on the first page for that matter. Also, in-text middle ("here")
% positioning is not used. Most IEEE journals use top floats exclusively.
% Note that, LaTeX2e, unlike IEEE journals, places footnotes above bottom
% floats. This can be corrected via the \fnbelowfloat command of the
% stfloats package.

\section{Conclusion}
The conclusion goes here.

\appendices
\section{}
Appendix one text goes here.

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
\section{}
Appendix two text goes here.

% use section* for acknowledgement
\section*{Acknowledgment}
The authors would like to thank...

\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% references section

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,BrainIDE}


% biography section
%
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{biography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

\begin{IEEEbiography}{Dean Freestone}
Biography text here.
\end{IEEEbiography}

% if you will not have a photo at all:
\begin{IEEEbiography}{Parham Aram}
Biography text here.
\end{IEEEbiography}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

\begin{IEEEbiography}{Michael Dewar}
Biography text here.
\end{IEEEbiography}

\begin{IEEEbiography}{Kenneth Scerri}
Biography text here.
\end{IEEEbiography}

\begin{IEEEbiography}{Visakan Kadirkamanathani}
Biography text here.
\end{IEEEbiography}

\begin{IEEEbiography}{Other Bosses}
Biography text here.
\end{IEEEbiography}

% that's all folks
\end{document} 