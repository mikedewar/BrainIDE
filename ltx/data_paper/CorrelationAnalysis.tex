%
%  untitled
%
%  Created by Parham Aram on 2010-07-14.
%  Copyright (c) 2010 . All rights reserved.
%
\documentclass[]{article}

% Setup for fullpage use
\usepackage{fullpage}

\usepackage[pdftex]{graphicx}


% More symbols
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{color}
\newcommand{\dean}[1]{\textsf{\emph{\textbf{\textcolor{red}{#1}}}}} 
\newcommand{\red}{\textcolor{red}}
\newcommand{\parham}[1]{\textsf{\emph{\textbf{\textcolor{blue}{#1}}}}} 
\newcommand{\cyan}{\textcolor{cyan}}
\begin{document}


% \renewcommand{\theequation}{S1.\arabic{equation}}

\newpage
\section*{Neural Field Model}
The stochastic IDE form of the Wilson-Cowan neural field  formulation is given by 
\begin{equation}
	\label{eq:DiscreteTimeModel} 
	v_{t+1}\left(\mathbf{r}\right) = 
	\xi v_t\left(\mathbf{r}\right) + 
	T_s g_t\left( \mathbf{r}\right) 
	+ e_t\left(\mathbf{r}\right), 
\end{equation}
where $g_t(\mathbf r)$ describes non-local interactions between cortical populations at positions $\mathbf{r}$ and $\mathbf{r}'$ 
\begin{equation}
	\label{eq:RateBasedInteractions} g_t\left( \mathbf{r}\right) = (w \ast f(v_t))(\mathbf r),
\end{equation}
The mapping between the membrane voltage and the electrophysiological data, denoted by $\mathbf{y}_t$, is modeled using the observation function that incorporates sensors with a spatial extent by
\begin{equation}\label{eq:ObservationEquation}
	y_t(\mathbf{r}) = (m\ast v_t)(\mathbf r) + \varepsilon_t(\mathbf{r}).
\end{equation}
\section*{Estimation of Kernel Support}
The spatial relationship between consecutive observations is governed by the connectivity kernel. Therefore, the cross-correlation between consecutive observations is used to estimate the support of the connectivity kernel. The cross-correlation is defined as
\begin{align}
	R_{y_{t+1}y_t}(\boldsymbol{\tau})& =\int_{\Omega} y_{t+1}(\mathbf{r}) y_t(\mathbf{r}+\boldsymbol{\tau}) d\mathbf{r}
 \nonumber \\
 &=(y_{t+1} \star y_t)(\boldsymbol\tau),
\end{align}
where $\tau$ is the spatial shift.  Now substituting equation~\ref{eq:ObservationEquation} for $y_{t+1}(\mathbf{r})$  gives 
\begin{equation}
	R_{y_{t+1}y_t}(\boldsymbol{\tau}) = \left[  (m\ast v_{t+1})(\boldsymbol\tau)+\varepsilon_{t+1}(\boldsymbol\tau)\right] \star  y_t(\boldsymbol{\tau}) 
\end{equation}
Next the equation~\ref{eq:DiscreteTimeModel} is substituted in for $v_{t+1}(\boldsymbol\tau)$ giving 
\begin{align}\label{eq:CrossCorrKernel}
	R_{y_{t+1}y_t}(\boldsymbol{\tau}) &= \left[  \xi(m\ast v_{t})(\boldsymbol\tau)+T_s(m\ast g_{t})(\boldsymbol\tau)+(m\ast e_{t})(\boldsymbol{\tau})+\varepsilon_{t+1}(\boldsymbol\tau)\right] \star  y_t(\boldsymbol{\tau}) \nonumber \\
&=\xi(m\ast v_{t})(\boldsymbol\tau)\star y_t(\boldsymbol\tau)+T_s(m\ast g_t)(\boldsymbol\tau)\star y_t(\boldsymbol\tau) \nonumber \\
&+(m\ast  e_t)(\boldsymbol\tau) \star y_t(\boldsymbol\tau)+\varepsilon_{t+1}(\boldsymbol\tau) \star y_t(\boldsymbol\tau).
\end{align}
Rearranging we get
\begin{align}
	R_{y_{t+1}y_t}(\boldsymbol{\tau}) - \xi(m\ast v_{t})(\boldsymbol\tau)\star y_t(\boldsymbol\tau) -(m\ast  e_t)(\boldsymbol\tau) \star y_t(\boldsymbol\tau) - \varepsilon_{t+1}(\boldsymbol\tau) \star y_t(\boldsymbol\tau) &= T_s(m\ast g_t)(\boldsymbol\tau)\star y_t(\boldsymbol\tau).
\end{align}
The cross-correlation is simplified by recognizing that 
\begin{align}
\xi(m\ast v_{t})(\boldsymbol\tau)\star y_t(\boldsymbol\tau)&=\xi(y_t-\varepsilon_t)(\boldsymbol\tau) \star y_t(\boldsymbol\tau)\nonumber \\
&=\xi \left(R_{y_ty_t}(\boldsymbol{\tau})-\parham{$n_y$}\sigma_{\epsilon}^2  \delta_{K}\left(\boldsymbol\tau\right)\right),
\end{align}
% \begin{align}
% 	\int_{\Omega}\int_{\Omega} m(\mathbf{r}-\mathbf{r}')&\xi v_t(\mathbf{r}')d\mathbf{r}'y_t(\mathbf{r}+\boldsymbol{\tau}) d\mathbf{r} = \nonumber \\ 
%  &\xi \left(R_{y_ty_t}(\boldsymbol{\tau})-\parham{$n_y$}\sigma_{\epsilon}^2  \delta_{K}\left(\boldsymbol\tau\right)\right),
% \end{align}
where $\delta_{K}\left(.\right)$ denotes Kronecker delta. Since the observation and process noise at time $t$ are uncorrelated we have
\begin{align}
(m\ast  e_t)(\boldsymbol\tau) \star y_t(\boldsymbol\tau)=0
\end{align}
and also
\begin{align}
 \varepsilon_{t+1}(\boldsymbol\tau) \star y_t(\boldsymbol\tau)=0
\end{align}
giving 
\begin{align}
	R_{y_{t+1}y_t}(\boldsymbol{\tau}) &= \xi \left(R_{y_ty_t}(\boldsymbol{\tau})-\sigma_{\epsilon}^2  \delta_{K}\left(\boldsymbol\tau\right)\right)\nonumber \\
	&+ T_s(m\ast g_t)(\boldsymbol\tau) \star y_t(\boldsymbol\tau)
\end{align}
Rearranging and substituting in equation~\ref{eq:RateBasedInteractions} for $g_t(\boldsymbol\tau)$ gives
\begin{align}\label{eq:BeforeLinearisation}
	R_{y_{t+1}y_t}(\boldsymbol{\tau}) &-\xi \left(R_{y_ty_t}(\boldsymbol{\tau})-\sigma_{\epsilon}^2  \delta_{K}\left(\boldsymbol\tau\right)\right) =  T_s(m\ast w*f(v_t))(\boldsymbol\tau) \star y_t(\boldsymbol\tau)
\end{align}  
Next, the nonlinear function $f(\cdot)$ is approximated by the simple linear relationship
\begin{equation}
	f\left(v_t(\mathbf{r})\right) \approx \varsigma v_t(\mathbf{r})
\end{equation} 
giving
\begin{align}
	R_{y_{t+1}y_t}(\boldsymbol{\tau}) &-\xi \left(R_{y_ty_t}(\boldsymbol{\tau})-\sigma_{\epsilon}^2  \delta_{K}\left(\boldsymbol\tau\right)\right) = \varsigma T_s (m\ast w\ast v_t)(\boldsymbol\tau)\star y_t(\boldsymbol\tau)
\end{align} 
Using the commutativity property of convolution, and substituting for $(m\ast v_t)(\boldsymbol\tau)$ from \ref{eq:ObservationEquation} we have
\begin{align}
	R_{y_{t+1}y_t}(\boldsymbol{\tau})&-\xi \left(R_{y_ty_t}(\boldsymbol{\tau})-\sigma_{\epsilon}^2  \delta_{K}\left(\boldsymbol\tau\right)\right) =  \varsigma T_s \left( w\ast \left[y_t-\varepsilon_t \right]\right)(\boldsymbol\tau) \star y_t(\boldsymbol\tau)
\end{align}
A property of cross-correlation and convolution is $(a \ast b)(\boldsymbol\tau) \star c(\boldsymbol\tau) = a(-\boldsymbol\tau)\ast(b \star c)(\boldsymbol\tau)$ (see Appendix I for a proof), so we can write

\begin{align}
	R_{y_{t+1}y_t}(\boldsymbol{\tau})-\xi \left(R_{y_ty_t}(\boldsymbol{\tau})-\sigma_{\epsilon}^2  \delta_{K}\left(\boldsymbol\tau\right)\right) &=  \varsigma T_s  w(-\boldsymbol\tau)\ast (\left[y_t-\varepsilon_t \right] \star y_t)(\boldsymbol\tau)\nonumber \\
&= \varsigma T_s  w(-\boldsymbol\tau) \ast\left[R_{y_ty_t}(\boldsymbol\tau)-\sigma_{\varepsilon}^2\delta_K(\boldsymbol\tau) \right] 
\end{align}
% In a case  connectivity kernel is isotropic $ w(-\boldsymbol\tau)= w(\boldsymbol\tau)$ and we have \parham{(investigate the conjugate Fourier transform $ W^{*}(\nu)=W(-\nu)$ for real $w$ (reality condition)) then it might be correct for anisotropic kernel as well}
\begin{align}
	R_{y_{t+1}y_t}(\boldsymbol{\tau}) -\xi \left(R_{y_ty_t}(\boldsymbol{\tau})-\sigma_{\epsilon}^2  \delta_{K}\left(\boldsymbol\tau\right)\right)& = \varsigma T_s \left[ w(-\boldsymbol\tau)\ast R_{y_ty_t}(\boldsymbol\tau)\right] -\varsigma T_s \sigma^2_{\varepsilon}w(-\boldsymbol\tau) 
\end{align}

The solution of the above equation for the connectivity kernel is a deconvolution. This can be approached from a number of different standpoints. The simplest solution is to use the convolution theorem. Taking Fourier transform gives
\begin{align}
 \mathcal{F}\left(R_{y_{t+1}y_t}(\boldsymbol{\tau})\right)-\xi\mathcal{F}\left(R_{y_ty_t}(\boldsymbol{\tau})-\sigma^2_{\varepsilon}\delta_K(\boldsymbol\tau)\right)&=\varsigma T_s\mathcal{F}\left(w(-\boldsymbol\tau)\right)\times \mathcal F\left(R_{y_ty_t}(\boldsymbol\tau)\right)-\varsigma T_s\sigma_{\varepsilon}^2\mathcal{F}\left(w(-\boldsymbol\tau)\right) \nonumber \\
&=\varsigma T_s\mathcal{F}\left(w(-\boldsymbol\tau)\right)\left[\mathcal{F}\left(R_{y_ty_t}(\boldsymbol{\tau})-\sigma^2_{\varepsilon}\delta_K(\boldsymbol\tau)\right) \right] 
\end{align}
 which gives
\begin{equation}\label{eq:FourierKernelSolution}
	\mathcal{F}\left(w(-\boldsymbol\tau)\right) = \frac{1}{\varsigma T_s}\frac{\mathcal{F}\left(R_{y_{t+1}y_t}(\boldsymbol{\tau})\right)}{\mathcal{F}\left(R_{y_ty_t}(\boldsymbol{\tau})-\sigma^2_{\varepsilon}\delta_K(\boldsymbol\tau)\right)}-\xi.
\end{equation}
It should be noted that since the connectivity kernel is a real function the following relation holds \cite{Bracewell2000}
\begin{equation}
 \mathcal{F}\left(w(\boldsymbol\tau)\right)=\overline{\mathcal{F}\left(w(-\boldsymbol\tau)\right)}
\end{equation}
where over-bar denotes complex conjugate operator. Taking the inverse Fourier tranform yields
\begin{equation}\label{eq:KernelSolution}
	w(\boldsymbol{\tau}) = \frac{1}{\varsigma T_s}\mathcal{F}^{-1}\left\{\frac{\overline{\mathcal{F}\left(R_{y_{t+1}y_t}(\boldsymbol{\tau})\right)}}{\overline{\mathcal{F}\left(R_{y_ty_t}(\boldsymbol{\tau})\right)}-\sigma^2_{\varepsilon}}-\xi\right\}.
\end{equation}

\newpage

Next, the nonlinear function $f(\cdot)$ is approximated by linearising about the threshold. The linearised activation function is found by taking the first order Taylor expansion. The derivative of the activation function is
\begin{align}
 f'(v_t\left(\mathbf{r}\right))=& \frac{\varsigma}{\left(1 + \exp \left( \varsigma \left( v_0 - v_t\left(\mathbf{r}\right) \right) \right)\right)^2} \times \exp \left( \varsigma \left( v_0 - v_t\left(\mathbf{r}\right) \right) \right) \nonumber \\
=&\frac{\varsigma}{1 + \exp \left( \varsigma \left( v_0 - v_t\left(\mathbf{r}\right) \right) \right)} \times \left(1-\frac{1}{1 + \exp \left( \varsigma \left( v_0 - v_t\left(\mathbf{r}\right) \right) \right)}\right) \nonumber \\
=& \varsigma f(v_t\left(\mathbf{r}\right))\left( 1-f( v_t\left(\mathbf{r}\right)\right).
\end{align}
The linearised activation function is 
\begin{align}
	\hat{f}(v_t\left(\mathbf{r}\right)) &= f(v_0) + f'(v_0)(v_t\left(\mathbf{r}\right) - v_0) \\
	&= \frac{2 + \varsigma(v_t\left(\mathbf{r}\right) - v_0)}{4}. 
\end{align}
Substituting this back into the RHS of Eq.~\ref{eq:BeforeLinearisation} gives
\begin{align}
	T_s(m \ast g_t) \star y_t(\boldsymbol\tau) &= \frac{T_s}{4}(m \ast w \ast (2 + \varsigma v_t - \varsigma v_0)))(\boldsymbol\tau) \star y_t(\boldsymbol\tau) \\
	&\approx \frac{T_s}{4}(m\ast w \ast (2 -\varsigma v_0) + \varsigma m\ast w\ast v_t) (\boldsymbol\tau) \star y_t(\boldsymbol\tau) \\
&\approx \frac{T_s}{4}(c_1 + \varsigma w \ast m\ast v_t) (\boldsymbol\tau) \star y_t(\boldsymbol\tau) \\
&\approx \frac{T_s}{4} c_1 \star y_t(\boldsymbol\tau) + \frac{T_s}{4}\varsigma w \ast m\ast v_t (\boldsymbol\tau) \star y_t(\boldsymbol\tau) \\
&\approx \frac{T_s\varsigma}{4}( w \ast m\ast v_t ) (\boldsymbol\tau) \star y_t(\boldsymbol\tau) + c_2 \\
&\approx \frac{T_s\varsigma}{4}( w \ast \left[y_t-\varepsilon_t \right] ) (\boldsymbol\tau) \star y_t(\boldsymbol\tau) + c_2 \\
&\approx \frac{T_s\varsigma}{4} w(-\boldsymbol\tau) \ast (\left[y_t-\varepsilon_t \right]\star y_t ) (\boldsymbol\tau) + c_2 \\ 
&\approx \frac{T_s\varsigma}{4} w(-\boldsymbol\tau) \ast \left[R_{y_ty_t}(\boldsymbol\tau) - \sigma_{\varepsilon}^2 \delta_K(\boldsymbol\tau) \right] + c_2,
\end{align}
where the constants $c_1$ and $c_2$ are 
\begin{align}
	c_1 &= m\ast w\ast (2 - \varsigma v_0) \\
	c_2 &= \frac{T_s}{4}c_1 \star y_t(\boldsymbol\tau).
\end{align}
Substituting back into \ref{eq:BeforeLinearisation} gives
\begin{equation}
	R_{y_{t+1}y_t}(\boldsymbol{\tau}) -\xi \left(R_{y_ty_t}(\boldsymbol{\tau})-\sigma_{\epsilon}^2  \delta_{K}\left(\boldsymbol\tau\right)\right) \approx \frac{T_s\varsigma}{4} w(-\boldsymbol\tau) \ast \left[R_{y_ty_t}(\boldsymbol\tau) - \sigma_{\varepsilon}^2 \delta_K(\boldsymbol\tau) \right] + c_2.
\end{equation}
Now taking the Fourier transform of both sides gives
\begin{equation}
	\mathcal{F}(R_{y_{t+1}y_t}(\boldsymbol{\tau})) - \xi \left( \mathcal{F}\left(R_{y_ty_t}(\boldsymbol{\tau}) \right) - \sigma_{\epsilon}^2  \right) \approx \frac{T_s\varsigma}{4}\mathcal{F}(w(-\boldsymbol\tau)) (\mathcal{F}(R_{y_ty_t}(\boldsymbol\tau)) - \sigma_{\varepsilon}^2) + c_2\delta_K(\boldsymbol\nu).
\end{equation}
Now rearranging to get an expression for $w(\boldsymbol\tau)$
\begin{align}
	\mathcal{F}(R_{y_{t+1}y_t}(\boldsymbol{\tau})) - \xi \left( \mathcal{F}\left(R_{y_ty_t}(\boldsymbol{\tau}) \right) - \sigma_{\epsilon}^2  \right) - c_2\delta_K(\boldsymbol\nu) &\approx \frac{T_s\varsigma}{4}\mathcal{F}(w(-\boldsymbol\tau)) (\mathcal{F}(R_{y_ty_t}(\boldsymbol\tau)) - \sigma_{\varepsilon}^2) \\
	\frac{\mathcal{F}(R_{y_{t+1}y_t}(\boldsymbol{\tau}))}{\mathcal{F}(R_{y_ty_t}(\boldsymbol\tau)) - \sigma_{\varepsilon}^2} - \xi - \frac{c_2\delta_K(\boldsymbol\nu)}{\mathcal{F}(R_{y_ty_t}(\boldsymbol\tau)) - \sigma_{\varepsilon}^2} &\approx \frac{T_s\varsigma}{4}\mathcal{F}(w(-\boldsymbol\tau)) \\ 
	\frac{\mathcal{F}(R_{y_{t+1}y_t}(\boldsymbol{\tau})) -c_2\delta_K(\boldsymbol\nu) }{\mathcal{F}(R_{y_ty_t}(\boldsymbol\tau)) - \sigma_{\varepsilon}^2} - \xi &\approx \frac{T_s\varsigma}{4}\mathcal{F}(w(-\boldsymbol\tau))
\end{align}

\newpage
\section*{Estimation of Disturbance Support}
The observation auto-correlation at time $t+1$ can be expressed as
\begin{align}
	R_{y_{t+1}y_{t+1}}(\boldsymbol{\tau})& =\int_{\Omega} y_{t+1}(\mathbf{r}) y_{t+1}(\mathbf{r}+\boldsymbol{\tau}) d\mathbf{r}
 \nonumber \\
 &=(y_{t+1} \star y_{t+1})(\boldsymbol\tau),
\end{align}
similar to  kernel support calculation $R_{y_{t+1}y_{t+1}}(\boldsymbol{\tau})$ can be written as 
\begin{equation}
	R_{y_{t+1}y_{t+1}}(\boldsymbol{\tau}) = \left[  \xi(m\ast v_{t})(\boldsymbol\tau)+T_s(m\ast g_{t})(\boldsymbol\tau)+(m\ast e_{t})(\boldsymbol{\tau})+\varepsilon_{t+1}(\boldsymbol\tau)\right] \star  y_{t+1}(\boldsymbol{\tau}) 
\end{equation}
By noting 
\begin{align}\label{eq:term1}
  \xi(m\ast v_{t})(\boldsymbol\tau)\star  y_{t+1}(\boldsymbol{\tau})&=\xi R_{y_ty_{t+1}}(\boldsymbol{\tau}),
\end{align}
and
\begin{align}\label{eq:term2}
 \varepsilon_{t+1}(\boldsymbol\tau)\star y_{t+1}(\boldsymbol\tau)&=\sigma_{\epsilon}^2\delta_K(\boldsymbol{\tau})
\end{align}
the auto-correlation result can be simplified
\begin{align}\label{eq:Auto&CrossNoisy}
	R_{y_{t+1}y_{t+1}}(\boldsymbol{\tau}) &= \xi R_{y_ty_{t+1}}(\boldsymbol{\tau})+ T_s (m\ast g_t)(\boldsymbol\tau) \star y_{t+1}(\boldsymbol\tau) \nonumber \\
	&+(m\ast e_t)(\boldsymbol\tau)\star y_{t+1}(\boldsymbol\tau)+\sigma_{\epsilon}^2\delta_K(\boldsymbol{\tau}).
\end{align}
Substituting in equation~\ref{eq:RateBasedInteractions} for $g_t(\boldsymbol\tau)$ and using linear approximation for $f(.)$, the second term in \eqref{eq:Auto&CrossNoisy} becomes
\begin{align}
	 T_s (m\ast g_t)(\boldsymbol\tau) \star y_{t+1}(\boldsymbol\tau) = \varsigma T_s(m\ast w*v_t)(\boldsymbol\tau) \star y_{t+1}(\boldsymbol\tau)
\end{align}
Using the commutativity property of convolution, and substituting for $(m\ast v_t)(\boldsymbol\tau)$ from \ref{eq:ObservationEquation} we have
\begin{align}\label{eq:term3}
	T_s (m\ast g_t)(\boldsymbol\tau) \star y_{t+1}(\boldsymbol\tau)&=  \varsigma T_s \left( w\ast \left[y_t-\varepsilon_t \right]\right)(\boldsymbol\tau) \star y_{t+1}(\boldsymbol\tau) \nonumber \\
&=\varsigma T_s(w\ast y_t)(\boldsymbol{\tau}) \star  y_{t+1}(\boldsymbol{\tau}) \nonumber \\
&=\varsigma T_s w(-\boldsymbol\tau)\ast R_{y_ty_{t+1}} (\boldsymbol{\tau}). 
% &=\varsigma T_s w(\boldsymbol\tau)\ast R_{y_ty_{t+1}} (\boldsymbol{\tau}).
\end{align}
the third term in \eqref{eq:Auto&CrossNoisy} can be simplified as
\begin{align}\label{eq:term3Noisy}
(m\ast e_t)(\boldsymbol\tau)\star y_{t+1}(\boldsymbol\tau)&= \left(m \ast e_t\right)(\boldsymbol\tau) \star\left[m\left(\boldsymbol\tau\right) \ast \left[\xi v_t\left(\boldsymbol\tau\right) + 
	T_s \varsigma \left(w \ast v_t\right)(\boldsymbol \tau)
	+ e_t\left(\boldsymbol{\tau}\right) \right]+\varepsilon_{t+1}(\boldsymbol\tau) \right]\nonumber \\
	&=\left(m \ast e_t\right)(\boldsymbol\tau)\star\left(m \ast e_t\right)(\boldsymbol\tau)
\end{align}
Note that other terms in \ref{eq:term3Noisy} are all zero. A property of cross-correlation and convolution is $(a \ast b)(\boldsymbol\tau) \star (a \ast b)(\boldsymbol\tau)=(a \star a)(\boldsymbol\tau)\ast(b \star b)(\boldsymbol\tau)$ (see Appendix II for a proof), using the isotropy property of the observation kernel we can write
\begin{align}\label{eq:term4}
\left(m \ast e_t\right)(\boldsymbol\tau)\star\left(m \ast e_t\right)(\boldsymbol\tau)&=\left(m \star m\right)(\boldsymbol\tau)\ast\left(e_t \star e_t\right)(\boldsymbol\tau) \nonumber\\
&=m(-\boldsymbol\tau)\ast m(\boldsymbol\tau)\ast \gamma(\boldsymbol\tau) \nonumber \\
&=(m\ast m \ast \gamma)(\boldsymbol\tau)
\end{align}
Using results from \ref{eq:term1}, \ref{eq:term2}, \ref{eq:term3} and \ref{eq:term4}, $R_{y_{t+1}y_{t+1}}$ can be calculated as
\begin{align}
	R_{y_{t+1}y_{t+1}}(\boldsymbol{\tau})= \xi R_{y_ty_{t+1}}(\boldsymbol{\tau})+\varsigma T_s w(-\boldsymbol\tau) \ast R_{y_ty_{t+1}}(\boldsymbol{\tau})+(m\ast m \ast \gamma)(\boldsymbol\tau)+\sigma_{\epsilon}^2\delta_K(\boldsymbol{\tau}).
\end{align}
Taking Fourier transform and rearranging we have
\begin{align}
 \mathcal F\left\lbrace (m\ast m \ast \gamma)(\boldsymbol\tau)\right\rbrace&= \mathcal F\left\lbrace R_{y_{t+1}y_{t+1}}(\boldsymbol{\tau})\right\rbrace-\xi\mathcal F\left\lbrace R_{y_{t}y_{t+1}}(\boldsymbol{\tau})\right\rbrace-\varsigma T_s \mathcal F\left\lbrace w(\boldsymbol{-\tau})\right\rbrace \times \mathcal F \left\lbrace R_{y_ty_{t+1}}(\boldsymbol{\tau})\right\rbrace-\sigma_{\epsilon}^2 
\end{align}
Substituting in for $\mathcal{F}\left\lbrace w(-\boldsymbol{\tau})\right\rbrace$ From \ref{eq:FourierKernelSolution} we get
% \begin{align}
%  W(\boldsymbol{\nu}) &=\frac{1}{\varsigma T_s}\left(\frac{S_{y_{t+1}y_t}(\boldsymbol{\nu})}{S_{y_ty_t}(\boldsymbol{\nu})-\sigma_{\epsilon}^2}-\xi\right).
% \end{align}
% \begin{equation}
% 	\mathcal{F}\left\lbrace w(\boldsymbol{\tau}) \right\rbrace  = \frac{1}{\varsigma T_s}\frac{\mathcal{F}\left\lbrace R_{y_{t+1}y_t}(\boldsymbol{\tau})\right\rbrace}{\mathcal{F}\left\lbrace R_{y_ty_t}(\boldsymbol{\tau})\right\rbrace-\sigma^2_{\varepsilon}}-\xi.
% \end{equation}
\begin{align}
 \mathcal F\left\lbrace (m\ast m \ast \gamma)(\boldsymbol\tau)\right\rbrace&= \mathcal F\left\lbrace R_{y_{t+1}y_{t+1}}(\boldsymbol{\tau})\right\rbrace-\frac{\mathcal F\left\lbrace R_{y_{t+1}y_t}(\boldsymbol{\tau})\right\rbrace \mathcal F \left\lbrace R_{y_{t}y_{t+1}}(\boldsymbol{\tau})\right\rbrace}{\mathcal F \left\lbrace R_{y_ty_t}(\boldsymbol{\tau})\right\rbrace-\sigma_{\epsilon}^2}-\sigma_{\epsilon}^2
\end{align}
Assuming Fourier transform of the observation kernel is known the support of the disturbance covariance function can be estimated.
\section*{Appendix}
\appendix
\section*{I}
To show 
\begin{equation}
 \left(a \ast b \right)\left(\boldsymbol\tau\right)  \star c\left(\boldsymbol\tau\right)  = a\left(-\boldsymbol\tau\right)\ast\left(b \star c\right)\left(\boldsymbol\tau\right),
\end{equation}
we note that cross-correlation function is related to the convolution by \cite{Yarlagadda2009}
\begin{equation}
 \left(a \star b\right)\left(\boldsymbol\tau\right)= a\left(-\boldsymbol\tau \right)\ast b\left(\boldsymbol\tau\right).
\end{equation}
Therefore, we can write
\begin{align}
 \left(a \ast b\right)\left(\boldsymbol\tau\right) \star c\left(\boldsymbol\tau\right)&= \left(a \ast b\right)\left(-\boldsymbol\tau \right)\ast c\left(\boldsymbol\tau\right) \nonumber \\
&=a\left(-\boldsymbol\tau\right)\ast \left(b\left(-\boldsymbol\tau\right) \ast c\left(\boldsymbol\tau\right)\right)\nonumber \\
&=a\left(-\boldsymbol\tau\right)\ast\left(b\star c\right)\left(\boldsymbol\tau\right)
\end{align}
\subsection*{II}
To show  
\begin{equation}
(a \ast b)(\boldsymbol \tau) \star (a \ast b)(\boldsymbol\tau)=(a \star a)(\boldsymbol\tau)\ast(b \star b)(\boldsymbol\tau)
\end{equation}
we note that cross-correlation function is related to the convolution by 
\begin{equation}
 \left(a \star b\right)\left(\boldsymbol\tau\right)= a\left(-\boldsymbol\tau \right)\ast b\left(\boldsymbol\tau\right).
\end{equation}
Therefore, we can write
\begin{align}
 (a \ast b)(\boldsymbol \tau) \star (a \ast b)(\boldsymbol\tau)&=(a \ast b)(-\boldsymbol\tau) \ast (a \ast b)(\boldsymbol\tau) \nonumber \\
&=a(-\boldsymbol\tau)\ast a(\boldsymbol\tau) \ast b(-\boldsymbol\tau)\ast b(\boldsymbol\tau) \nonumber \\
&=(a \star a)(\boldsymbol\tau)\ast(b \star b)(\boldsymbol\tau)
\end{align}
\bibliographystyle{plain}
\bibliography{EMIDE}
\end{document}
