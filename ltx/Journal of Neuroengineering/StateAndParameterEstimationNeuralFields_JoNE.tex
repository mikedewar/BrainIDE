%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    INSTITUTE OF PHYSICS PUBLISHING                                   %
%                                                                      %
%   `Preparing an article for publication in an Institute of Physics   %
%    Publishing journal using LaTeX'                                   %
%                                                                      %
%    LaTeX source code `ioplau2e.tex' used to generate `author         %
%    guidelines', the documentation explaining and demonstrating use   %
%    of the Institute of Physics Publishing LaTeX preprint files       %
%    `iopart.cls, iopart12.clo and iopart10.clo'.                      %
%                                                                      %
%    `ioplau2e.tex' itself uses LaTeX with `iopart.cls'                %
%                                                                      %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
% First we have a character check
%
% ! exclamation mark    " double quote  
% # hash                ` opening quote (grave)
% & ampersand           ' closing quote (acute)
% $ dollar              % percent       
% ( open parenthesis    ) close paren.  
% - hyphen              = equals sign
% | vertical bar        ~ tilde         
% @ at sign             _ underscore
% { open curly brace    } close curly   
% [ open square         ] close square bracket
% + plus sign           ; semi-colon    
% * asterisk            : colon
% < open angle bracket  > close angle   
% , comma               . full stop
% ? question mark       / forward slash 
% \ backslash           ^ circumflex
%
% ABCDEFGHIJKLMNOPQRSTUVWXYZ 
% abcdefghijklmnopqrstuvwxyz 
% 1234567890
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% \documentclass[twocolumn,11pt,a4paper]{article}		%<-use this for checking equation length
\documentclass[12pt]{iopart}		%<-comment out this for checking equations (author list too)
\usepackage{graphicx, algorithm, algorithmicx, algpseudocode}
\usepackage{color}  
\usepackage{iopams,setstack}                  %<-comment out this for checking equations
% \usepackage{anysize}							%<-use this for checking equation length
% \marginsize{1.7cm}{1.5cm}{0.3cm}{0.8cm}			%<-use this for checking equation length
% \setlength{\columnsep}{0.5cm}					%<-use this for checking equation length
% \usepackage{amsmath,amssymb,amsfonts} 			%<-use this for checking equation length
% \newcommand{\gguide}{{\it Preparing graphics for IOP journals}}
\newcommand{\todo}[1]{\textsf{\emph{\textbf{\textcolor{blue}{#1}}}}}
  
\begin{document}

\title[State and Parameter Estimation for a Spatio-Temporal Neural Fields]{State and Parameter Estimation for a Spatio-Temporal Neural Fields}

\author{Dean R. Freestone$^1$, Parham Aram$^2$, Michael Dewar$^3$, Kenneth Scerri$^4$, David B. Grayden$^1$, and Visakan Kadirkamanathan$^2$}

\address{$^1$ Department
of Electrical and Electronic Engineering, University of Melbourne, Melbourne,
Vic, 3010 Australia}
\address{$^2$ Department
of Automatic Control and Systems Engineering, University of Sheffield, Mappin Street, Sheffield,
S1 3JD, UK}
\address{$^3$ Mike's}
\address{$^4$ Ken's}
\ead{dfreestone@bionicear.org}
\begin{abstract}
Will do the abstract towards the end.
\end{abstract}

%Uncomment for PACS numbers title message
%\pacs{00.00, 20.00, 42.10}
% Keywords required only for MST, PB, PMB, PM, JOA, JOB? 
%\vspace{2pc}
%\noindent{\it Keywords}: Article preparation, IOP journals
% Uncomment for Submitted to journal title message
%\submitto{\JPA}
% Comment out if separate title page not required
\maketitle

\section{Introduction}
The human brain is arguably the world's most complex system. Approximately 100 billion neurons and 60 trillion synapses operate in concert to process information, resulting in the cognition that determines our behavior. The brain's efficiency, robustness, and adaptability are unparalleled by any man-made device and, despite decades of concerted research, our understanding of its complex dynamics remains modest. This has led researchers to study different scales of dynamics: starting from individual proteins, synapses, and neurons; moving through to neuronal networks and ensembles of neuronal networks. While our understanding of the function of neurons is well developed, the overall behavior of the brain's meso and macro-scale dynamics remains largely a mystery.  Understanding the brain at this level is extremely important since this is the scale where pathologies such as epilepsy, Parkinson's disease and schizophrenia are manifested.

To date, there has been a considerable volume of work in generating physiologically plausible neural field models to fill the void of understanding brain dynamics at the meso/macroscopic scale. Mathematical neural field  models provide insights into the underlying physics and dynamics of electroencephalography (EEG) and magnetoencephalography (MEG) (see~\cite{Deco2008}~\cite{David2003} for recent reviews). These models have demonstrated possible mechanisms for the genesis of neural rhythms (such as the alpha and gamma rhythms) \cite{Liley1999} \cite{RENNIE2000}, epileptic seizure generation \cite{DaSilva2003},~\cite{Suffczynski2004} and~\cite{Wendling2005} and insights into other pathologies~\cite{Moran2008} \cite{Schiff2009} that would be impossible to gain from experimental data alone. Unfortunately, to date the use of these models in the clinic has been limited, since the neural field model are constructed for a general brain dynamics and pathologies almost always have unique underlying patient specific causes. Data from EEG and functional magnetic resonance imaging (fMRI) offers the patient specific link to macro-scale cortical dynamics, making these tools readily applicable to the clinic. However the underlying system properties, or system states, are hidden from EEG and fMRI data, making predictions of the underlying physiology inherently difficult.

For models to be clinically viable they must be patient specific. A possible approach to achieve this would be to use a general neural field model, like the Wilson and Cohen~\cite{Wilson1973} model or a neural mass model like the Jansen and Ritt model~\cite{Jansen1995}, and fit it to a patients EEG data. Fitting the neural models to individuals is a highly non-trivial task, and until very recently this has not been reported in the literature. An estimation frame work for neural field models known as dynamical causal modelling (DCM)~\cite{David2003} \cite{David2006} has recently been proposed for studying evoked potential dynamics. Via a Bayesian inference scheme, DCM estimates the long range connectivity structure between the specific isolated brain regions that best explains a given data set. The interconnected brain regions are modelled by the Jansen and Ritt equations. The approach has proven very useful in understanding specific hierarchical networks of neural information processing. Another recent publication describing a parameter estimation method with a neural field model used an unscented Kalman filter with the Wilson-Cowan neural field equations~\cite{schiff2008kalman}.  This work takes a more system theoretic approach to the neural estimation problem and marks a first step in what has the potential to revolutionize the treatment of many neurological diseases where therapeutic electrical stimulation is viable.

Generating patient specific models allows the application of a range of techniques from control systems theory, where tailored electrical stimulation could be used therapeutically in a closed loop fashion.  Currently available epileptic seizure prediction and control devices (i.e., the vagal nerve stimulator) are implemented in an ``open loop".  That is, the therapeutic electrical stimulation waveforms are adjusted for each patient by trial and error, disregarding the patient's neurodynamics and information about their particular pathologies. Given access to an accurate model, the application of control theory in these circumstances would allow for robust therapeutic stimulations.
The work from Schiff and Sauer~\cite{schiff2008kalman} successfully demonstrated  it is possible to estimate parameters from the WC equations. This motivates the question, what parameters are the most important? How patient specific do the assumed parameters have to be for the model to be useful? In this paper, we address both of these questions and provide a theoretical platform to perform patient-specific grey-box modeling of the human neocortex.

\section{The Importance of Cortical Structure}
Neural field models use statistics of the cortex to relate mean firing rates of pre-synaptic neural populations to post-synaptic mean membrane potentials. Each neural population represents a functional cortical processing unit referred to as a column. The columnar organization of the cortex is not discrete, but is continuous, where pyramidal cells are members of many columns. In general, cortical structure can be modeled in a physiologically plausible manner as being locally homogeneous (in short range intracortical connectivity) and heterogeneous (in long range corticocortical and corticothalamic connectivity)~\cite{Jirsa2009}~\cite{Qubbaj2007}. Locally, each column is thought be connected via symmetric short range local excitation, with surround inhibition~\cite{Braitenberg1998}. For example, this structural organization is most studied in the visual system, where the surrounding inhibition effectively tunes a cortical column to a particular receptive visual field~\cite{Sullivan2006}.

Recent studies using neural field models have demonstrated the theoretical implications of specific connectivity structure of the cortical columns in neural field models, where the connectivity kernel governs the bifurcation points of the system~\cite{Hutt2005} and types of oscillations that can be generated~\cite{Schmidt2009}. This implies that if we could estimate the connectivity structure for an individual, then we could capture the essential patient specific neurodynamics that lead to various oscillatory states. Estimating functional cortical connectivity via EEG measures is currently a highly active area of research. There is a lot of interest in understanding the hierarchy of brain regions that are involved in specific tasks, motivating the use of techniques like DCM. Other comparable methods are based on auto-regressive (AR) modeling of the EEG, MEG, or fMRI times series. Studies have concentrated on finding functional connectivity patterns using information contained in the AR coefficients using techniques like Granger causality~\cite{Hesse2003}, the direct transfer function~\cite{Kaminski1991}, and partial directed coherence~\cite{Sameshima1999}. Again, like the DCM approach, these methods estimate long range connectivity patterns that does not provide a clear relationship between the continuum field models and data.
%Analysis of the connectivity estimates provided from these approaches has involved graph theoretical techniques (small worldedness).

Until now, estimation of local intracortical connectivity structure has not been attempted. Recently, it has been shown that it is possible to estimate local coupling of systems governed by integro-difference equations by formulating a state-space model~\cite{Dewar2009}. The key development in this work was to define a state-space model with an order independent of the number of observations (or ECoG recording electrodes in this case). In addition, the appropriate model selection tools have been developed~\cite{Scerri2009} allowing for the application of this technique to neural fields. Modelling the neural dynamics within this framework has a distinct advantage over AR models, such that the number of parameters to define the connectivity basis functions (three in the paper) is considerably smaller than the number of AR coefficients typically required to achieve the relevant information criteria (AIC or BIC). In this paper, we demonstrate for the first time how intracortical connectivity can be inferred from ECoG data, based on a variant of the  Wilson and Cowan neural field model~\cite{Wilson1973}. This work provides a fundamental link between the theoretical advances in neural field modelling and patient specific data.
%Long range connectivity can be estimated using DTI~\cite{Knock2009}, however this short range connectivity can not..

\section{Neural Field Model}
In this section, we describe a variant of the  Wilson and Cowan neural field model~\cite{Wilson1973} that will be used to create our intracortical connectivity estimator. This model is descriptive of a range of neurodynamics of the cortex such as evoked potentials, visual hallucinations, and epileptic behaviour. 
\begin{todo}
	{Really need to add some citations here.}
\end{todo}
It is also capable of generating complex patterns of activity such as Turing patterns, spirals, and traveling oscillations. 
\begin{todo}
	{Same here.}
\end{todo}
The neural field model is popular due to being parsimonious yet having a strong link with the underlying physiology. The model relates the average number of action potentials $g(\mathbf{r},t)$ arriving at position $r$ to the local post synaptic membrane voltage $v(\mathbf{r},t)$. The post-synaptic potentials generated at a neuronal population at location $\mathbf{r}$ by action potentials arriving from all other connected populations at locations $\mathbf{r}'$ can be described by
\begin{equation}\label{SpikesToPotential}
	v\left( {\mathbf{r},t} \right) = \int_{ - \infty }^t {h\left( {t - t'} \right)g\left( {\mathbf{r},t'} \right)dt'}.
\end{equation}
The post-synaptic response kernel $h(t)$ is described by
\begin{equation}\label{SynapticRespKernel}
	h(t) = \eta(t)\exp{\left(-\alpha t\right)}.
\end{equation}
where $\alpha=\tau^{-1}$ and $\tau$ is the synaptic time constant and $\eta(t)$ is the Heaviside step function. Nonlocal interactions between cortical populations are described by	
\begin{equation}\label{RateBasedInteractions}
	g\left( \mathbf{r},t \right) = \int_\Omega  {w\left( \mathbf{r},\mathbf{r}' \right)f\left( v\left( \mathbf{r}',t \right) \right)d\mathbf{r}'} + u\left(\mathbf{r},t\right),
\end{equation}
where $f(\cdot)$ is the firing rate function, $w(\cdot)$ is the spatial connectivity kernel, and $\Omega$ is the spatial domain, representing a cortical sheet or surface. The term $u\left(\mathbf{r},t\right)$ represents a known external input. The connectivity kernel is typically a ``Mexican hat'' function \todo{which describes strong local activation, weak mid-range repression and weak long-range activation \cite{}}. The exact shape of this kernel is assumed to vary across patients, and hence needs to be inferred from data. 

The firing rate of the presynaptic neurons is related to the postsynaptic membrane potential by the sigmoidal activation function
\begin{equation}\label{ActivationFunction}
	f\left( v\left( \mathbf{r}', t \right) \right) = \frac{f_{max}\left(\mathbf{r}\right)}{1 + \exp \left( \beta \left( v_0\left( \mathbf{r} \right) - v\left(\mathbf{r}',t\right) \right) \right)}.
\end{equation}
%Even though we will model $v_0$ and $\nu_0$ as being homogeneous across the cortical sheet we will express it as being spatially dependant allowing for generality at this point. 
By substituting \ref{RateBasedInteractions} into \ref{SpikesToPotential}  we get the spatiotemporal model
\begin{equation}\label{FullDoubleIntModel}
	v\left(\mathbf{r},t\right) = \int_{-\infty}^t h\left(t - t'\right) \left(\int_\Omega   w\left(\mathbf{r},\mathbf{r}'\right) f\left( v\left( \mathbf{r}',t \right)\right)d\mathbf{r}' + u(\mathbf{r},t) \right)dt'.
\end{equation}
To arrive at the final form of the model we shall state the synaptic response kernel as a Green's function
\begin{equation}\label{GreensFuncDef}
	Lh\left( t \right) = \delta \left( t \right),
\end{equation}
\todo{Is the above right? I can't think of any way to make $\frac{d}{dt} h(t) + \alpha h(t)$ equal to $\delta(t)$}
where $L=\frac{d}{dt} + \alpha$ is a temporal differential operator and $\delta(t)$ is the Dirac-delta function giving 
\begin{equation}\label{FinalFormContinuous}
	\frac{dv\left( \mathbf{r},t \right)}{dt} + \alpha v\left( \mathbf{r},t \right) = \int_\Omega  {w\left( \mathbf{r},\mathbf{r}' \right)f\left( {v\left( \mathbf{r}',t \right)} \right)dr'} + u\left(\mathbf{r},t\right).
\end{equation}
To arrive at the integro-difference equation (IDE) form of the model we discretize time using a first-order Euler method (see Appendix~\ref{Time Discretization}) giving
\begin{equation}\label{DiscreteTimeModel}
	v_{t+1}\left(\mathbf{r}\right) = \lambda v_t\left(\mathbf{r}\right) + T \int_\Omega { w\left(\mathbf{r}-\mathbf{r}'\right) f\left(v_t\left(\mathbf{r}'\right)\right) d\mathbf{r}'} + T u_t\left(\mathbf{r}\right) + e_t\left(\mathbf{r}\right),
\end{equation} 
where $T$ is the time step, $\lambda = 1-\alpha T$ and is an iid spatially colored, temporally white disturbance term with the covariance 
\begin{equation}
    cov(\mathbf{e}_t,\mathbf{e}_{t+\tau}) = \left\{ \begin{array}{*{20}{c}}
    \gamma\left(\mathbf{r}-\mathbf{r'}\right), & \tau=0 \\
    0 & \mathrm{otherwise}. \\
    \end{array} \right.
\end{equation}
This term was added to drive the system and account for uncertainty in the model. To generate iEEG/LFP data we used the output function 
\begin{equation}
	\mathbf{y}_t = \int_{\Omega}{m\left(\mathbf{r}_n-\mathbf{r}'\right)v_t\left(\mathbf{r}'\right)d\mathbf{r}'} + \boldsymbol{\varepsilon}_t,
\end{equation}
where $\mathbf{r}_n$ defines the location of the sensors in the field where $n=1,...,N$ indexes the sensors and $\varepsilon_t \sim \mathcal{N}\left(0,\Sigma{\varepsilon}\right)$. The output kernel $m(\mathbf{r}-\mathbf{r}')$ governs the geometry of pick-up range of sensors where
\begin{equation}
	m\left(\mathbf{r}-\mathbf{r}'\right) = \exp{\left(-\frac{(\mathbf{r}-\mathbf{r}')^\top(\mathbf{r}-\mathbf{r}')}{\sigma_m^2}\right)}.
\end{equation}

\section{Derivation of Finite Dimensional State-Space Model}
In order to implement standard estimation techniques we use a decomposition of the field using a set of Gaussian basis functions. Decomposition allows a continuous field to be represented by a finite dimensional state vector. This allows for the application of standard nonlinear, non-Gaussian state estimation methods such as the unscented Kalman filter. The field decomposition is described by
\begin{equation}\label{DefFieldDecomp}
	v_t\left(\mathbf{r}\right) \approx \boldsymbol{\phi}^{\top}\left(\mathbf{r}\right) \mathbf{x}_t,
\end{equation}
where $\mathbf{\boldsymbol{\phi}}(\mathbf{r})$ is a vector of Gaussian basis functions that are scaled by the state vector, $\mathbf{x}_t$. The width and positioning of the basis function can be determined by spectral analysis explain in detail in Section~\ref{SpectralAnalysisSection}. The connectivity kernel can also be considered a decomposition into a set of basis functions in 
\begin{equation}\label{DefKernelDecomp}
	w\left(\mathbf{r}-\mathbf{r}'\right) = \boldsymbol{\theta}^{\top} \boldsymbol{\psi}\left(\mathbf{r}-\mathbf{r}'\right).
\end{equation}
We will assume we know the parametric form of the connectivity basis functions, where the parameter $\boldsymbol{\theta}$ is unknown. Each connectivity basis function can be considered a layer in the Wilson and Cowan model, representing short range excitation, surround inhibition and mid-range excitation. Making substitutions of~\ref{DefFieldDecomp} and~\ref{DefKernelDecomp} into~\ref{DiscreteTimeModel} we get
\begin{equation}\label{reduced continuous model}
	\boldsymbol{\phi}^{\top}\left( \mathbf{r} \right) \mathbf{x}_{t+1} = T\boldsymbol{\theta}^{\top}\int_\Omega  {\boldsymbol{\psi} \left(\mathbf{r}-\mathbf{r}'\right) f\left( \boldsymbol{\phi} ^{\top}\left(\mathbf{r}'\right)\mathbf{x}_t \right)d\mathbf{r}'}  - \lambda\boldsymbol{\phi}^{\top}\left(\mathbf{r}\right)x_t + T u_t\left(\mathbf{r}\right) + e_t\left(\mathbf{r}\right).
\end{equation}
By defining the term
\begin{equation}\label{DefGamma}
	\boldsymbol{\Gamma}  = \int_\Omega  {\boldsymbol{\phi} \left(\mathbf{r}\right)\boldsymbol{\phi} ^{\top}\left(\mathbf{r}\right)d\mathbf{r}}
\end{equation}
and manipulating the model (see Appendix~\ref{Simplifying Decomposition}) we isolate the state vector to form a state-space model
\begin{eqnarray}\label{Homogeneous SS Model}
	\mathbf{x}_t &=& q(\mathbf{x}_t)\boldsymbol{\theta} - \lambda\mathbf{x}_t + k\left(u_t\left(\mathbf{r}\right)\right) + \mathbf{e}_t \\
	q\left(\mathbf{x}_t\right) &=& T\boldsymbol{\Gamma}^{ - 1}\int_\Omega {\boldsymbol{\phi}\left(\mathbf{r}\right) \int_\Omega {f\left(\boldsymbol{\phi}^{\top}\left(\mathbf{r}'\right)\mathbf{x}_t\right)\boldsymbol{\psi}^{\top} \left(\mathbf{r}-\mathbf{r}'\right)d\mathbf{r}'} d\mathbf{r}} \\
	k(u_t(\mathbf{r})) &=& \boldsymbol{\Gamma}^{-1}T \int_\Omega{\boldsymbol{\phi} \left(\mathbf{r}\right) u_t\left(\mathbf{r}\right)d\mathbf{r}} \\
	\mathbf{e}_t &=& \boldsymbol{\Gamma}^{-1}\int_\Omega{\boldsymbol{\phi}\left(\mathbf{r}\right)e_t\left(\mathbf{r}\right)d\mathbf{r}},
\end{eqnarray}
where $\mathbf{e}_t$ is a zero-mean normal white noise term with the covariance (see Appendix \ref{ColoredNoise})
\begin{equation} 
\boldsymbol{\Sigma}_e=\boldsymbol{\Gamma}^{-1}\int\limits_{\Omega}\int\limits_{\Omega}\boldsymbol{\phi}\left(\mathbf r\right) \rho\left(\mathbf{r}- \mathbf{r}' \right)\boldsymbol{\phi}\left(\mathbf{r}'\right)^{\top}d\mathbf{r}' d\mathbf{r}\boldsymbol{\Gamma}^{- \top} 
\end{equation}
The observation equation of the reduced model is
\begin{equation}\label{ObservationEquation}
    \mathbf{y}_t = \mathbf{C}\mathbf{x}_t + \boldsymbol{\varepsilon}_t \\
\end{equation}
where the observation matrix is
\begin{equation}
\mathbf{C} = \left[\begin{array}{{cccc}}
\int_{\Omega}m(\mathbf{r}_1 - \mathbf{r}')\boldsymbol{\phi}_1(\mathbf{r}')d\mathbf{r}' & \int_{\Omega} m(\mathbf{r}_1 - \mathbf r')\boldsymbol \phi_2(\mathbf r')d\mathbf r' & \dots
&\int_{\Omega}m(\mathbf r_1 - \mathbf r')\boldsymbol \phi_n(\mathbf
r')d\mathbf r' \\
\int_{\Omega}m(\mathbf r_2 - \mathbf r')\boldsymbol \phi_1(\mathbf
r')d\mathbf r'&\int_{\Omega}m(\mathbf r_2 - \mathbf r')\boldsymbol
\phi_2(\mathbf r')d\mathbf r'& \dots &\int_{\Omega}m(\mathbf r_2 -
\mathbf r')\boldsymbol \phi_n(\mathbf r')d\mathbf r'\\
\vdots&\vdots&\ddots&\vdots\\
\int_{\Omega}m(\mathbf r_{n_y} - \mathbf r')\boldsymbol \phi_1(\mathbf
r')d\mathbf r'&\int_{\Omega}m(\mathbf r_{n_y} - \mathbf r')\boldsymbol
\phi_2(\mathbf r')d\mathbf r'& \dots &\int_{\Omega}m(\mathbf r_{n_y} -
\mathbf r')\boldsymbol \phi_n(\mathbf r')d\mathbf r'
\end{array}\right],
\end{equation}
and $m(\mathbf r_n - \mathbf{r}')$ is the observation kernel.
\section{Spectral Analysis and Decomposition}\label{SpectralAnalysisSection}
Spectral analysis has been utilized to identify the number of sensors and basis functions required to accurately represent dynamic spatial fields from sampled observations~\cite{Sanner1992,Scerri2009}. Based on the higher dimension extensions of Shannon's sampling theorem~\cite{Peterson1962}, the spatial bandwidth of the observed field provides a lower bound on both the number of sensors and the number of basis functions required.

The spectral characteristics of the field are governed by the the shape of the connectivity kernel or the coloring of the covariance of the field disturbance. If the bandwidth of the disturbance covariance was greater than the bandwidth of the connectivity kernel than the disturbance governs the bandwidth of the system. Conversely, if the bandwidth of the kernel was higher then it will govern the systems bandwidth. Note, the theoretically observable characteristics of the kernel are governed by how it is excited by an input.
 
Considering the spectral representation of the dynamic field at time $t$ denoted by $W_t(\boldsymbol{\nu})$ (\todo{can this be $V_t(\boldsymbol{\nu})$}), multi-dimensional sampling and reconstruction theory dictates that for and accurate representation, $W_t(\boldsymbol{\nu})$ need to be spatially bandlimited. Such a condition is usually satisfied due to the bandlimiting effect of most practical kernels and the spatially colored covariance functions \todo{~cite\{some eeg literature here\}}. In addition, the Gaussian pick-up of the sensors provides a low-pass filtering effect, further limiting the effective bandwidth of the field. Thus the spectral response $W_t(\boldsymbol{\nu})$ is usually band-limited with $W_t(\boldsymbol{\nu}) \approx 0 ~ \forall \boldsymbol{\nu} > \boldsymbol{\nu}_c$, where $\boldsymbol{\nu}_c$ is a cutoff frequency. Note that for the 3-dimensional, homogeneous and isotopic fields being considered $\boldsymbol{\nu}_c = [\nu_c ~ \nu_c ~ \nu_c]^\top$. Given such a band-limited field and based on Shannon's sampling theorem, the distance between adjacent sensors, $\Delta_s$, needs to satisfy
\begin{equation}
\Delta_s = \frac{1}{2\rho\nu_c},
\end{equation}
where $\rho \in \mathbb{R} \ge 1$ is an oversampling parameter. Satisfying this condition is required for the hidden dynamic field $w_t(\mathbf{r})$ to be precisely inferred from the sampled observations, $\mathbf{y}(t)$. If the condition is not satisfied, spatial aliasing will occur.

There are two options available to alleviate problems with aliasing. The first is to increase the spatial sampling frequency (using a higher resolution of sensors), and the second is to use wider sensors, which act like an anti-aliasing filters. The first option is more desirable, since more information is retained. However, if sensors are modelled as having a Gaussian pickup range than the effective filter will not have a sharp cut-off and high frequency components will still remain and mild aliasing will still occur. In practice, the bandwidth of the cortex can not be estimated by traditional electrophysiological measurements due to spatial limitations. It may be estimated using other modalities with higher spatial resolution such as, fMRI, NIRS or other optical imaging techniques. Errors associated with mild aliasing result in distortions in the high frequency components of the field dynamics. However, after applying the basis function decomposition, high frequency component contribute negligibly to the inverse problem and are therefore not a major problem for the state and parameter estimation~\cite{Sanner1992}. This is further illustrated our results.
$\mathcal{W}$
Similar considerations need to be made regarding the representation of the dynamic field $w_t(\mathbf{r})$ by its basis functions decompositions. The minimum distance between adjacent basis functions also needs to satisfy Shannon's sampling theorem. Thus, the minimum distance between basis must satisfy
\begin{equation}
\Delta_b < \frac{1}{2\nu_c}.
\end{equation}
For the Gaussian basis functions $\phi(\mathbf{r})$ being considered, the basis width parameter $\sigma^2$ can also be inferred from spectral considerations \cite{Sanner1992,Scerri2009}. The Fourier transform of a Gaussian is another Gaussian with width $\sigma^2_{\nu}$ given by
\begin{equation}
\sigma^2_{\nu} = \frac{1}{\pi^2\sigma^2}.
\end{equation}
Thus to ensure that the basis functions can represent fields with frequency content up to $\boldsymbol{\nu}_c$, $\sigma_{\nu}$ should be set with the 3~dB frequency for the basis functions equal to $\boldsymbol{\nu}_c$, thus obtained $\sigma^2_{\nu}$ and therefore $\sigma^2$.

The dynamic field must be spatially oversampled by using a larger number of sensor then imposed by frequency considerations, to use frequency analysis to determine the minimum distance and width of the basis functions. Following this, the distance between basis functions is usually larger then the distance between sensors. \todo{However this is not the case with this work.} For a spatially homogeneous isotropic field the basis functions can be placed on a regular square grid. Following this, the knowledge of the distance between basis functions directly implies the total number of basis functions required.

Due to the high dimensionality of the brain and the current electrode systems, we can not expect to have more sensors than basis functions.
\section{State and Parameter Estimation}
The Unscented Rauch Tung Striebel Smoother (URTSS) was used for the state estimation \cite{Sarkka2010}, where the Unscented Kalman filter is used for the forward iteration \cite{Julier1997}. This method is appropriate for state estimation of stochastic nonlinear dynamical systems. Alternatives to this approach are the Extended Kalman Filter (EKF) \cite{Haykin2001}  and sequential Monte Carlo (SMC) filtering \cite{doucet2001}. The extended Kalman filter approximates the state transition equation by linearizing about the fixed points (by calculating the Jacobian) at each time step. The linearization maintains the Gaussianality in the model. The unscented Kalman filter approximates the \emph{a posteriori} state density by a Gaussian distribution using a minimal set of carefully chosen sigma points, whilst maintaining the nonlinearity in the system. This has been shown to give superior performance over the EKF in state estimation, as the EKF maintains a first order approximation where the UKF provides an approximation accurate at least to the second order. In addition, the computation of the Jacobian used in the
EKF can be problematic. SMC filtering can theoretically provide an exact \emph{a posteriori} state density for nonlinear systems. However, currently this method is not appropriate for our problem, due to computation demands.

The approximation of the state density is made by propagating the so called sigma points through the state-space model. The sigma points describe the mean and covariance of the state estimate. The accuracy of the approximation is linked to the time step used in the temporal discretization of the system, where the state transition appears more linear with a finer resolution. To explain the operation of the UKF we begin by deriving the traditional Kalman filter with the state-space model
\begin{eqnarray}
	\mathbf{x}_t = q(\mathbf{x}_t)\boldsymbol{\theta} - \lambda\mathbf{x}_t + k\left(u_t\left(\mathbf{r}\right)\right) + \mathbf{e}_t \\
	\mathbf{y}_t = \mathbf{C}\mathbf{x}_t + \boldsymbol{\varepsilon}_t,
\end{eqnarray}
where $\boldsymbol{\theta}$ and $\lambda$ are the systems parameters. In forwards iteration since the observation equation is linear in parameters we only need to use Unsented transformation in prediction step. The update step equations remains the same as the standard Kalman filtering formulations. In backward smoothing path, estimation from the forward filtering is used to form the new set of sigma points in a similar way to the filterin step, however the predicted cross-covariance of the states need to be calculated. A summary of the algorithm is given in Algorithm 
\begin{algorithm}
\caption{The Unscented Kalman Filter}\label{CPTEMDAlgorithm}
\begin{algorithmic}[1]
\State Initialise
\begin{equation*}
\hat{\mathbf x}_0, \mathbf P_0
\end{equation*}
\State Forward iteration
For $t \in \left\lbrace 0,\cdots, N\right\rbrace $,
\begin{equation*}
\mathcal X_t^f=[\hat{\mathbf x}_t^f \quad \hat{\mathbf x}_t^f+\gamma\sqrt{\mathbf P_t^f} \quad \hat{\mathbf x}_t^{f}-\gamma\sqrt{\mathbf P_t^f}]
\end{equation*}
Propogate the sigma points through the dynamic model
\begin{equation}
\mathcal X_{i,t+1|t}^{f*}=Q(\mathcal X_t^f,u_t)
\end{equation}
\begin{equation*}
 \hat{\mathbf x}_{k +t}^{f-}=\sum_{i=0}^{2L} W_i^{(m)}\mathcal X_{i,t+1|t}^{f*}
\end{equation*}
\begin{equation*}
 \mathbf P_{t +1}^-=\sum_{i=0}^{2L} W_i^{(c)}(\mathcal X_{i,t+1}^{f-}-\hat{\mathbf x}_{t +1}^{f-})(\mathcal X_{i,t+1}^{f-}-\hat{\mathbf x}_{t +1}^{f-})^\top+\boldsymbol \Sigma_e
\end{equation*}
\begin{equation*}
\mathcal K_{t+1}=\mathbf P_{t +1}^{f-}\mathbf C ^\top(\mathbf C \mathbf P_{t +1}^{f-}\mathbf C ^\top+\boldsymbol \Sigma_v)^{-1}
\end{equation*}
\begin{equation*}
\hat{\mathbf x}_{t+1}^{f}=\hat{\mathbf x}_{t+1}^{f-}+\mathcal K_{t+1}(\mathbf y_{t+1}-\mathbf C\hat{\mathbf x}_{t +1}^{f-})
\end{equation*}
\begin{equation*}
 \mathbf P_{t+1}^f=(\mathbf I - \mathcal K_{t+1}\mathbf C)\mathbf P_{t +1}^{f-}
\end{equation*}
\State Backward iteration
\State initialisation
\begin{equation}
 \mathbf P_T^s= \mathbf P_T^f
\end{equation}
\begin{equation}
\hat{\mathbf x}^s_T= \hat{\mathbf x}^f_T
\end{equation}
For $t \in \left\lbrace N-1, \cdots, 0 \right\rbrace $
\begin{equation*}
\mathcal X_t^s=[\hat{\mathbf x}_t^f \quad \hat{\mathbf x}_t^f+\gamma\sqrt{\mathbf P_t^f} \quad \hat{\mathbf x}_t^f-\gamma\sqrt{\mathbf P_t^f}]
\end{equation*}
\end{algorithmic}
\end{algorithm}
\section{Results}

\subsection{Frequency Analysis}

\subsection{State and Parameter Estimates}


\section{Discussion}

\begin{itemize}

	\item Discuss the novelty of using the reduced model (BrainIDE) for creating patient specific models. 
	\item The size of the dentritic arbour is consistent in the cortex.
	\item Spatial frequency of field and the relationship to the required density of electrodes/sensors required for estimation or to capture the required information without aliasing.
	
	\item The flexibility of the approach
	
	\item Combining this method with other to incorporate longer range hetrogeneous connectivity.
	
	\item The need for activity to perform estimation.
\end{itemize}

\section{Conclusion}
\appendix
\section{Discrete Time Model}\label{Time Discretization}
To form the IDE neural field model a time discretization must be perform. We used a one-step Euler method where Eq.~\ref{FinalForm1} can be approximated by
\begin{equation}\label{Euler Approximation}	
\frac{v\left( \mathbf{r},t+T \right) - v\left( \mathbf{r},t\right)}{T} =   -\alpha v\left( \mathbf{r},t \right) + \int_\Omega  {w\left( \mathbf{r}-\mathbf{r}' \right)f\left( {v\left( \mathbf{r}',t \right)} \right)d\mathbf{r}'} + u\left(\mathbf{r},t\right).
\end{equation}
For clarity, we shall index time points in the discrete time form of the model using the subscript $t$ and the next time point as $t+1$. Rearranging Eq.~\ref{Euler Approximation} we get
\begin{eqnarray}\label{Euler Approximation}	
v_{t+1}\left( \mathbf{r}\right) &=& v_t\left( \mathbf{r}\right) -T \alpha v_t\left( \mathbf{r}\right) + T \int_\Omega  {w\left( \mathbf{r}-\mathbf{r}' \right)f\left( {v_t\left( \mathbf{r}'\right)} \right)d\mathbf{r}'} + T u_t\left(\mathbf{r}\right).
\end{eqnarray}
The discrete time form of the model is
\begin{equation}\label{Discrete Time Model1}
	v_{t+1}\left(\mathbf{r}\right) = \lambda v_t\left(\mathbf{r}\right) + T \int_\Omega { w\left(\mathbf{r}-\mathbf{r}'\right) f\left(v_t\left(\mathbf{r}'\right)\right) d\mathbf{r}'} + T u_t\left(\mathbf{r}\right),
\end{equation}
where $\lambda = 1 - T \alpha$.
 \section{Numerical Simulation of IDE Model}\label{Space Discretization}
To solve the intregro-difference equation in Eq.~\ref{Discrete Time Model1} we define the spatial aspect of the model on a regular square $i,j$ grid of neural masses, where the spatial step size $\Delta \mathbf{r}_i = \Delta \mathbf{r}_j = \Delta $ giving
\begin{equation}\label{discrete space}
	v_{t+1}\left(\mathbf{r}_{ij}\right) = \lambda v_t\left(\mathbf{r}_{ij}\right) + T \Delta^2 \sum_{i=1}^{i=I}{ \sum_{j=1}^{j=J}{ w\left( \mathbf{r}-\mathbf{r}_{ij}' \right)f\left( v_t\left( \mathbf{r}_{ij}'\right) \right)} } + T u_t\left(\mathbf{r}\right) + e_t(\mathbf{r}_{i,j}),
\end{equation}
where $e_t(\mathbf{r}_{i,j}) \sim \mathcal{N}\left(0,\Sigma\right)$.
We have used free boundary conditions and extended the spatial domain to alleviate problems associated with edge effects.
\section{Reduction to Finite State-Space Model}\label{Simplifying Decomposition}
Starting from Eq.~\ref{reduced continuous model} we multiply throughout by $\boldsymbol{\phi}(r)$ and integrate over the spatial domain $\Omega$ to get
\begin{eqnarray}\label{Poo}
	\int_\Omega  {\boldsymbol{\phi} \left(\mathbf{r}\right)\boldsymbol{\phi}^{\top}\left(\mathbf{r}\right) d\mathbf{r}} \mathbf{x}_{t+1} = T \int_\Omega  {\boldsymbol{\phi} (\mathbf{r}) \boldsymbol{\theta}^{\top} \int_\Omega  {\boldsymbol{\psi}  \left(\mathbf{r}-\mathbf{r}'\right) f\left(\boldsymbol{\phi}^{\top}\left(\mathbf{r}'\right) \mathbf{x}_t \right)d\mathbf{r}'}d\mathbf{r}} - \lambda\int_\Omega {\boldsymbol{\phi}(\mathbf{r})\boldsymbol{\phi}^{\top}(\mathbf{r})d\mathbf{r}} \mathbf{x}_t \\
	+ T \int_\Omega{\boldsymbol{\phi} \left(\mathbf{r}\right) u_t\left(\mathbf{r}\right)d\mathbf{r}} + \int_\Omega{\boldsymbol{\phi} \left(\mathbf{r}\right) e_t\left(\mathbf{r}\right)d\mathbf{r}}.
\end{eqnarray}
Substituting Eq.~\ref{DefGamma} into Eq.~\ref{Poo} and cross-multiplying by $\boldsymbol{\Gamma}^{-1}$ gives 
\begin{eqnarray}\label{Homogeneous SS Model}
	\mathbf{x}_{t+1} = T\boldsymbol{\Gamma}^{ - 1}\int_\Omega {\boldsymbol{\phi}\left(\mathbf{r}\right) \int_\Omega {f\left(\boldsymbol{\phi}^{\top}\left(\mathbf{r}'\right)\mathbf{x}_t\right) \boldsymbol{\psi}^{\top} \left(\mathbf{r}-\mathbf{r}'\right)d\mathbf{r}'} d\mathbf{r}} \boldsymbol{\theta} - \lambda \mathbf{x}_t \\
	+ \boldsymbol{\Gamma}^{-1}T \int_\Omega{\boldsymbol{\phi} \left(\mathbf{r}\right) u_t\left(\mathbf{r}\right)d\mathbf{r}} + \boldsymbol{\Gamma}^{-1} \int_\Omega{\boldsymbol{\phi}\left(\mathbf{r}\right)e_t\left(\mathbf{r}\right)d\mathbf{r}}.
\end{eqnarray}
\section{Reduction to Finite State-Space Model}\label{Simplifying Decomposition}
Starting from Eq.~\ref{reduced continuous model} we multiply throughout by $\boldsymbol{\phi}(r)$ and integrate over the spatial domain $\Omega$ to get
\begin{eqnarray}\label{Poo}
	\int_\Omega  {\boldsymbol{\phi} \left(\mathbf{r}\right)\boldsymbol{\phi}^{\top}\left(\mathbf{r}\right) d\mathbf{r}} \mathbf{x}_{t+1} = T \int_\Omega  {\boldsymbol{\phi} (\mathbf{r}) \boldsymbol{\theta}^{\top} \int_\Omega  {\boldsymbol{\psi}  \left(\mathbf{r}-\mathbf{r}'\right) f\left(\boldsymbol{\phi}^{\top}\left(\mathbf{r}'\right) \mathbf{x}_t \right)d\mathbf{r}'}d\mathbf{r}} - \lambda\int_\Omega {\boldsymbol{\phi}(\mathbf{r})\boldsymbol{\phi}^{\top}(\mathbf{r})d\mathbf{r}} \mathbf{x}_t \\
	+ T \int_\Omega{\boldsymbol{\phi} \left(\mathbf{r}\right) u_t\left(\mathbf{r}\right)d\mathbf{r}} + \int_\Omega{\boldsymbol{\phi} \left(\mathbf{r}\right) e_t\left(\mathbf{r}\right)d\mathbf{r}}.
\end{eqnarray}
Substituting Eq.~\ref{DefGamma} into Eq.~\ref{Poo} and cross-multiplying by $\boldsymbol{\Gamma}^{-1}$ gives 
\begin{eqnarray}\label{Homogeneous SS Model}
	\mathbf{x}_{t+1} = T\boldsymbol{\Gamma}^{ - 1}\int_\Omega {\boldsymbol{\phi}\left(\mathbf{r}\right) \int_\Omega {f\left(\boldsymbol{\phi}^{\top}\left(\mathbf{r}'\right)\mathbf{x}_t\right) \boldsymbol{\psi}^{\top} \left(\mathbf{r}-\mathbf{r}'\right)d\mathbf{r}'} d\mathbf{r}} \boldsymbol{\theta} - \lambda \mathbf{x}_t \\
	+ \boldsymbol{\Gamma}^{-1}T \int_\Omega{\boldsymbol{\phi} \left(\mathbf{r}\right) u_t\left(\mathbf{r}\right)d\mathbf{r}} + \boldsymbol{\Gamma}^{-1} \int_\Omega{\boldsymbol{\phi}\left(\mathbf{r}\right)e_t\left(\mathbf{r}\right)d\mathbf{r}}.
\end{eqnarray}
\section{}\label{ColoredNoise}
\newtheorem{lemma}{Lemma}
\begin{lemma}
Consider the zero-mean process $e_t\left(\mathbf r\right)$ with covariance as defined by (\ref{eq:FieldCovariance}) then
\begin{equation}
 \mathbf e_t=\boldsymbol{\Gamma}^{-1}\int_\Omega  {\boldsymbol{\phi} ( \mathbf{r} )e_t( \mathbf{r} )d\mathbf{r}}
\label{eq:AppendixWt}
\end{equation}
is a vector valued, zero mean normally distributed white noise process with covariance
\begin{equation}
\boldsymbol\Sigma_e =\mathbf{\Gamma}^{-1}\int_{\Omega}\int_{\Omega}\boldsymbol{\phi}\left(\mathbf r\right) \beta\left(\mathbf r- \mathbf r' \right)\boldsymbol{\phi}\left(\mathbf r'\right)^{\top}d\mathbf r' d\mathbf r\mathbf{\Gamma}^{- \top} 
\end{equation}
\label{lemma:FieldCovariance}
\end{lemma}
\subsection{proof}
 Equation (\ref{eq:AppendixWt}) is a linear function of $e_t(\mathbf r)$ and hence $\mathbf{e}_t$ is also normally distributed. The expected value of $\mathbf e_t$ is given by 
\begin{eqnarray}
 \mathbf E\left[ \mathbf e_t\right]&=& \mathbf{\Gamma}^{-1}\int_{\Omega}\boldsymbol\phi\left(\mathbf{r}\right)\mathbf E\left[e_t\left(\mathbf{r}\right)\right] d\mathbf{r} \nonumber \\
&=&\mathbf 0
\end{eqnarray}
The covariance of $\mathbf{e}_t$ is
\begin{eqnarray}
\mathbf{\Sigma}_e&=&\mathbf{\Gamma}^{-1}\mathbf E[\int_{\Omega}\boldsymbol{\phi}\left(\mathbf{r}\right)e_t\left(\mathbf{r}\right)d\mathbf{r} \int_{\Omega}\boldsymbol{\phi}\left(\mathbf{r}'\right)^{\top} e_t\left(\mathbf{r}'\right)d\mathbf{r}']\mathbf{\Gamma}^{- \top} \nonumber \\
&=&\mathbf{\Gamma}^{-1}\int_{\Omega}\int_{\Omega} \boldsymbol{\phi}\left(\mathbf{r}\right) \mathbf E[e_t\left(\mathbf{r}\right)e_t\left(\mathbf{r}'\right)]\boldsymbol{\phi}\left(\mathbf{r}'\right)^{\top}d\mathbf{r}' d\mathbf r\mathbf{\Gamma}^{- \top} \nonumber\\
&=&\mathbf{\Gamma}^{-1}\int\int\limits_{\Omega}\boldsymbol{\phi}\left(\mathbf r\right) \gamma\left(\mathbf r- \mathbf r' \right)\boldsymbol{\phi}\left(\mathbf r'\right)^{\top}d\mathbf r' d\mathbf r\mathbf{\Gamma}^{- \top} 
\end{eqnarray}
% \end{proof}
\section{Parameter estimation using LS}
\begin{equation}
 \mathbf x_{t+1}=\mathbf{x}_{t}+T_s \mathbf{Q}(\mathbf{x}_t)\boldsymbol{\theta}-\alpha T_s\mathbf{x}_t+T_s\Gamma^{-1}\int_\Omega\boldsymbol{\phi}(\mathbf r)\varepsilon_t(\mathbf r)d\mathbf r
\label{eq:statespacemodel}
\end{equation}
where the matrix $\mathbf Q_{n_x \times n_{\boldsymbol{\theta}}}(\mathbf x_t)$ is defined as
\begin{equation}
 Q_{n_x \times n_{\boldsymbol{\theta}}}=\Gamma^{-1}\int_\Omega\boldsymbol{\phi}(\mathbf r)\int_\Omega f(\boldsymbol{\phi}^\top(\mathbf r')\mathbf x_t)\mathbf{\psi}^\top(\mathbf r-\mathbf r')d\mathbf r'd\mathbf r
\end{equation}
and $T_s$ is the sampling period. Least Square method can be used as (\ref{eq:statespacemodel}) is linear in parameters. At each time instant we have
\begin{eqnarray}
 \mathbf x_{1}&=&\mathbf x_{0}+T_s \mathbf Q(\mathbf x_0) \boldsymbol{\theta}-\alpha T_s\mathbf x_0+T_s\mathbf e_0 \nonumber \\
 \mathbf x_{2}&=&\mathbf x_{1}+T_s \mathbf Q(\mathbf x_1) \boldsymbol{\theta}-\alpha T_s\mathbf x_1+T_s\mathbf e_1\nonumber\\
&\vdots& \nonumber\\
 \mathbf x_{n}&=&\mathbf x_{n-1}+T_s \mathbf Q(\mathbf x_{n-1}) \boldsymbol{\theta}-\alpha T_s\mathbf x_{n-1}+T_s\mathbf e_{n-1}
\end{eqnarray}
in matrix form
\begin{equation}
 \left[\begin{array}{{cccc}} \mathbf x_{1}\\\mathbf x_{2}\\\vdots\\\mathbf x_{n}\end{array}\right]
	-\left[\begin{array}{cccc} \mathbf x_{0}\\\mathbf x_{1}\\\vdots\\\mathbf x_{n-1}\end{array}\right]
	=T_s\left[\begin{array}{cc}\mathbf Q(\mathbf x_0)&-\mathbf x_{0}\\\mathbf Q(\mathbf x_1)&-\mathbf x_{1}\\\vdots\\ \mathbf Q(\mathbf x_{n-1})&-\mathbf x_{n-1}\end{array}\right]
	\left[\begin{array}{cc} \boldsymbol{\theta} \\ \alpha\end{array}\right]+T_s
		\left[\begin{array}{cccc}\mathbf e_0\\\mathbf e_1\\\vdots\\\mathbf e_{n-1}\end{array}\right]
\end{equation}
or in compact form we have
\begin{equation}
 \mathbf Z=\mathbf X \mathbf W+\boldsymbol \xi
\end{equation}
where
\begin{equation}
 \mathbf Z=\left[\begin{array}{cccc} \mathbf x_{1}-\mathbf x_{0}\\
\mathbf x_{2}-\mathbf x_{1}\\\vdots\\
\mathbf x_{n}-\mathbf x_{n-1}\end{array}\right],\quad \mathbf X=T_s\left[\begin{array}{cccc}\mathbf Q(\mathbf x_0)&-\mathbf x_{0}\\
\mathbf Q(\mathbf x_1)&-\mathbf x_{1}\\\vdots\\ \mathbf Q(\mathbf x_{n-1})&-\mathbf x_{n-1}\end{array}\right]
,\quad \mathbf W=\left[\begin{array}{cc} \boldsymbol{\theta} \\ \alpha\end{array}\right],\quad \boldsymbol \xi=T_s\left[\begin{array}{cccc}\mathbf e_0\\\mathbf e_1\\\vdots\\\mathbf e_{n-1}\end{array}\right]
\end{equation}
using the method of Least Square $ \mathbf W$ can be estimated
\begin{equation}
 \mathbf W\approx(\mathbf X^\top\mathbf X)^{-1}\mathbf X^\top\mathbf Z
\end{equation}
\section{Parameters}

\begin{tabular} {c|c} 
	\hline\hline
	Symbol & Description \\ \hline
	$v(\mathbf{r},t)$ & mean membrane potential \\
	$g(\mathbf{r},t)$ & average action potential rate \\
	$\mathbf{r}$ & spatial location \\
	$t$ & time (s) \\
	$h(t)$ & post-synaptic response kernel \\
	$\eta(t)$ & Heaviside function \\
	$\alpha$ & inverse synaptic time constant \\
	$w(\mathbf{r},\mathbf{r}')$ & spatial connectivity kernel \\
	$f(\mathbf{r},t)$ & firing function rate \\
	$u(\mathbf{r},t)$ & external input \\
	$\Omega$ & spatial domain \\
	$f_{max}$ & maximal firing rate \\
	$\beta$ & slope of sigmoidal activation function \\
	$v_0$ & firing threshold \\
	$\delta(t)$ & Dirac-delta function \\
	$L$ & temporal differential operator \\
	$e(\mathbf{r},t)$ & field disturbance, covariance $\gamma$\\
	$m(\mathbf{r},\mathbf{r}')$ & sensor kernel, variance $\sigma_m^2$ \\
	$\epsilon(\mathbf{r}_n,t)$ & observation noise, covariance $\Sigma_\epsilon$ \\
	$y(\mathbf{r}_n,t)$ & obseration \\
	$n$ & sensor index $n=1,..,N$ \\
	$T$ & time step \\
	$\lambda$ & time constant parameter \\
	$\mathbf{\phi(r)}$ & vector of Gaussian basis functions \\
	$\mathbf{x}_t$ & state vector at time $t$ \\
	$\mathbf{\psi}$ & vector of connectivity kernel basis functions \\
	$\theta$ & vector of connectivity kernel parameters \\
	$\Gamma$ & inner product of field basis functions \\
	$q()$ & state function \\
	$k()$ & maps into to discrete state of input function \\
	$\mathbf{e}_t$ & state disturbance, covariance $\Sigma_e$ \\
	$\mathbf{C}$ & observation matrix \\
	$V(\nu)$ & Spectrum of the dynamic field \\ 
	$\nu$ & Spatial frequency \\
	$\nu_c$ & Spatial cut-off frequency \\
\end{tabular}

\begin{tabular}{c|c}
	\hline\hline
	Symbol & Description \\ \hline
	$\Delta_s$ & Distance between adjacent sensors \\
	$\rho$ & Over-sampling parameter \\	
	$\Delta_b$ & Distance between field basis functions \\	
	$\sigma_{nu}^2$ & Variance of FT of Gaussian basis function \\
	$\sigma^2$ & Spatial variance of Gaussian in spatial domain \\
	$\hat{\mathbf{x}}$ & State estimate \\
	$\chi$ & Matrix of sigma vectors \\
	$\hat{\mathbf{x}}_t^{f-}$ & Forward \emph{a priori} estimate \\
	$\hat{\mathbf{x}}_t^f$ & Forward \emph{a posteriori} estimate \\
	$\hat{\mathbf{x}}_t^{b-}$ & Backward \emph{a priori} estimate \\
	$\hat{\mathbf{x}}_t^{b}$ & Backward \emph{a posteriori} estimate \\
	$P^f_t$ & Forward \emph{a posteriori} covariance matrix \\
	$P^{f-}_t$ & Forward \emph{a priori} covariance matrix \\	
\end{tabular}
\section*{References}
\bibliographystyle{unsrt}
\bibliography{BrainIDE}

% \begin{thebibliography}{10}
% \bibitem{book1} Goosens M, Rahtz S and Mittelbach F 1997 {\it The \LaTeX\ Graphics Companion\/} 
% (Reading, MA: Addison-Wesley)
% \bibitem{eps} Reckdahl K 1997 {\it Using Imported Graphics in \LaTeX\ } (search CTAN for the file `epslatex.pdf')
% \end{thebibliography}

\end{document}

